{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RWdznnBV21Jp"
   },
   "source": [
    "# CMSC471 Artificial Intelligence\n",
    "\n",
    "# Assignment-5: Classification and Regression with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bsZ7vKYn21Ju"
   },
   "source": [
    "## Overview and Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tPqFH4ne21Jv"
   },
   "source": [
    "You have learned the fundamentals of Neural Networks and Deep Learning. You have also learned how to train them based on the techniques discussed in lectures and the contents from Chapter 10 of the textbook (and you'll learn more on training deep neural networks in Chapter 11).\n",
    "\n",
    "In Part I of this assignment, you are going to build a NN for classification. In Part II, you will build a NN for regression.\n",
    "\n",
    "Pedagogically, this assignment will help you:\n",
    "- better understand Neural Networks.\n",
    "\n",
    "- practice with Tensorflow and Keras API.\n",
    "\n",
    "- practice the skills you learned in sklearn and combine them with tf/keras to use in your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6yQGPKle21Jx"
   },
   "source": [
    "Notice that you should have Tensorflow version 2.0 installed and ready.\n",
    "\n",
    "Alternatively, you may complete the assignment in [Colab](https://colab.research.google.com) but you need to run the following magic command in Colab to switch to version 2.0:\n",
    "\n",
    "`%tensorflow_version 2.x`\n",
    "\n",
    "<b>Important Notice:</b> Some outputs/plots are shared with you in this notebook for your reference. Some outputs are intentionally not shared. Notice that it is the strict course policy NOT to include any (or even parts) of the code solutions and/or answers to the questions in your posts in Piazza. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "D4wWpA6U21Jy",
    "outputId": "e351ad86-7132-4f0e-93a3-1c7c1f42419a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.x selected.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary Python/tf/keras modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "# Sorry, I needed this\n",
    "%tensorflow_version 2.x \n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "-3GvhX7o21J2",
    "outputId": "49b53dcd-95c6-400b-911c-caef98ffd528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf Version:  2.0.0\n",
      "Eager Execution mode:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"tf Version: \", tf.__version__)\n",
    "print(\"Eager Execution mode: \", tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l2M_t7_o21J8"
   },
   "source": [
    "## Part I - Classification with NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z38Sztpw21J9"
   },
   "source": [
    "First, [download the data](https://github.com/fereydoonvafaei/UMBC-CMSC-471-Fall-2019/blob/master/Assignment-5/diabetes.csv) `diabetes.csv` The target is to predict the onset of diabetes based on patient's features. You can read more about the data [here](https://www.kaggle.com/uciml/pima-indians-diabetes-database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "lc_nu-_521J-",
    "outputId": "7b96eb58-9a5c-4a97-9383-742f2bb25ef1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n",
       "0            6      148             72  ...                     0.627   50        1\n",
       "1            1       85             66  ...                     0.351   31        0\n",
       "2            8      183             64  ...                     0.672   32        1\n",
       "3            1       89             66  ...                     0.167   21        0\n",
       "4            0      137             40  ...                     2.288   33        1\n",
       "\n",
       "[5 rows x 9 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load diabetes data with Pandas, it should be in the same working directory.\n",
    "df = pd.read_csv(\"diabetes.csv\")\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u2CoIWnI21KC"
   },
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "z8E8ZDNZ21KD",
    "outputId": "b4334970-b7d0-41e4-e4c0-4f2c6ef7a458"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 8)\n",
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "# Create X, y - Notice that X should contain all the features (columns) except 'Outcome'\n",
    "# y should include only 'Outcome' because it's the label!\n",
    "### START CODING HERE ###\n",
    "X = df.drop(['Outcome'], axis=1)\n",
    "y = df['Outcome']\n",
    "### END CODING HERE ###\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "TkqXPP4e21KH",
    "outputId": "98406013-3c44-4313-e0e0-7309ed73c485"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(514, 8)\n",
      "(514,)\n",
      "(254, 8)\n",
      "(254,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data to train and test with test_size=0.33 and random_state=66\n",
    "### START CODING HERE ###\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=66)\n",
    "### END CODING HERE ###\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cdccab0r21KL"
   },
   "source": [
    "> There are different ways to load the data into tf tensors depeneding on your data type (image, text, etc). The following cell is one way of loading pandas dataframes to tensorflow tensors so that you can use tf/keras methods on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-rmjcuZZ21KM"
   },
   "outputs": [],
   "source": [
    "# Load train and test data to tf\n",
    "train_tensor = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))\n",
    "test_tensor = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0tsDidm721KQ",
    "outputId": "4dda53fc-42ed-4605-e1c7-4b8a5180f625"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.TensorSliceDataset"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "NZhsOL2-21KT",
    "outputId": "d6ab6044-a66f-4dd5-8f36-76f52187d898"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: [  3.   123.   100.    35.   240.    57.3    0.88  22.  ], Target: 0\n",
      "Features: [  0.    104.     64.     23.    116.     27.8     0.454  23.   ], Target: 0\n",
      "Features: [10.    75.    82.     0.     0.    33.3    0.263 38.   ], Target: 0\n",
      "Features: [  4.    183.      0.      0.      0.     28.4     0.212  36.   ], Target: 1\n",
      "Features: [  2.     98.     60.     17.    120.     34.7     0.198  22.   ], Target: 0\n"
     ]
    }
   ],
   "source": [
    "for feat, targ in train_tensor.take(5):\n",
    "  print ('Features: {}, Target: {}'.format(feat, targ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ggaUdnqo21KW"
   },
   "source": [
    "> As discussed in the lectures, data is fed into the network in batches (mini-batches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "YMDI7snX21KX",
    "outputId": "512effec-b5e8-4fee-cb5f-30b02c3b5510"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>\n"
     ]
    }
   ],
   "source": [
    "# Batch train and test data\n",
    "train_batch = train_tensor.shuffle(len(X_train)).batch(10)\n",
    "test_batch = test_tensor.shuffle(len(X_test)).batch(10)\n",
    "\n",
    "print(type(train_batch))\n",
    "print(type(test_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZAn0HKCz21Kc"
   },
   "source": [
    ">Now, build the model based on the given architecture specifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qQvOscS621Kd"
   },
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZM04rbFb21Ke"
   },
   "outputs": [],
   "source": [
    "### START CODING HERE ###\n",
    "# Build a Sequential neural network - a classifier model\n",
    "nn_clf = tf.keras.Sequential([\n",
    "    # Create a dense layer with 12 units, input_dim=8, and 'relu' activation function ~ 1 line\n",
    "    tf.keras.layers.Dense(12, input_dim=8, activation='relu'),\n",
    "    # Create a dense layer with 8 units, and 'relu' activation function ~ 1 line\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    # Create a dense layer with ? unit(s), and '?' activation function ~ 1 line\n",
    "    # YOU should decide on the number of untis and the activation function for this last layer (output layer)\n",
    "    # Hint: What type of ML task is this problem?\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])  \n",
    "### END CODING HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2TeK41qY21Kh"
   },
   "outputs": [],
   "source": [
    "### START CODING HERE ###\n",
    "# Compile the model by 'adam' optimizer, 'binary_crossentropy' loss and 'accuracy' as metrics ~ 1 line\n",
    "nn_clf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "### END CODING HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3XocYY7q21Kl",
    "outputId": "00f76894-36d8-4957-ee34-88d4ef370397"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 514 samples\n",
      "Epoch 1/150\n",
      "514/514 [==============================] - 0s 171us/sample - loss: 2.0773 - accuracy: 0.5623\n",
      "Epoch 2/150\n",
      "514/514 [==============================] - 0s 47us/sample - loss: 1.3509 - accuracy: 0.4669\n",
      "Epoch 3/150\n",
      "514/514 [==============================] - 0s 52us/sample - loss: 1.1852 - accuracy: 0.4786\n",
      "Epoch 4/150\n",
      "514/514 [==============================] - 0s 50us/sample - loss: 1.0914 - accuracy: 0.5136\n",
      "Epoch 5/150\n",
      "514/514 [==============================] - 0s 61us/sample - loss: 1.0205 - accuracy: 0.5039\n",
      "Epoch 6/150\n",
      "514/514 [==============================] - 0s 50us/sample - loss: 0.9870 - accuracy: 0.5272\n",
      "Epoch 7/150\n",
      "514/514 [==============================] - 0s 46us/sample - loss: 0.9381 - accuracy: 0.5311\n",
      "Epoch 8/150\n",
      "514/514 [==============================] - 0s 47us/sample - loss: 0.8969 - accuracy: 0.5681\n",
      "Epoch 9/150\n",
      "514/514 [==============================] - 0s 57us/sample - loss: 0.8628 - accuracy: 0.5798\n",
      "Epoch 10/150\n",
      "514/514 [==============================] - 0s 54us/sample - loss: 0.8413 - accuracy: 0.6051\n",
      "Epoch 11/150\n",
      "514/514 [==============================] - 0s 45us/sample - loss: 0.8058 - accuracy: 0.6070\n",
      "Epoch 12/150\n",
      "514/514 [==============================] - 0s 54us/sample - loss: 0.7831 - accuracy: 0.6167\n",
      "Epoch 13/150\n",
      "514/514 [==============================] - 0s 53us/sample - loss: 0.7635 - accuracy: 0.6089\n",
      "Epoch 14/150\n",
      "514/514 [==============================] - 0s 51us/sample - loss: 0.7487 - accuracy: 0.6109\n",
      "Epoch 15/150\n",
      "514/514 [==============================] - 0s 45us/sample - loss: 0.7318 - accuracy: 0.6284\n",
      "Epoch 16/150\n",
      "514/514 [==============================] - 0s 50us/sample - loss: 0.7217 - accuracy: 0.6323\n",
      "Epoch 17/150\n",
      "514/514 [==============================] - 0s 55us/sample - loss: 0.7100 - accuracy: 0.6304\n",
      "Epoch 18/150\n",
      "514/514 [==============================] - 0s 51us/sample - loss: 0.7108 - accuracy: 0.6342\n",
      "Epoch 19/150\n",
      "514/514 [==============================] - 0s 58us/sample - loss: 0.7052 - accuracy: 0.6323\n",
      "Epoch 20/150\n",
      "514/514 [==============================] - 0s 54us/sample - loss: 0.6922 - accuracy: 0.6459\n",
      "Epoch 21/150\n",
      "514/514 [==============================] - 0s 47us/sample - loss: 0.6879 - accuracy: 0.6537\n",
      "Epoch 22/150\n",
      "514/514 [==============================] - 0s 62us/sample - loss: 0.6787 - accuracy: 0.6362\n",
      "Epoch 23/150\n",
      "514/514 [==============================] - 0s 51us/sample - loss: 0.6647 - accuracy: 0.6595\n",
      "Epoch 24/150\n",
      "514/514 [==============================] - 0s 49us/sample - loss: 0.6597 - accuracy: 0.6420\n",
      "Epoch 25/150\n",
      "514/514 [==============================] - 0s 49us/sample - loss: 0.6513 - accuracy: 0.6518\n",
      "Epoch 26/150\n",
      "514/514 [==============================] - 0s 54us/sample - loss: 0.6525 - accuracy: 0.6537\n",
      "Epoch 27/150\n",
      "514/514 [==============================] - 0s 47us/sample - loss: 0.6439 - accuracy: 0.6381\n",
      "Epoch 28/150\n",
      "514/514 [==============================] - 0s 44us/sample - loss: 0.6380 - accuracy: 0.6459\n",
      "Epoch 29/150\n",
      "514/514 [==============================] - 0s 52us/sample - loss: 0.6390 - accuracy: 0.6634\n",
      "Epoch 30/150\n",
      "514/514 [==============================] - 0s 66us/sample - loss: 0.6382 - accuracy: 0.6381\n",
      "Epoch 31/150\n",
      "514/514 [==============================] - 0s 53us/sample - loss: 0.6360 - accuracy: 0.6556\n",
      "Epoch 32/150\n",
      "514/514 [==============================] - 0s 49us/sample - loss: 0.6339 - accuracy: 0.6479\n",
      "Epoch 33/150\n",
      "514/514 [==============================] - 0s 45us/sample - loss: 0.6231 - accuracy: 0.6615\n",
      "Epoch 34/150\n",
      "514/514 [==============================] - 0s 56us/sample - loss: 0.6242 - accuracy: 0.6595\n",
      "Epoch 35/150\n",
      "514/514 [==============================] - 0s 46us/sample - loss: 0.6241 - accuracy: 0.6518\n",
      "Epoch 36/150\n",
      "514/514 [==============================] - 0s 56us/sample - loss: 0.6237 - accuracy: 0.6654\n",
      "Epoch 37/150\n",
      "514/514 [==============================] - 0s 52us/sample - loss: 0.6175 - accuracy: 0.6673\n",
      "Epoch 38/150\n",
      "514/514 [==============================] - 0s 47us/sample - loss: 0.6160 - accuracy: 0.6537\n",
      "Epoch 39/150\n",
      "514/514 [==============================] - 0s 48us/sample - loss: 0.6152 - accuracy: 0.6498\n",
      "Epoch 40/150\n",
      "514/514 [==============================] - 0s 51us/sample - loss: 0.6100 - accuracy: 0.6595\n",
      "Epoch 41/150\n",
      "514/514 [==============================] - 0s 49us/sample - loss: 0.6068 - accuracy: 0.6712\n",
      "Epoch 42/150\n",
      "514/514 [==============================] - 0s 49us/sample - loss: 0.6081 - accuracy: 0.6654\n",
      "Epoch 43/150\n",
      "514/514 [==============================] - 0s 49us/sample - loss: 0.6058 - accuracy: 0.6751\n",
      "Epoch 44/150\n",
      "514/514 [==============================] - 0s 56us/sample - loss: 0.6076 - accuracy: 0.6693\n",
      "Epoch 45/150\n",
      "514/514 [==============================] - 0s 56us/sample - loss: 0.5990 - accuracy: 0.6693\n",
      "Epoch 46/150\n",
      "514/514 [==============================] - 0s 49us/sample - loss: 0.6000 - accuracy: 0.6673\n",
      "Epoch 47/150\n",
      "514/514 [==============================] - 0s 49us/sample - loss: 0.6035 - accuracy: 0.6751\n",
      "Epoch 48/150\n",
      "514/514 [==============================] - 0s 54us/sample - loss: 0.5998 - accuracy: 0.6712\n",
      "Epoch 49/150\n",
      "514/514 [==============================] - 0s 51us/sample - loss: 0.5957 - accuracy: 0.6712\n",
      "Epoch 50/150\n",
      "514/514 [==============================] - 0s 58us/sample - loss: 0.5945 - accuracy: 0.6673\n",
      "Epoch 51/150\n",
      "514/514 [==============================] - 0s 47us/sample - loss: 0.5924 - accuracy: 0.6790\n",
      "Epoch 52/150\n",
      "514/514 [==============================] - 0s 44us/sample - loss: 0.5911 - accuracy: 0.6848\n",
      "Epoch 53/150\n",
      "514/514 [==============================] - 0s 54us/sample - loss: 0.5928 - accuracy: 0.6693\n",
      "Epoch 54/150\n",
      "514/514 [==============================] - 0s 54us/sample - loss: 0.6066 - accuracy: 0.6245\n",
      "Epoch 55/150\n",
      "514/514 [==============================] - 0s 56us/sample - loss: 0.5938 - accuracy: 0.6790\n",
      "Epoch 56/150\n",
      "514/514 [==============================] - 0s 44us/sample - loss: 0.5906 - accuracy: 0.6440\n",
      "Epoch 57/150\n",
      "514/514 [==============================] - 0s 59us/sample - loss: 0.5921 - accuracy: 0.6654\n",
      "Epoch 58/150\n",
      "514/514 [==============================] - 0s 50us/sample - loss: 0.5886 - accuracy: 0.6868\n",
      "Epoch 59/150\n",
      "514/514 [==============================] - 0s 46us/sample - loss: 0.5854 - accuracy: 0.6868\n",
      "Epoch 60/150\n",
      "514/514 [==============================] - 0s 52us/sample - loss: 0.5863 - accuracy: 0.6751\n",
      "Epoch 61/150\n",
      "514/514 [==============================] - 0s 55us/sample - loss: 0.5878 - accuracy: 0.6770\n",
      "Epoch 62/150\n",
      "514/514 [==============================] - 0s 51us/sample - loss: 0.5853 - accuracy: 0.6770\n",
      "Epoch 63/150\n",
      "514/514 [==============================] - 0s 56us/sample - loss: 0.5852 - accuracy: 0.6809\n",
      "Epoch 64/150\n",
      "514/514 [==============================] - 0s 46us/sample - loss: 0.5865 - accuracy: 0.6615\n",
      "Epoch 65/150\n",
      "514/514 [==============================] - 0s 44us/sample - loss: 0.5847 - accuracy: 0.6751\n",
      "Epoch 66/150\n",
      "514/514 [==============================] - 0s 53us/sample - loss: 0.5789 - accuracy: 0.6809\n",
      "Epoch 67/150\n",
      "514/514 [==============================] - 0s 55us/sample - loss: 0.5800 - accuracy: 0.6926\n",
      "Epoch 68/150\n",
      "514/514 [==============================] - 0s 43us/sample - loss: 0.5799 - accuracy: 0.7082\n",
      "Epoch 69/150\n",
      "514/514 [==============================] - 0s 51us/sample - loss: 0.5777 - accuracy: 0.6965\n",
      "Epoch 70/150\n",
      "514/514 [==============================] - 0s 49us/sample - loss: 0.5773 - accuracy: 0.6848\n",
      "Epoch 71/150\n",
      "514/514 [==============================] - 0s 48us/sample - loss: 0.5870 - accuracy: 0.6829\n",
      "Epoch 72/150\n",
      "514/514 [==============================] - 0s 47us/sample - loss: 0.5800 - accuracy: 0.6809\n",
      "Epoch 73/150\n",
      "514/514 [==============================] - 0s 51us/sample - loss: 0.5744 - accuracy: 0.6809\n",
      "Epoch 74/150\n",
      "514/514 [==============================] - 0s 50us/sample - loss: 0.5739 - accuracy: 0.6946\n",
      "Epoch 75/150\n",
      "514/514 [==============================] - 0s 51us/sample - loss: 0.5913 - accuracy: 0.6479\n",
      "Epoch 76/150\n",
      "514/514 [==============================] - 0s 48us/sample - loss: 0.5781 - accuracy: 0.6965\n",
      "Epoch 77/150\n",
      "514/514 [==============================] - 0s 53us/sample - loss: 0.5770 - accuracy: 0.6770\n",
      "Epoch 78/150\n",
      "514/514 [==============================] - 0s 45us/sample - loss: 0.5738 - accuracy: 0.7004\n",
      "Epoch 79/150\n",
      "514/514 [==============================] - 0s 54us/sample - loss: 0.5713 - accuracy: 0.6907\n",
      "Epoch 80/150\n",
      "514/514 [==============================] - 0s 45us/sample - loss: 0.5747 - accuracy: 0.6926\n",
      "Epoch 81/150\n",
      "514/514 [==============================] - 0s 59us/sample - loss: 0.5710 - accuracy: 0.7023\n",
      "Epoch 82/150\n",
      "514/514 [==============================] - 0s 48us/sample - loss: 0.5766 - accuracy: 0.6751\n",
      "Epoch 83/150\n",
      "514/514 [==============================] - 0s 45us/sample - loss: 0.5680 - accuracy: 0.6946\n",
      "Epoch 84/150\n",
      "514/514 [==============================] - 0s 64us/sample - loss: 0.5732 - accuracy: 0.7198\n",
      "Epoch 85/150\n",
      "514/514 [==============================] - 0s 55us/sample - loss: 0.5728 - accuracy: 0.6790\n",
      "Epoch 86/150\n",
      "514/514 [==============================] - 0s 59us/sample - loss: 0.5701 - accuracy: 0.6907\n",
      "Epoch 87/150\n",
      "514/514 [==============================] - 0s 48us/sample - loss: 0.5688 - accuracy: 0.6946\n",
      "Epoch 88/150\n",
      "514/514 [==============================] - 0s 51us/sample - loss: 0.5658 - accuracy: 0.7023\n",
      "Epoch 89/150\n",
      "514/514 [==============================] - 0s 45us/sample - loss: 0.5659 - accuracy: 0.6926\n",
      "Epoch 90/150\n",
      "514/514 [==============================] - 0s 52us/sample - loss: 0.5771 - accuracy: 0.6693\n",
      "Epoch 91/150\n",
      "514/514 [==============================] - 0s 52us/sample - loss: 0.5713 - accuracy: 0.7082\n",
      "Epoch 92/150\n",
      "514/514 [==============================] - 0s 51us/sample - loss: 0.5661 - accuracy: 0.6926\n",
      "Epoch 93/150\n",
      "514/514 [==============================] - 0s 57us/sample - loss: 0.5678 - accuracy: 0.6887\n",
      "Epoch 94/150\n",
      "514/514 [==============================] - 0s 56us/sample - loss: 0.5663 - accuracy: 0.7043\n",
      "Epoch 95/150\n",
      "514/514 [==============================] - 0s 49us/sample - loss: 0.5629 - accuracy: 0.7062\n",
      "Epoch 96/150\n",
      "514/514 [==============================] - 0s 58us/sample - loss: 0.5652 - accuracy: 0.7121\n",
      "Epoch 97/150\n",
      "514/514 [==============================] - 0s 46us/sample - loss: 0.5766 - accuracy: 0.6732\n",
      "Epoch 98/150\n",
      "514/514 [==============================] - 0s 58us/sample - loss: 0.5612 - accuracy: 0.7082\n",
      "Epoch 99/150\n",
      "514/514 [==============================] - 0s 50us/sample - loss: 0.5616 - accuracy: 0.7160\n",
      "Epoch 100/150\n",
      "514/514 [==============================] - 0s 45us/sample - loss: 0.5607 - accuracy: 0.7160\n",
      "Epoch 101/150\n",
      "514/514 [==============================] - 0s 56us/sample - loss: 0.5626 - accuracy: 0.6926\n",
      "Epoch 102/150\n",
      "514/514 [==============================] - 0s 50us/sample - loss: 0.5591 - accuracy: 0.7121\n",
      "Epoch 103/150\n",
      "514/514 [==============================] - 0s 49us/sample - loss: 0.5622 - accuracy: 0.7121\n",
      "Epoch 104/150\n",
      "514/514 [==============================] - 0s 49us/sample - loss: 0.5612 - accuracy: 0.6829\n",
      "Epoch 105/150\n",
      "514/514 [==============================] - 0s 44us/sample - loss: 0.5595 - accuracy: 0.7023\n",
      "Epoch 106/150\n",
      "514/514 [==============================] - 0s 46us/sample - loss: 0.5609 - accuracy: 0.6926\n",
      "Epoch 107/150\n",
      "514/514 [==============================] - 0s 58us/sample - loss: 0.5772 - accuracy: 0.7101\n",
      "Epoch 108/150\n",
      "514/514 [==============================] - 0s 50us/sample - loss: 0.5592 - accuracy: 0.7082\n",
      "Epoch 109/150\n",
      "514/514 [==============================] - 0s 54us/sample - loss: 0.5560 - accuracy: 0.7101\n",
      "Epoch 110/150\n",
      "514/514 [==============================] - 0s 42us/sample - loss: 0.5555 - accuracy: 0.7101\n",
      "Epoch 111/150\n",
      "514/514 [==============================] - 0s 42us/sample - loss: 0.5550 - accuracy: 0.7101\n",
      "Epoch 112/150\n",
      "514/514 [==============================] - 0s 45us/sample - loss: 0.5592 - accuracy: 0.7140\n",
      "Epoch 113/150\n",
      "514/514 [==============================] - 0s 44us/sample - loss: 0.5550 - accuracy: 0.7179\n",
      "Epoch 114/150\n",
      "514/514 [==============================] - 0s 50us/sample - loss: 0.5570 - accuracy: 0.7276\n",
      "Epoch 115/150\n",
      "514/514 [==============================] - 0s 50us/sample - loss: 0.5617 - accuracy: 0.6946\n",
      "Epoch 116/150\n",
      "514/514 [==============================] - 0s 51us/sample - loss: 0.5558 - accuracy: 0.7198\n",
      "Epoch 117/150\n",
      "514/514 [==============================] - 0s 48us/sample - loss: 0.5527 - accuracy: 0.7140\n",
      "Epoch 118/150\n",
      "514/514 [==============================] - 0s 46us/sample - loss: 0.5605 - accuracy: 0.7043\n",
      "Epoch 119/150\n",
      "514/514 [==============================] - 0s 48us/sample - loss: 0.5592 - accuracy: 0.7179\n",
      "Epoch 120/150\n",
      "514/514 [==============================] - 0s 48us/sample - loss: 0.5525 - accuracy: 0.7160\n",
      "Epoch 121/150\n",
      "514/514 [==============================] - 0s 51us/sample - loss: 0.5572 - accuracy: 0.7160\n",
      "Epoch 122/150\n",
      "514/514 [==============================] - 0s 52us/sample - loss: 0.5514 - accuracy: 0.7198\n",
      "Epoch 123/150\n",
      "514/514 [==============================] - 0s 54us/sample - loss: 0.5554 - accuracy: 0.7198\n",
      "Epoch 124/150\n",
      "514/514 [==============================] - 0s 47us/sample - loss: 0.5513 - accuracy: 0.7218\n",
      "Epoch 125/150\n",
      "514/514 [==============================] - 0s 51us/sample - loss: 0.5504 - accuracy: 0.7179\n",
      "Epoch 126/150\n",
      "514/514 [==============================] - 0s 43us/sample - loss: 0.5511 - accuracy: 0.7198\n",
      "Epoch 127/150\n",
      "514/514 [==============================] - 0s 51us/sample - loss: 0.5509 - accuracy: 0.7140\n",
      "Epoch 128/150\n",
      "514/514 [==============================] - 0s 48us/sample - loss: 0.5563 - accuracy: 0.7140\n",
      "Epoch 129/150\n",
      "514/514 [==============================] - 0s 51us/sample - loss: 0.5502 - accuracy: 0.7179\n",
      "Epoch 130/150\n",
      "514/514 [==============================] - 0s 55us/sample - loss: 0.5494 - accuracy: 0.7198\n",
      "Epoch 131/150\n",
      "514/514 [==============================] - 0s 48us/sample - loss: 0.5531 - accuracy: 0.7043\n",
      "Epoch 132/150\n",
      "514/514 [==============================] - 0s 49us/sample - loss: 0.5473 - accuracy: 0.7160\n",
      "Epoch 133/150\n",
      "514/514 [==============================] - 0s 46us/sample - loss: 0.5566 - accuracy: 0.7296\n",
      "Epoch 134/150\n",
      "514/514 [==============================] - 0s 46us/sample - loss: 0.5609 - accuracy: 0.6984\n",
      "Epoch 135/150\n",
      "514/514 [==============================] - 0s 50us/sample - loss: 0.5492 - accuracy: 0.7198\n",
      "Epoch 136/150\n",
      "514/514 [==============================] - 0s 50us/sample - loss: 0.5538 - accuracy: 0.7218\n",
      "Epoch 137/150\n",
      "514/514 [==============================] - 0s 47us/sample - loss: 0.5478 - accuracy: 0.7179\n",
      "Epoch 138/150\n",
      "514/514 [==============================] - 0s 48us/sample - loss: 0.5472 - accuracy: 0.7160\n",
      "Epoch 139/150\n",
      "514/514 [==============================] - 0s 48us/sample - loss: 0.5444 - accuracy: 0.7218\n",
      "Epoch 140/150\n",
      "514/514 [==============================] - 0s 58us/sample - loss: 0.5481 - accuracy: 0.7198\n",
      "Epoch 141/150\n",
      "514/514 [==============================] - 0s 52us/sample - loss: 0.5499 - accuracy: 0.7121\n",
      "Epoch 142/150\n",
      "514/514 [==============================] - 0s 45us/sample - loss: 0.5447 - accuracy: 0.7257\n",
      "Epoch 143/150\n",
      "514/514 [==============================] - 0s 51us/sample - loss: 0.5495 - accuracy: 0.7198\n",
      "Epoch 144/150\n",
      "514/514 [==============================] - 0s 61us/sample - loss: 0.5535 - accuracy: 0.7335\n",
      "Epoch 145/150\n",
      "514/514 [==============================] - 0s 52us/sample - loss: 0.5468 - accuracy: 0.7140\n",
      "Epoch 146/150\n",
      "514/514 [==============================] - 0s 48us/sample - loss: 0.5451 - accuracy: 0.7237\n",
      "Epoch 147/150\n",
      "514/514 [==============================] - 0s 53us/sample - loss: 0.5431 - accuracy: 0.7335\n",
      "Epoch 148/150\n",
      "514/514 [==============================] - 0s 41us/sample - loss: 0.5489 - accuracy: 0.7179\n",
      "Epoch 149/150\n",
      "514/514 [==============================] - 0s 54us/sample - loss: 0.5455 - accuracy: 0.7160\n",
      "Epoch 150/150\n",
      "514/514 [==============================] - 0s 51us/sample - loss: 0.5432 - accuracy: 0.7276\n"
     ]
    }
   ],
   "source": [
    "### START CODING HERE ###\n",
    "# Fit nn_clf model on train_batch with 150 epochs\n",
    "nn_clf_history = nn_clf.fit(X_train, y_train, epochs=150)\n",
    "### END CODING HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OiPghb3K21Ko"
   },
   "source": [
    "> `history` now contains values for `loss` and `accuracy` during training, let's plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "colab_type": "code",
    "id": "-ruZWGIs21Kp",
    "outputId": "ef33ee2e-3074-4a34-b43f-2851168f7dca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAEzCAYAAAAVXYYvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(nn_clf_history.history).plot(figsize=(10, 5))\n",
    "plt.grid(True)\n",
    "\n",
    "# set the y-axis range to [0-1]\n",
    "plt.gca().set_ylim(0, 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0eAp9gE21Ku"
   },
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "1ssaI5CE21Kv",
    "outputId": "e63ec376-6107-4c25-bbcc-f3b4867dd6b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/254 [==============================] - 0s 1ms/step - loss: 0.5908 - accuracy: 0.7441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5908294675493531, 0.7440945]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### START CODING HERE ###\n",
    "# evaluate nn_clf model on test_batch using .evaluate method\n",
    "nn_clf.evaluate(test_batch)\n",
    "### END CODING HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "SAfb1xlD21Kz",
    "outputId": "39f829d3-4b02-44c7-81d6-c89ed9e4113c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn_clf.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xXpxh53621K3"
   },
   "source": [
    "> Now, let's plot ROC curve for this neural network model. Recall from Assignment-4 that you need to get class probabilities, fpr and tpr. To get class probabilities, keras has `predict()` method. Notice that it's applied on `X_test` not `test_batch`. Alternatively, you can use `predict_proba()` method, similar to sklearn, which would generate identical results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "wKjXeyV721K4",
    "outputId": "fea59c36-917f-46b7-868d-14b82319974b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "# Get class probabilities for nn - ignore the warning\n",
    "nn_preds = nn_clf.predict(X_test).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "ibi65q8621LB",
    "outputId": "5299b085-592b-4790-a376-d459335d1514"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12502682, 0.14913681, 0.2097309 , 0.36301482, 0.00806487],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See class probabilities predicted by nn classifier\n",
    "nn_preds[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YafkeW_Q21LK"
   },
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "11sjLcLv21LL",
    "outputId": "0ad0029f-1b42-4868-b91d-090160c72e09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'False Positive Rate')"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### START CODING HERE ### \n",
    "# Plot ROC curve for nn_clf - Write as many lines of code as needed\n",
    "# Hint: check back your Assignment-4 code, you need to calculate tpr, fpr, thresholds\n",
    "# Plot should have all the elements that Assignment-4 ROC curves had, title, xlabel and ylabel, xlim & ylim\n",
    "# Plot should also have AUC_NN (roc_auc) shown on lower right\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, nn_preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "plt.title('ROC')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC_RF = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "### END CODING HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4817UJSP21LQ"
   },
   "source": [
    "## Part I - Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bBdhnyK021LR"
   },
   "source": [
    "<b>ANSWER THE FOLLOWING QUESTIONS HERE:</b><br>\n",
    "\n",
    "Q1 - If this problem was a multi-class classification, what activation function would you use for the output layer neurons? How many neurons would be required for the output layer for multi-class? What other hyperparameters of nn can you change in different tasks? Name at least 3 other hyperparameters. GIVE COMPLETE ANSWER!\n",
    "\n",
    "**For a multi-class classification, I would still use a sigmoid activation function. The output layer would require n neurons, where n is the number of classes that you designate in your one-vs-all design. For other classification tasks, you could always change the batch size, the number of layers in your neural network, and the number of units in each respective dense layer.**\n",
    "\n",
    "\n",
    "Q2 - Change the batch number from 1 to 10. Would it improve or hurt the results? Make an argument with reasoning on your observation (you may consult with page 326 of the textbook).\n",
    "\n",
    "**Increasing batch size tends to improve results. When you train a neural network with a larger batch (but still less than the size of the sample), you gain the benefit of having a better estimated gradient, and if you adjust the batch size accordingly, you can have better estimates (less jumpy), as well as extra updates to the network. The only tradeoff is in memory usage and speed when a batch size increases.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g85OxiJk21LT"
   },
   "source": [
    "## Part II - Regression with NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "npfaKHWL21LU"
   },
   "source": [
    "In this part, you will create a neural network to do a regression task.\n",
    "\n",
    "[Download the data from here](https://github.com/fereydoonvafaei/UMBC-CMSC-471-Fall-2019/blob/master/Assignment-5/auto.csv). This is cars dataset. You can read more about the data [here](https://archive.ics.uci.edu/ml/datasets/auto+mpg). The goal is predicting MPG based on other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "DktbMSNx21LW",
    "outputId": "81acfc0d-4033-4f28-c074-c77c454216ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(398, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MPG</th>\n",
       "      <th>Cylinders</th>\n",
       "      <th>Displacement</th>\n",
       "      <th>Horsepower</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Acceleration</th>\n",
       "      <th>Model Year</th>\n",
       "      <th>Origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    MPG  Cylinders  Displacement  ...  Acceleration  Model Year  Origin\n",
       "0  18.0          8         307.0  ...          12.0          70       1\n",
       "1  15.0          8         350.0  ...          11.5          70       1\n",
       "2  18.0          8         318.0  ...          11.0          70       1\n",
       "3  16.0          8         304.0  ...          12.0          70       1\n",
       "4  17.0          8         302.0  ...          10.5          70       1\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_data = pd.read_csv('auto.csv')\n",
    "print(auto_data.shape)\n",
    "auto_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "jJ6nAcJl21LZ",
    "outputId": "35a87492-8f3f-4554-b5e4-e335a9d83074"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPG             0\n",
       "Cylinders       0\n",
       "Displacement    0\n",
       "Horsepower      6\n",
       "Weight          0\n",
       "Acceleration    0\n",
       "Model Year      0\n",
       "Origin          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQu6I20D21Ld"
   },
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "TVAUxsk521Le",
    "outputId": "ffe0e439-5014-446c-bfbe-e019b380eb61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPG             0\n",
       "Cylinders       0\n",
       "Displacement    0\n",
       "Horsepower      0\n",
       "Weight          0\n",
       "Acceleration    0\n",
       "Model Year      0\n",
       "Origin          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### START CODING HERE ### \n",
    "# Drop all na's using dataframe .dropna(inplace=True) method ~ 1 line\n",
    "auto_data.dropna(inplace=True)\n",
    "### END CODING HERE ###\n",
    "auto_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2RpjDlHN21Lh"
   },
   "source": [
    ">For some datasets like this, a technique in data preprocessing is used to encode categorical features to dummy variables. Here, we convert <b>Origin</b> (which looks numeric but is actually categorical) using pandas [get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) method. This technique is one example of One Hot Encoding of categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "iNfT4nVj21Li",
    "outputId": "5e2f1ccf-b50e-47ad-9f24-1082e70745d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(392, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MPG</th>\n",
       "      <th>Cylinders</th>\n",
       "      <th>Displacement</th>\n",
       "      <th>Horsepower</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Acceleration</th>\n",
       "      <th>Model Year</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Japan</th>\n",
       "      <th>USA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    MPG  Cylinders  Displacement  Horsepower  ...  Model Year  Europe  Japan  USA\n",
       "0  18.0          8         307.0       130.0  ...          70       0      0    1\n",
       "1  15.0          8         350.0       165.0  ...          70       0      0    1\n",
       "2  18.0          8         318.0       150.0  ...          70       0      0    1\n",
       "3  16.0          8         304.0       150.0  ...          70       0      0    1\n",
       "4  17.0          8         302.0       140.0  ...          70       0      0    1\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_data['Origin'] = auto_data['Origin'].map(lambda x: {1: 'USA', 2: 'Europe', 3: 'Japan'}.get(x))\n",
    "auto_data = pd.get_dummies(auto_data, prefix='', prefix_sep='')\n",
    "print(auto_data.shape)\n",
    "auto_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5QZ0VTnP21Ll"
   },
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "colab_type": "code",
    "id": "OKnJx8OM21Lm",
    "outputId": "a3b8d4f8-6e8c-4ef7-a9ca-05712e88949a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(392, 9)\n",
      "(392,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cylinders</th>\n",
       "      <th>Displacement</th>\n",
       "      <th>Horsepower</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Acceleration</th>\n",
       "      <th>Model Year</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Japan</th>\n",
       "      <th>USA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cylinders  Displacement  Horsepower  Weight  ...  Model Year  Europe  Japan  USA\n",
       "0          8         307.0       130.0  3504.0  ...          70       0      0    1\n",
       "1          8         350.0       165.0  3693.0  ...          70       0      0    1\n",
       "2          8         318.0       150.0  3436.0  ...          70       0      0    1\n",
       "3          8         304.0       150.0  3433.0  ...          70       0      0    1\n",
       "4          8         302.0       140.0  3449.0  ...          70       0      0    1\n",
       "\n",
       "[5 rows x 9 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### START CODING HERE ### \n",
    "# Create X and y, X should contain all features except MPG column and y should only contain MPG column\n",
    "# Hint: You can use dataframe .pop() method, but you may need to create a deep copy of the dataframe first\n",
    "# There are usually multiple ways of doing these operations in pnadas dataframes\n",
    "X = auto_data.drop(['MPG'], axis=1)\n",
    "y = auto_data['MPG']\n",
    "### END CODING HERE ###\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CqHFXsz821Lq"
   },
   "source": [
    "> <b>Note:</b> The original auto_data dataframe should remain the same and should still include MPG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "jNTeZt6z21Lr",
    "outputId": "55fdbe8f-e7e3-40c7-b62f-408a36ba2b5b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MPG</th>\n",
       "      <th>Cylinders</th>\n",
       "      <th>Displacement</th>\n",
       "      <th>Horsepower</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Acceleration</th>\n",
       "      <th>Model Year</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Japan</th>\n",
       "      <th>USA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    MPG  Cylinders  Displacement  Horsepower  ...  Model Year  Europe  Japan  USA\n",
       "0  18.0          8         307.0       130.0  ...          70       0      0    1\n",
       "1  15.0          8         350.0       165.0  ...          70       0      0    1\n",
       "2  18.0          8         318.0       150.0  ...          70       0      0    1\n",
       "3  16.0          8         304.0       150.0  ...          70       0      0    1\n",
       "4  17.0          8         302.0       140.0  ...          70       0      0    1\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H-jebXc621Lv"
   },
   "source": [
    ">Normalization is a good pratice when you work with values with different ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "bzPWkq6-21Lw",
    "outputId": "ba1f6b5f-00b2-48ab-ec2e-b4a325bd4d57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(392, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cylinders</th>\n",
       "      <th>Displacement</th>\n",
       "      <th>Horsepower</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Acceleration</th>\n",
       "      <th>Model Year</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Japan</th>\n",
       "      <th>USA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.482053</td>\n",
       "      <td>1.075915</td>\n",
       "      <td>0.663285</td>\n",
       "      <td>0.619748</td>\n",
       "      <td>-1.283618</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>-0.457538</td>\n",
       "      <td>-0.501749</td>\n",
       "      <td>0.773608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.482053</td>\n",
       "      <td>1.486832</td>\n",
       "      <td>1.572585</td>\n",
       "      <td>0.842258</td>\n",
       "      <td>-1.464852</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>-0.457538</td>\n",
       "      <td>-0.501749</td>\n",
       "      <td>0.773608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.482053</td>\n",
       "      <td>1.181033</td>\n",
       "      <td>1.182885</td>\n",
       "      <td>0.539692</td>\n",
       "      <td>-1.646086</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>-0.457538</td>\n",
       "      <td>-0.501749</td>\n",
       "      <td>0.773608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.482053</td>\n",
       "      <td>1.047246</td>\n",
       "      <td>1.182885</td>\n",
       "      <td>0.536160</td>\n",
       "      <td>-1.283618</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>-0.457538</td>\n",
       "      <td>-0.501749</td>\n",
       "      <td>0.773608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.482053</td>\n",
       "      <td>1.028134</td>\n",
       "      <td>0.923085</td>\n",
       "      <td>0.554997</td>\n",
       "      <td>-1.827320</td>\n",
       "      <td>-1.623241</td>\n",
       "      <td>-0.457538</td>\n",
       "      <td>-0.501749</td>\n",
       "      <td>0.773608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cylinders  Displacement  Horsepower  ...    Europe     Japan       USA\n",
       "0   1.482053      1.075915    0.663285  ... -0.457538 -0.501749  0.773608\n",
       "1   1.482053      1.486832    1.572585  ... -0.457538 -0.501749  0.773608\n",
       "2   1.482053      1.181033    1.182885  ... -0.457538 -0.501749  0.773608\n",
       "3   1.482053      1.047246    1.182885  ... -0.457538 -0.501749  0.773608\n",
       "4   1.482053      1.028134    0.923085  ... -0.457538 -0.501749  0.773608\n",
       "\n",
       "[5 rows x 9 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize X \n",
    "X = (X - X.mean())/X.std()\n",
    "print(X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RE9h30jU21Lz"
   },
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "QAcSOFe621L0",
    "outputId": "e310dd2a-418e-495f-839e-4657fc645379"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(313, 9)\n",
      "(313,)\n",
      "(79, 9)\n",
      "(79,)\n"
     ]
    }
   ],
   "source": [
    "### START CODING HERE ###\n",
    "# Split the data to train and test using train_test_split method with test_size=0.2 and random_state=42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "### END CODING HERE ###\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RE8djAQP21L4"
   },
   "outputs": [],
   "source": [
    "### START CODING HERE ### \n",
    "# Build a neural network for regression\n",
    "nn_reg1 = tf.keras.Sequential([\n",
    "    # Create a dense layer with 64 neurons, 'relu' activation function and input_shape=[len(X_train.keys())]\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=[len(X_train.keys())]),\n",
    "    # Create a dense layer with 64 neurons and 'relu' activation function\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    # Create a dense layer with ? neuron(s) and ? activation\n",
    "    # YOU should decide how many neuron(s) is/are needed and what activation function (if any) to use for output\n",
    "    # Hint: What type of ML task is this problem?\n",
    "    tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "### END CODING HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zpZ3Si4l21L7"
   },
   "outputs": [],
   "source": [
    "# This is another way of defining the optimizer, you can pass learning_rate and other parameters to it.\n",
    "optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "### START CODING HERE ### \n",
    "# Compile nn_reg1 with 'mse' loss, optimizer=optimizer, metrics=['mae', 'mse']\n",
    "nn_reg1.compile(loss='mse', optimizer=optimizer, metrics=['mae', 'mse'])\n",
    "### END CODING HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "3C6ql28J21L-",
    "outputId": "8b4a09be-2832-426f-cca8-a5ba52013613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 64)                640       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 4,865\n",
      "Trainable params: 4,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn_reg1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2HqqLmXI21MD"
   },
   "source": [
    "## <font color=\"red\"> Required Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "aKb7YYyI21MF",
    "outputId": "9f5e1c3d-e7e7-4a4d-a0a6-815e253f991e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 250 samples, validate on 63 samples\n",
      "Epoch 1/1000\n",
      "250/250 [==============================] - 0s 528us/sample - loss: 589.2738 - mae: 22.9176 - mse: 589.2738 - val_loss: 637.3457 - val_mae: 24.0202 - val_mse: 637.3456\n",
      "Epoch 2/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 533.4516 - mae: 21.7111 - mse: 533.4515 - val_loss: 581.4424 - val_mae: 22.8746 - val_mse: 581.4424\n",
      "Epoch 3/1000\n",
      "250/250 [==============================] - 0s 88us/sample - loss: 479.3911 - mae: 20.5091 - mse: 479.3911 - val_loss: 520.2952 - val_mae: 21.5716 - val_mse: 520.2952\n",
      "Epoch 4/1000\n",
      "250/250 [==============================] - 0s 78us/sample - loss: 419.8520 - mae: 19.1260 - mse: 419.8520 - val_loss: 451.5813 - val_mae: 20.0171 - val_mse: 451.5813\n",
      "Epoch 5/1000\n",
      "250/250 [==============================] - 0s 101us/sample - loss: 357.4414 - mae: 17.5246 - mse: 357.4414 - val_loss: 383.2041 - val_mae: 18.3253 - val_mse: 383.2040\n",
      "Epoch 6/1000\n",
      "250/250 [==============================] - 0s 81us/sample - loss: 293.9781 - mae: 15.7691 - mse: 293.9781 - val_loss: 312.1635 - val_mae: 16.3772 - val_mse: 312.1635\n",
      "Epoch 7/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 232.4072 - mae: 13.8228 - mse: 232.4072 - val_loss: 246.1894 - val_mae: 14.3186 - val_mse: 246.1894\n",
      "Epoch 8/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 174.6594 - mae: 11.7560 - mse: 174.6594 - val_loss: 182.6437 - val_mae: 12.0580 - val_mse: 182.6437\n",
      "Epoch 9/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 124.8812 - mae: 9.6938 - mse: 124.8812 - val_loss: 131.3210 - val_mae: 9.9525 - val_mse: 131.3210\n",
      "Epoch 10/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 86.2859 - mae: 7.7313 - mse: 86.2859 - val_loss: 92.9227 - val_mae: 7.9893 - val_mse: 92.9228\n",
      "Epoch 11/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 59.8938 - mae: 6.2212 - mse: 59.8938 - val_loss: 66.5153 - val_mae: 6.3942 - val_mse: 66.5153\n",
      "Epoch 12/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 44.2605 - mae: 5.1934 - mse: 44.2605 - val_loss: 50.9785 - val_mae: 5.5752 - val_mse: 50.9785\n",
      "Epoch 13/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 34.8002 - mae: 4.6073 - mse: 34.8002 - val_loss: 40.0311 - val_mae: 4.9716 - val_mse: 40.0311\n",
      "Epoch 14/1000\n",
      "250/250 [==============================] - 0s 83us/sample - loss: 28.2869 - mae: 4.0960 - mse: 28.2869 - val_loss: 32.5938 - val_mae: 4.5202 - val_mse: 32.5938\n",
      "Epoch 15/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 23.6129 - mae: 3.7739 - mse: 23.6129 - val_loss: 26.5902 - val_mae: 4.0675 - val_mse: 26.5902\n",
      "Epoch 16/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 20.3531 - mae: 3.5269 - mse: 20.3531 - val_loss: 22.3985 - val_mae: 3.6643 - val_mse: 22.3985\n",
      "Epoch 17/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 17.8683 - mae: 3.3011 - mse: 17.8683 - val_loss: 19.7171 - val_mae: 3.4195 - val_mse: 19.7171\n",
      "Epoch 18/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 15.9614 - mae: 3.1138 - mse: 15.9614 - val_loss: 17.4151 - val_mae: 3.2192 - val_mse: 17.4151\n",
      "Epoch 19/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 14.5401 - mae: 2.9692 - mse: 14.5401 - val_loss: 15.6547 - val_mae: 3.0500 - val_mse: 15.6547\n",
      "Epoch 20/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 13.2430 - mae: 2.8396 - mse: 13.2430 - val_loss: 13.3739 - val_mae: 2.7215 - val_mse: 13.3739\n",
      "Epoch 21/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 12.1304 - mae: 2.6791 - mse: 12.1304 - val_loss: 13.2082 - val_mae: 2.7775 - val_mse: 13.2082\n",
      "Epoch 22/1000\n",
      "250/250 [==============================] - 0s 84us/sample - loss: 11.6661 - mae: 2.5991 - mse: 11.6661 - val_loss: 11.1015 - val_mae: 2.4727 - val_mse: 11.1015\n",
      "Epoch 23/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 10.8931 - mae: 2.4784 - mse: 10.8931 - val_loss: 9.9866 - val_mae: 2.3169 - val_mse: 9.9866\n",
      "Epoch 24/1000\n",
      "250/250 [==============================] - 0s 82us/sample - loss: 10.1418 - mae: 2.3762 - mse: 10.1418 - val_loss: 10.1900 - val_mae: 2.3650 - val_mse: 10.1900\n",
      "Epoch 25/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 9.9566 - mae: 2.3355 - mse: 9.9566 - val_loss: 8.9189 - val_mae: 2.2017 - val_mse: 8.9189\n",
      "Epoch 26/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 9.2116 - mae: 2.2342 - mse: 9.2116 - val_loss: 8.5398 - val_mae: 2.1482 - val_mse: 8.5398\n",
      "Epoch 27/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 9.2070 - mae: 2.2277 - mse: 9.2070 - val_loss: 8.7379 - val_mae: 2.1557 - val_mse: 8.7379\n",
      "Epoch 28/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 8.7205 - mae: 2.1462 - mse: 8.7205 - val_loss: 7.8273 - val_mae: 2.0378 - val_mse: 7.8273\n",
      "Epoch 29/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 8.6453 - mae: 2.1542 - mse: 8.6453 - val_loss: 7.7381 - val_mae: 2.0142 - val_mse: 7.7381\n",
      "Epoch 30/1000\n",
      "250/250 [==============================] - 0s 81us/sample - loss: 8.5275 - mae: 2.1252 - mse: 8.5275 - val_loss: 8.1777 - val_mae: 2.0529 - val_mse: 8.1777\n",
      "Epoch 31/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 8.3329 - mae: 2.0824 - mse: 8.3329 - val_loss: 8.2612 - val_mae: 2.0698 - val_mse: 8.2612\n",
      "Epoch 32/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 7.9703 - mae: 2.0299 - mse: 7.9703 - val_loss: 7.3892 - val_mae: 1.9636 - val_mse: 7.3892\n",
      "Epoch 33/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 8.0660 - mae: 2.0452 - mse: 8.0660 - val_loss: 7.6619 - val_mae: 1.9876 - val_mse: 7.6619\n",
      "Epoch 34/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 7.8776 - mae: 2.0244 - mse: 7.8776 - val_loss: 7.2130 - val_mae: 1.9286 - val_mse: 7.2130\n",
      "Epoch 35/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 7.7224 - mae: 2.0089 - mse: 7.7224 - val_loss: 8.2698 - val_mae: 2.0750 - val_mse: 8.2698\n",
      "Epoch 36/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 7.8679 - mae: 2.0099 - mse: 7.8679 - val_loss: 8.0540 - val_mae: 2.0536 - val_mse: 8.0540\n",
      "Epoch 37/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 7.6438 - mae: 1.9828 - mse: 7.6438 - val_loss: 7.7019 - val_mae: 2.0120 - val_mse: 7.7019\n",
      "Epoch 38/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 7.7173 - mae: 1.9998 - mse: 7.7173 - val_loss: 6.9272 - val_mae: 1.9033 - val_mse: 6.9272\n",
      "Epoch 39/1000\n",
      "250/250 [==============================] - 0s 78us/sample - loss: 7.6460 - mae: 2.0029 - mse: 7.6460 - val_loss: 7.8031 - val_mae: 2.0223 - val_mse: 7.8031\n",
      "Epoch 40/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 7.7235 - mae: 1.9908 - mse: 7.7235 - val_loss: 8.0718 - val_mae: 2.0540 - val_mse: 8.0718\n",
      "Epoch 41/1000\n",
      "250/250 [==============================] - 0s 78us/sample - loss: 7.6162 - mae: 1.9854 - mse: 7.6162 - val_loss: 7.4442 - val_mae: 1.9727 - val_mse: 7.4442\n",
      "Epoch 42/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 7.3376 - mae: 1.9528 - mse: 7.3376 - val_loss: 7.0773 - val_mae: 1.9284 - val_mse: 7.0773\n",
      "Epoch 43/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 7.1265 - mae: 1.9152 - mse: 7.1265 - val_loss: 9.0232 - val_mae: 2.2146 - val_mse: 9.0232\n",
      "Epoch 44/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 7.5010 - mae: 1.9503 - mse: 7.5010 - val_loss: 7.4418 - val_mae: 1.9727 - val_mse: 7.4418\n",
      "Epoch 45/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 7.2624 - mae: 1.9346 - mse: 7.2624 - val_loss: 6.6049 - val_mae: 1.8806 - val_mse: 6.6049\n",
      "Epoch 46/1000\n",
      "250/250 [==============================] - 0s 85us/sample - loss: 7.2182 - mae: 1.9278 - mse: 7.2182 - val_loss: 7.5469 - val_mae: 1.9945 - val_mse: 7.5469\n",
      "Epoch 47/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 7.1547 - mae: 1.9146 - mse: 7.1547 - val_loss: 6.6170 - val_mae: 1.8655 - val_mse: 6.6170\n",
      "Epoch 48/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 7.2034 - mae: 1.9183 - mse: 7.2034 - val_loss: 7.7302 - val_mae: 2.0259 - val_mse: 7.7302\n",
      "Epoch 49/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 7.2364 - mae: 1.9125 - mse: 7.2364 - val_loss: 7.4595 - val_mae: 1.9776 - val_mse: 7.4595\n",
      "Epoch 50/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 7.1144 - mae: 1.9357 - mse: 7.1144 - val_loss: 8.5665 - val_mae: 2.1643 - val_mse: 8.5665\n",
      "Epoch 51/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 7.0980 - mae: 1.9461 - mse: 7.0980 - val_loss: 7.6308 - val_mae: 2.0107 - val_mse: 7.6308\n",
      "Epoch 52/1000\n",
      "250/250 [==============================] - 0s 83us/sample - loss: 7.0363 - mae: 1.8682 - mse: 7.0363 - val_loss: 6.5705 - val_mae: 1.8745 - val_mse: 6.5705\n",
      "Epoch 53/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 7.1309 - mae: 1.8932 - mse: 7.1309 - val_loss: 6.7102 - val_mae: 1.9013 - val_mse: 6.7102\n",
      "Epoch 54/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 7.1090 - mae: 1.8954 - mse: 7.1090 - val_loss: 7.0236 - val_mae: 1.9330 - val_mse: 7.0236\n",
      "Epoch 55/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 6.9737 - mae: 1.8749 - mse: 6.9737 - val_loss: 6.6705 - val_mae: 1.9068 - val_mse: 6.6705\n",
      "Epoch 56/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 6.9369 - mae: 1.8793 - mse: 6.9369 - val_loss: 7.5272 - val_mae: 1.9947 - val_mse: 7.5272\n",
      "Epoch 57/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 6.9534 - mae: 1.8632 - mse: 6.9534 - val_loss: 7.4887 - val_mae: 1.9900 - val_mse: 7.4887\n",
      "Epoch 58/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 6.8008 - mae: 1.8353 - mse: 6.8008 - val_loss: 7.0108 - val_mae: 1.9522 - val_mse: 7.0108\n",
      "Epoch 59/1000\n",
      "250/250 [==============================] - 0s 101us/sample - loss: 6.8132 - mae: 1.8470 - mse: 6.8132 - val_loss: 6.6270 - val_mae: 1.9100 - val_mse: 6.6270\n",
      "Epoch 60/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 6.7538 - mae: 1.8473 - mse: 6.7538 - val_loss: 7.0062 - val_mae: 1.9281 - val_mse: 7.0062\n",
      "Epoch 61/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 6.6703 - mae: 1.8209 - mse: 6.6703 - val_loss: 6.3398 - val_mae: 1.8473 - val_mse: 6.3398\n",
      "Epoch 62/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 6.6237 - mae: 1.8340 - mse: 6.6237 - val_loss: 6.9119 - val_mae: 1.9345 - val_mse: 6.9119\n",
      "Epoch 63/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 6.8144 - mae: 1.8462 - mse: 6.8144 - val_loss: 7.1258 - val_mae: 1.9461 - val_mse: 7.1258\n",
      "Epoch 64/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 6.7082 - mae: 1.8618 - mse: 6.7082 - val_loss: 7.0097 - val_mae: 1.9432 - val_mse: 7.0097\n",
      "Epoch 65/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 6.5538 - mae: 1.8072 - mse: 6.5538 - val_loss: 6.2932 - val_mae: 1.8424 - val_mse: 6.2932\n",
      "Epoch 66/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 6.5909 - mae: 1.8294 - mse: 6.5909 - val_loss: 6.7071 - val_mae: 1.9084 - val_mse: 6.7071\n",
      "Epoch 67/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 6.6339 - mae: 1.8436 - mse: 6.6339 - val_loss: 8.1264 - val_mae: 2.1124 - val_mse: 8.1264\n",
      "Epoch 68/1000\n",
      "250/250 [==============================] - 0s 97us/sample - loss: 6.7095 - mae: 1.8323 - mse: 6.7095 - val_loss: 6.5860 - val_mae: 1.8955 - val_mse: 6.5860\n",
      "Epoch 69/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 6.6857 - mae: 1.8379 - mse: 6.6857 - val_loss: 6.8683 - val_mae: 1.9285 - val_mse: 6.8683\n",
      "Epoch 70/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 6.6510 - mae: 1.8512 - mse: 6.6510 - val_loss: 6.5798 - val_mae: 1.9049 - val_mse: 6.5798\n",
      "Epoch 71/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 6.6273 - mae: 1.8168 - mse: 6.6273 - val_loss: 7.0184 - val_mae: 1.9479 - val_mse: 7.0184\n",
      "Epoch 72/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 6.6641 - mae: 1.8211 - mse: 6.6641 - val_loss: 7.3005 - val_mae: 1.9745 - val_mse: 7.3005\n",
      "Epoch 73/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 6.4670 - mae: 1.8066 - mse: 6.4670 - val_loss: 6.2264 - val_mae: 1.8422 - val_mse: 6.2264\n",
      "Epoch 74/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 6.5053 - mae: 1.8082 - mse: 6.5053 - val_loss: 6.2146 - val_mae: 1.8566 - val_mse: 6.2146\n",
      "Epoch 75/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 6.4127 - mae: 1.7825 - mse: 6.4127 - val_loss: 7.6287 - val_mae: 2.0156 - val_mse: 7.6287\n",
      "Epoch 76/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 6.3319 - mae: 1.7988 - mse: 6.3319 - val_loss: 7.9911 - val_mae: 2.0951 - val_mse: 7.9911\n",
      "Epoch 77/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 6.5466 - mae: 1.8133 - mse: 6.5466 - val_loss: 6.5893 - val_mae: 1.8989 - val_mse: 6.5893\n",
      "Epoch 78/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 6.3160 - mae: 1.7759 - mse: 6.3160 - val_loss: 7.3917 - val_mae: 1.9897 - val_mse: 7.3917\n",
      "Epoch 79/1000\n",
      "250/250 [==============================] - 0s 60us/sample - loss: 6.4515 - mae: 1.8052 - mse: 6.4515 - val_loss: 6.7524 - val_mae: 1.9121 - val_mse: 6.7524\n",
      "Epoch 80/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 6.2656 - mae: 1.7886 - mse: 6.2656 - val_loss: 7.2523 - val_mae: 1.9686 - val_mse: 7.2523\n",
      "Epoch 81/1000\n",
      "250/250 [==============================] - 0s 84us/sample - loss: 6.2399 - mae: 1.7889 - mse: 6.2399 - val_loss: 6.7137 - val_mae: 1.9204 - val_mse: 6.7137\n",
      "Epoch 82/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 6.3078 - mae: 1.7569 - mse: 6.3078 - val_loss: 7.1200 - val_mae: 1.9367 - val_mse: 7.1200\n",
      "Epoch 83/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 6.3560 - mae: 1.7758 - mse: 6.3560 - val_loss: 6.4338 - val_mae: 1.8733 - val_mse: 6.4338\n",
      "Epoch 84/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 6.4435 - mae: 1.8201 - mse: 6.4435 - val_loss: 6.8104 - val_mae: 1.9098 - val_mse: 6.8104\n",
      "Epoch 85/1000\n",
      "250/250 [==============================] - 0s 78us/sample - loss: 6.3283 - mae: 1.7970 - mse: 6.3283 - val_loss: 6.7162 - val_mae: 1.9061 - val_mse: 6.7162\n",
      "Epoch 86/1000\n",
      "250/250 [==============================] - 0s 79us/sample - loss: 6.1418 - mae: 1.7630 - mse: 6.1418 - val_loss: 6.1716 - val_mae: 1.8540 - val_mse: 6.1716\n",
      "Epoch 87/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 6.3306 - mae: 1.7966 - mse: 6.3306 - val_loss: 7.4216 - val_mae: 1.9862 - val_mse: 7.4216\n",
      "Epoch 88/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 6.2644 - mae: 1.7605 - mse: 6.2644 - val_loss: 6.9183 - val_mae: 1.9263 - val_mse: 6.9183\n",
      "Epoch 89/1000\n",
      "250/250 [==============================] - 0s 79us/sample - loss: 6.1975 - mae: 1.7706 - mse: 6.1975 - val_loss: 6.3390 - val_mae: 1.8795 - val_mse: 6.3390\n",
      "Epoch 90/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 6.1415 - mae: 1.7486 - mse: 6.1415 - val_loss: 6.9087 - val_mae: 1.9304 - val_mse: 6.9087\n",
      "Epoch 91/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 6.2855 - mae: 1.7863 - mse: 6.2855 - val_loss: 7.5832 - val_mae: 1.9938 - val_mse: 7.5832\n",
      "Epoch 92/1000\n",
      "250/250 [==============================] - 0s 119us/sample - loss: 6.2269 - mae: 1.7487 - mse: 6.2269 - val_loss: 6.1906 - val_mae: 1.8462 - val_mse: 6.1906\n",
      "Epoch 93/1000\n",
      "250/250 [==============================] - 0s 97us/sample - loss: 6.1317 - mae: 1.7630 - mse: 6.1317 - val_loss: 6.0571 - val_mae: 1.8450 - val_mse: 6.0571\n",
      "Epoch 94/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 6.1695 - mae: 1.7538 - mse: 6.1695 - val_loss: 6.1233 - val_mae: 1.8499 - val_mse: 6.1233\n",
      "Epoch 95/1000\n",
      "250/250 [==============================] - 0s 93us/sample - loss: 6.2222 - mae: 1.7568 - mse: 6.2222 - val_loss: 6.5329 - val_mae: 1.8888 - val_mse: 6.5329\n",
      "Epoch 96/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 5.9955 - mae: 1.7354 - mse: 5.9955 - val_loss: 5.9214 - val_mae: 1.8409 - val_mse: 5.9214\n",
      "Epoch 97/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 6.0567 - mae: 1.7752 - mse: 6.0567 - val_loss: 6.4922 - val_mae: 1.8896 - val_mse: 6.4922\n",
      "Epoch 98/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 6.1984 - mae: 1.7652 - mse: 6.1984 - val_loss: 6.4571 - val_mae: 1.8796 - val_mse: 6.4571\n",
      "Epoch 99/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 6.0871 - mae: 1.7295 - mse: 6.0871 - val_loss: 6.2060 - val_mae: 1.8577 - val_mse: 6.2060\n",
      "Epoch 100/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 6.0783 - mae: 1.7466 - mse: 6.0783 - val_loss: 6.8803 - val_mae: 1.9178 - val_mse: 6.8803\n",
      "Epoch 101/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 6.0503 - mae: 1.7458 - mse: 6.0503 - val_loss: 6.3609 - val_mae: 1.8760 - val_mse: 6.3609\n",
      "Epoch 102/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 5.9291 - mae: 1.7356 - mse: 5.9291 - val_loss: 7.5046 - val_mae: 1.9968 - val_mse: 7.5046\n",
      "Epoch 103/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 5.9656 - mae: 1.7255 - mse: 5.9656 - val_loss: 6.0046 - val_mae: 1.8254 - val_mse: 6.0046\n",
      "Epoch 104/1000\n",
      "250/250 [==============================] - 0s 61us/sample - loss: 5.9880 - mae: 1.7241 - mse: 5.9880 - val_loss: 6.0565 - val_mae: 1.8488 - val_mse: 6.0565\n",
      "Epoch 105/1000\n",
      "250/250 [==============================] - 0s 60us/sample - loss: 6.2707 - mae: 1.7401 - mse: 6.2707 - val_loss: 5.8675 - val_mae: 1.8102 - val_mse: 5.8675\n",
      "Epoch 106/1000\n",
      "250/250 [==============================] - 0s 61us/sample - loss: 6.0786 - mae: 1.7396 - mse: 6.0786 - val_loss: 5.8986 - val_mae: 1.8255 - val_mse: 5.8986\n",
      "Epoch 107/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 5.9082 - mae: 1.7220 - mse: 5.9082 - val_loss: 6.9484 - val_mae: 1.9285 - val_mse: 6.9484\n",
      "Epoch 108/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 5.9006 - mae: 1.7574 - mse: 5.9006 - val_loss: 7.0002 - val_mae: 1.9232 - val_mse: 7.0002\n",
      "Epoch 109/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 5.9436 - mae: 1.7257 - mse: 5.9436 - val_loss: 6.6226 - val_mae: 1.8920 - val_mse: 6.6226\n",
      "Epoch 110/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 5.9906 - mae: 1.7333 - mse: 5.9906 - val_loss: 6.7092 - val_mae: 1.9141 - val_mse: 6.7092\n",
      "Epoch 111/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 5.8994 - mae: 1.6802 - mse: 5.8994 - val_loss: 6.4916 - val_mae: 1.8926 - val_mse: 6.4916\n",
      "Epoch 112/1000\n",
      "250/250 [==============================] - 0s 83us/sample - loss: 5.9362 - mae: 1.7588 - mse: 5.9362 - val_loss: 7.4271 - val_mae: 1.9733 - val_mse: 7.4271\n",
      "Epoch 113/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 5.9374 - mae: 1.7159 - mse: 5.9374 - val_loss: 6.7024 - val_mae: 1.9109 - val_mse: 6.7024\n",
      "Epoch 114/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 5.7928 - mae: 1.7086 - mse: 5.7928 - val_loss: 6.3222 - val_mae: 1.8785 - val_mse: 6.3222\n",
      "Epoch 115/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 5.8841 - mae: 1.7272 - mse: 5.8841 - val_loss: 6.6513 - val_mae: 1.8937 - val_mse: 6.6513\n",
      "Epoch 116/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 5.8007 - mae: 1.6957 - mse: 5.8007 - val_loss: 6.8241 - val_mae: 1.9083 - val_mse: 6.8241\n",
      "Epoch 117/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 5.9792 - mae: 1.7100 - mse: 5.9792 - val_loss: 6.8202 - val_mae: 1.9182 - val_mse: 6.8202\n",
      "Epoch 118/1000\n",
      "250/250 [==============================] - 0s 79us/sample - loss: 5.7464 - mae: 1.6696 - mse: 5.7464 - val_loss: 6.8457 - val_mae: 1.9124 - val_mse: 6.8457\n",
      "Epoch 119/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 6.0432 - mae: 1.7223 - mse: 6.0432 - val_loss: 6.7636 - val_mae: 1.9120 - val_mse: 6.7636\n",
      "Epoch 120/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 5.9105 - mae: 1.6961 - mse: 5.9105 - val_loss: 6.3351 - val_mae: 1.8613 - val_mse: 6.3351\n",
      "Epoch 121/1000\n",
      "250/250 [==============================] - 0s 88us/sample - loss: 5.8332 - mae: 1.6774 - mse: 5.8332 - val_loss: 6.3899 - val_mae: 1.8843 - val_mse: 6.3899\n",
      "Epoch 122/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 5.8043 - mae: 1.7048 - mse: 5.8043 - val_loss: 7.3785 - val_mae: 1.9918 - val_mse: 7.3785\n",
      "Epoch 123/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 5.7145 - mae: 1.6790 - mse: 5.7145 - val_loss: 6.8649 - val_mae: 1.9116 - val_mse: 6.8649\n",
      "Epoch 124/1000\n",
      "250/250 [==============================] - 0s 80us/sample - loss: 5.6949 - mae: 1.6580 - mse: 5.6949 - val_loss: 6.1828 - val_mae: 1.8644 - val_mse: 6.1828\n",
      "Epoch 125/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 5.6818 - mae: 1.7103 - mse: 5.6818 - val_loss: 5.7747 - val_mae: 1.8325 - val_mse: 5.7747\n",
      "Epoch 126/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 6.0389 - mae: 1.7273 - mse: 6.0389 - val_loss: 6.6212 - val_mae: 1.8973 - val_mse: 6.6212\n",
      "Epoch 127/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 5.6536 - mae: 1.6772 - mse: 5.6536 - val_loss: 6.2727 - val_mae: 1.8646 - val_mse: 6.2727\n",
      "Epoch 128/1000\n",
      "250/250 [==============================] - 0s 85us/sample - loss: 5.7279 - mae: 1.7080 - mse: 5.7279 - val_loss: 8.1532 - val_mae: 2.0932 - val_mse: 8.1532\n",
      "Epoch 129/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 6.0532 - mae: 1.7219 - mse: 6.0532 - val_loss: 6.4230 - val_mae: 1.8743 - val_mse: 6.4230\n",
      "Epoch 130/1000\n",
      "250/250 [==============================] - 0s 82us/sample - loss: 5.6690 - mae: 1.6720 - mse: 5.6690 - val_loss: 6.5501 - val_mae: 1.8816 - val_mse: 6.5501\n",
      "Epoch 131/1000\n",
      "250/250 [==============================] - 0s 78us/sample - loss: 5.7715 - mae: 1.7095 - mse: 5.7715 - val_loss: 7.2545 - val_mae: 1.9493 - val_mse: 7.2545\n",
      "Epoch 132/1000\n",
      "250/250 [==============================] - 0s 78us/sample - loss: 5.5274 - mae: 1.6463 - mse: 5.5274 - val_loss: 5.9921 - val_mae: 1.8357 - val_mse: 5.9921\n",
      "Epoch 133/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 5.6092 - mae: 1.6578 - mse: 5.6092 - val_loss: 6.3425 - val_mae: 1.8905 - val_mse: 6.3425\n",
      "Epoch 134/1000\n",
      "250/250 [==============================] - 0s 78us/sample - loss: 5.7769 - mae: 1.7087 - mse: 5.7769 - val_loss: 6.2725 - val_mae: 1.8591 - val_mse: 6.2725\n",
      "Epoch 135/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 5.7006 - mae: 1.6778 - mse: 5.7006 - val_loss: 6.3151 - val_mae: 1.8588 - val_mse: 6.3151\n",
      "Epoch 136/1000\n",
      "250/250 [==============================] - 0s 85us/sample - loss: 5.5771 - mae: 1.6670 - mse: 5.5771 - val_loss: 6.3133 - val_mae: 1.8667 - val_mse: 6.3133\n",
      "Epoch 137/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 5.9293 - mae: 1.7169 - mse: 5.9293 - val_loss: 6.0743 - val_mae: 1.8468 - val_mse: 6.0743\n",
      "Epoch 138/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 5.7399 - mae: 1.6989 - mse: 5.7399 - val_loss: 7.6417 - val_mae: 1.9849 - val_mse: 7.6417\n",
      "Epoch 139/1000\n",
      "250/250 [==============================] - 0s 82us/sample - loss: 5.6768 - mae: 1.6839 - mse: 5.6768 - val_loss: 7.8835 - val_mae: 2.0567 - val_mse: 7.8835\n",
      "Epoch 140/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 5.5904 - mae: 1.6677 - mse: 5.5904 - val_loss: 5.6817 - val_mae: 1.7974 - val_mse: 5.6817\n",
      "Epoch 141/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 5.6660 - mae: 1.6953 - mse: 5.6660 - val_loss: 6.4534 - val_mae: 1.8750 - val_mse: 6.4534\n",
      "Epoch 142/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 5.5876 - mae: 1.6760 - mse: 5.5876 - val_loss: 6.8799 - val_mae: 1.9212 - val_mse: 6.8799\n",
      "Epoch 143/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 5.5727 - mae: 1.6637 - mse: 5.5727 - val_loss: 6.0535 - val_mae: 1.8347 - val_mse: 6.0535\n",
      "Epoch 144/1000\n",
      "250/250 [==============================] - 0s 82us/sample - loss: 5.4341 - mae: 1.6180 - mse: 5.4341 - val_loss: 5.8391 - val_mae: 1.8198 - val_mse: 5.8391\n",
      "Epoch 145/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 5.8206 - mae: 1.7239 - mse: 5.8207 - val_loss: 7.0519 - val_mae: 1.9229 - val_mse: 7.0519\n",
      "Epoch 146/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 5.6579 - mae: 1.6677 - mse: 5.6579 - val_loss: 6.5756 - val_mae: 1.8896 - val_mse: 6.5756\n",
      "Epoch 147/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 5.7237 - mae: 1.6874 - mse: 5.7237 - val_loss: 6.0750 - val_mae: 1.8383 - val_mse: 6.0750\n",
      "Epoch 148/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 5.6009 - mae: 1.6751 - mse: 5.6009 - val_loss: 6.5533 - val_mae: 1.8848 - val_mse: 6.5533\n",
      "Epoch 149/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 5.5131 - mae: 1.6288 - mse: 5.5131 - val_loss: 6.0672 - val_mae: 1.8537 - val_mse: 6.0672\n",
      "Epoch 150/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 5.9191 - mae: 1.7004 - mse: 5.9191 - val_loss: 6.5334 - val_mae: 1.8740 - val_mse: 6.5334\n",
      "Epoch 151/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 5.5576 - mae: 1.6506 - mse: 5.5576 - val_loss: 6.8593 - val_mae: 1.9178 - val_mse: 6.8593\n",
      "Epoch 152/1000\n",
      "250/250 [==============================] - 0s 79us/sample - loss: 5.5059 - mae: 1.6311 - mse: 5.5059 - val_loss: 6.0029 - val_mae: 1.8341 - val_mse: 6.0029\n",
      "Epoch 153/1000\n",
      "250/250 [==============================] - 0s 79us/sample - loss: 5.4743 - mae: 1.6319 - mse: 5.4743 - val_loss: 6.1833 - val_mae: 1.8419 - val_mse: 6.1833\n",
      "Epoch 154/1000\n",
      "250/250 [==============================] - 0s 91us/sample - loss: 5.6969 - mae: 1.6925 - mse: 5.6969 - val_loss: 5.8698 - val_mae: 1.8074 - val_mse: 5.8698\n",
      "Epoch 155/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 5.4597 - mae: 1.6809 - mse: 5.4597 - val_loss: 8.0190 - val_mae: 2.0315 - val_mse: 8.0190\n",
      "Epoch 156/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 5.6625 - mae: 1.6691 - mse: 5.6625 - val_loss: 6.5144 - val_mae: 1.8789 - val_mse: 6.5144\n",
      "Epoch 157/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 5.3753 - mae: 1.6255 - mse: 5.3753 - val_loss: 5.8895 - val_mae: 1.8106 - val_mse: 5.8895\n",
      "Epoch 158/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 5.5761 - mae: 1.6740 - mse: 5.5761 - val_loss: 7.1660 - val_mae: 1.9423 - val_mse: 7.1660\n",
      "Epoch 159/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 5.3322 - mae: 1.6076 - mse: 5.3322 - val_loss: 6.0159 - val_mae: 1.8374 - val_mse: 6.0159\n",
      "Epoch 160/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 5.4627 - mae: 1.6595 - mse: 5.4627 - val_loss: 6.1994 - val_mae: 1.8673 - val_mse: 6.1994\n",
      "Epoch 161/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 5.4235 - mae: 1.6561 - mse: 5.4235 - val_loss: 6.0945 - val_mae: 1.8311 - val_mse: 6.0945\n",
      "Epoch 162/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 5.4804 - mae: 1.6509 - mse: 5.4804 - val_loss: 7.8035 - val_mae: 2.0168 - val_mse: 7.8035\n",
      "Epoch 163/1000\n",
      "250/250 [==============================] - 0s 79us/sample - loss: 5.4263 - mae: 1.6408 - mse: 5.4263 - val_loss: 6.1198 - val_mae: 1.8384 - val_mse: 6.1198\n",
      "Epoch 164/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 5.6537 - mae: 1.6899 - mse: 5.6537 - val_loss: 7.0528 - val_mae: 1.9169 - val_mse: 7.0528\n",
      "Epoch 165/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 5.3303 - mae: 1.6436 - mse: 5.3303 - val_loss: 7.8851 - val_mae: 2.0154 - val_mse: 7.8851\n",
      "Epoch 166/1000\n",
      "250/250 [==============================] - 0s 61us/sample - loss: 5.6301 - mae: 1.6611 - mse: 5.6301 - val_loss: 6.9697 - val_mae: 1.9068 - val_mse: 6.9697\n",
      "Epoch 167/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 5.4206 - mae: 1.6439 - mse: 5.4206 - val_loss: 6.2492 - val_mae: 1.8469 - val_mse: 6.2492\n",
      "Epoch 168/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 5.3302 - mae: 1.6193 - mse: 5.3302 - val_loss: 6.7069 - val_mae: 1.8847 - val_mse: 6.7069\n",
      "Epoch 169/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 5.5502 - mae: 1.6575 - mse: 5.5502 - val_loss: 6.7379 - val_mae: 1.9050 - val_mse: 6.7379\n",
      "Epoch 170/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 5.4045 - mae: 1.6378 - mse: 5.4045 - val_loss: 6.3172 - val_mae: 1.8557 - val_mse: 6.3173\n",
      "Epoch 171/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 5.3884 - mae: 1.6373 - mse: 5.3884 - val_loss: 6.2887 - val_mae: 1.8625 - val_mse: 6.2887\n",
      "Epoch 172/1000\n",
      "250/250 [==============================] - 0s 83us/sample - loss: 5.4749 - mae: 1.6335 - mse: 5.4749 - val_loss: 6.5506 - val_mae: 1.8872 - val_mse: 6.5506\n",
      "Epoch 173/1000\n",
      "250/250 [==============================] - 0s 84us/sample - loss: 5.3549 - mae: 1.6308 - mse: 5.3549 - val_loss: 6.0727 - val_mae: 1.8338 - val_mse: 6.0727\n",
      "Epoch 174/1000\n",
      "250/250 [==============================] - 0s 83us/sample - loss: 5.4599 - mae: 1.6420 - mse: 5.4599 - val_loss: 6.5905 - val_mae: 1.8825 - val_mse: 6.5905\n",
      "Epoch 175/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 5.5214 - mae: 1.6362 - mse: 5.5214 - val_loss: 6.2049 - val_mae: 1.8531 - val_mse: 6.2049\n",
      "Epoch 176/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 5.3360 - mae: 1.6266 - mse: 5.3360 - val_loss: 7.0503 - val_mae: 1.9158 - val_mse: 7.0503\n",
      "Epoch 177/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 5.3412 - mae: 1.6146 - mse: 5.3412 - val_loss: 5.9080 - val_mae: 1.8113 - val_mse: 5.9080\n",
      "Epoch 178/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 5.3726 - mae: 1.6398 - mse: 5.3726 - val_loss: 6.2066 - val_mae: 1.8430 - val_mse: 6.2066\n",
      "Epoch 179/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 5.3484 - mae: 1.6288 - mse: 5.3484 - val_loss: 6.4623 - val_mae: 1.8592 - val_mse: 6.4623\n",
      "Epoch 180/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 5.2453 - mae: 1.6257 - mse: 5.2453 - val_loss: 6.7629 - val_mae: 1.8954 - val_mse: 6.7629\n",
      "Epoch 181/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 5.3940 - mae: 1.6378 - mse: 5.3940 - val_loss: 6.8921 - val_mae: 1.9193 - val_mse: 6.8921\n",
      "Epoch 182/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 5.3355 - mae: 1.6251 - mse: 5.3355 - val_loss: 6.3783 - val_mae: 1.8683 - val_mse: 6.3783\n",
      "Epoch 183/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 5.4279 - mae: 1.6287 - mse: 5.4279 - val_loss: 6.8257 - val_mae: 1.8995 - val_mse: 6.8257\n",
      "Epoch 184/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 5.3996 - mae: 1.6335 - mse: 5.3996 - val_loss: 5.9156 - val_mae: 1.8146 - val_mse: 5.9156\n",
      "Epoch 185/1000\n",
      "250/250 [==============================] - 0s 78us/sample - loss: 5.3022 - mae: 1.6255 - mse: 5.3022 - val_loss: 7.2397 - val_mae: 1.9406 - val_mse: 7.2397\n",
      "Epoch 186/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 5.2065 - mae: 1.5979 - mse: 5.2065 - val_loss: 7.0453 - val_mae: 1.9534 - val_mse: 7.0453\n",
      "Epoch 187/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 5.3714 - mae: 1.6086 - mse: 5.3714 - val_loss: 7.0031 - val_mae: 1.9131 - val_mse: 7.0031\n",
      "Epoch 188/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 5.1528 - mae: 1.5911 - mse: 5.1528 - val_loss: 6.7880 - val_mae: 1.8950 - val_mse: 6.7880\n",
      "Epoch 189/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 5.4340 - mae: 1.6458 - mse: 5.4340 - val_loss: 7.4338 - val_mae: 1.9579 - val_mse: 7.4338\n",
      "Epoch 190/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 5.3147 - mae: 1.6312 - mse: 5.3147 - val_loss: 7.0558 - val_mae: 1.9153 - val_mse: 7.0558\n",
      "Epoch 191/1000\n",
      "250/250 [==============================] - 0s 87us/sample - loss: 5.2663 - mae: 1.6163 - mse: 5.2663 - val_loss: 8.0805 - val_mae: 2.0433 - val_mse: 8.0805\n",
      "Epoch 192/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 5.3386 - mae: 1.5645 - mse: 5.3386 - val_loss: 6.4777 - val_mae: 1.8841 - val_mse: 6.4777\n",
      "Epoch 193/1000\n",
      "250/250 [==============================] - 0s 78us/sample - loss: 5.2915 - mae: 1.6263 - mse: 5.2915 - val_loss: 6.3523 - val_mae: 1.8641 - val_mse: 6.3523\n",
      "Epoch 194/1000\n",
      "250/250 [==============================] - 0s 117us/sample - loss: 5.2777 - mae: 1.5900 - mse: 5.2777 - val_loss: 6.4404 - val_mae: 1.8707 - val_mse: 6.4404\n",
      "Epoch 195/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 5.2698 - mae: 1.6135 - mse: 5.2698 - val_loss: 6.8328 - val_mae: 1.9181 - val_mse: 6.8328\n",
      "Epoch 196/1000\n",
      "250/250 [==============================] - 0s 98us/sample - loss: 5.3008 - mae: 1.6415 - mse: 5.3008 - val_loss: 8.2771 - val_mae: 2.0626 - val_mse: 8.2771\n",
      "Epoch 197/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 5.2955 - mae: 1.6214 - mse: 5.2955 - val_loss: 7.0597 - val_mae: 1.9353 - val_mse: 7.0597\n",
      "Epoch 198/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 5.1740 - mae: 1.5776 - mse: 5.1740 - val_loss: 6.4488 - val_mae: 1.8708 - val_mse: 6.4488\n",
      "Epoch 199/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 5.3636 - mae: 1.6181 - mse: 5.3636 - val_loss: 6.2051 - val_mae: 1.8479 - val_mse: 6.2051\n",
      "Epoch 200/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 5.1834 - mae: 1.5713 - mse: 5.1834 - val_loss: 6.3736 - val_mae: 1.8631 - val_mse: 6.3736\n",
      "Epoch 201/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 5.2602 - mae: 1.6251 - mse: 5.2602 - val_loss: 6.5079 - val_mae: 1.8833 - val_mse: 6.5079\n",
      "Epoch 202/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 5.1485 - mae: 1.5755 - mse: 5.1485 - val_loss: 6.1582 - val_mae: 1.8377 - val_mse: 6.1582\n",
      "Epoch 203/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 5.0651 - mae: 1.6051 - mse: 5.0651 - val_loss: 7.1726 - val_mae: 1.9353 - val_mse: 7.1726\n",
      "Epoch 204/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 5.1310 - mae: 1.5774 - mse: 5.1310 - val_loss: 6.3195 - val_mae: 1.8567 - val_mse: 6.3195\n",
      "Epoch 205/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 5.1570 - mae: 1.5921 - mse: 5.1570 - val_loss: 5.8648 - val_mae: 1.8240 - val_mse: 5.8648\n",
      "Epoch 206/1000\n",
      "250/250 [==============================] - 0s 83us/sample - loss: 5.1544 - mae: 1.6230 - mse: 5.1544 - val_loss: 6.7305 - val_mae: 1.8901 - val_mse: 6.7305\n",
      "Epoch 207/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 5.2640 - mae: 1.6068 - mse: 5.2640 - val_loss: 7.0258 - val_mae: 1.9249 - val_mse: 7.0258\n",
      "Epoch 208/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 5.1985 - mae: 1.5927 - mse: 5.1985 - val_loss: 6.2086 - val_mae: 1.8454 - val_mse: 6.2086\n",
      "Epoch 209/1000\n",
      "250/250 [==============================] - 0s 87us/sample - loss: 5.3574 - mae: 1.5912 - mse: 5.3574 - val_loss: 6.1271 - val_mae: 1.8308 - val_mse: 6.1271\n",
      "Epoch 210/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 5.1291 - mae: 1.5877 - mse: 5.1291 - val_loss: 6.8143 - val_mae: 1.8946 - val_mse: 6.8144\n",
      "Epoch 211/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 5.1850 - mae: 1.5751 - mse: 5.1850 - val_loss: 6.4750 - val_mae: 1.8761 - val_mse: 6.4750\n",
      "Epoch 212/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 5.1687 - mae: 1.5689 - mse: 5.1687 - val_loss: 8.1019 - val_mae: 2.0282 - val_mse: 8.1019\n",
      "Epoch 213/1000\n",
      "250/250 [==============================] - 0s 86us/sample - loss: 5.1995 - mae: 1.6140 - mse: 5.1995 - val_loss: 6.1801 - val_mae: 1.8304 - val_mse: 6.1801\n",
      "Epoch 214/1000\n",
      "250/250 [==============================] - 0s 86us/sample - loss: 5.2153 - mae: 1.5912 - mse: 5.2153 - val_loss: 6.5009 - val_mae: 1.8918 - val_mse: 6.5009\n",
      "Epoch 215/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 5.2109 - mae: 1.6135 - mse: 5.2109 - val_loss: 6.6403 - val_mae: 1.8893 - val_mse: 6.6403\n",
      "Epoch 216/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 5.1557 - mae: 1.5874 - mse: 5.1557 - val_loss: 6.9790 - val_mae: 1.9074 - val_mse: 6.9790\n",
      "Epoch 217/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 5.0686 - mae: 1.5941 - mse: 5.0686 - val_loss: 7.2647 - val_mae: 1.9442 - val_mse: 7.2647\n",
      "Epoch 218/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 5.0342 - mae: 1.5716 - mse: 5.0342 - val_loss: 7.6488 - val_mae: 1.9918 - val_mse: 7.6488\n",
      "Epoch 219/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 4.9521 - mae: 1.5467 - mse: 4.9521 - val_loss: 7.7206 - val_mae: 1.9770 - val_mse: 7.7206\n",
      "Epoch 220/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 5.2745 - mae: 1.5852 - mse: 5.2745 - val_loss: 7.3108 - val_mae: 1.9395 - val_mse: 7.3108\n",
      "Epoch 221/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 5.0315 - mae: 1.5521 - mse: 5.0315 - val_loss: 7.0469 - val_mae: 1.9182 - val_mse: 7.0469\n",
      "Epoch 222/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 5.1277 - mae: 1.6103 - mse: 5.1277 - val_loss: 7.5677 - val_mae: 1.9779 - val_mse: 7.5677\n",
      "Epoch 223/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 5.0513 - mae: 1.5713 - mse: 5.0513 - val_loss: 6.5788 - val_mae: 1.8804 - val_mse: 6.5788\n",
      "Epoch 224/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 5.0096 - mae: 1.5885 - mse: 5.0096 - val_loss: 7.3783 - val_mae: 1.9515 - val_mse: 7.3783\n",
      "Epoch 225/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 5.2726 - mae: 1.5874 - mse: 5.2726 - val_loss: 6.7799 - val_mae: 1.8912 - val_mse: 6.7799\n",
      "Epoch 226/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 5.0006 - mae: 1.5530 - mse: 5.0006 - val_loss: 6.5723 - val_mae: 1.8959 - val_mse: 6.5723\n",
      "Epoch 227/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 5.0717 - mae: 1.5855 - mse: 5.0717 - val_loss: 6.8954 - val_mae: 1.8980 - val_mse: 6.8954\n",
      "Epoch 228/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 4.8837 - mae: 1.5379 - mse: 4.8837 - val_loss: 7.0736 - val_mae: 1.9226 - val_mse: 7.0736\n",
      "Epoch 229/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 5.2112 - mae: 1.6001 - mse: 5.2112 - val_loss: 9.5178 - val_mae: 2.2160 - val_mse: 9.5178\n",
      "Epoch 230/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 5.1391 - mae: 1.5769 - mse: 5.1391 - val_loss: 7.1355 - val_mae: 1.9297 - val_mse: 7.1355\n",
      "Epoch 231/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 5.0714 - mae: 1.5512 - mse: 5.0714 - val_loss: 8.6849 - val_mae: 2.0954 - val_mse: 8.6849\n",
      "Epoch 232/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 4.8740 - mae: 1.5133 - mse: 4.8740 - val_loss: 6.0561 - val_mae: 1.8467 - val_mse: 6.0561\n",
      "Epoch 233/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 5.0266 - mae: 1.5779 - mse: 5.0266 - val_loss: 6.6018 - val_mae: 1.8710 - val_mse: 6.6018\n",
      "Epoch 234/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 4.9638 - mae: 1.5611 - mse: 4.9638 - val_loss: 7.2144 - val_mae: 1.9442 - val_mse: 7.2144\n",
      "Epoch 235/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 4.9055 - mae: 1.5233 - mse: 4.9055 - val_loss: 7.7644 - val_mae: 1.9802 - val_mse: 7.7644\n",
      "Epoch 236/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 5.4331 - mae: 1.6111 - mse: 5.4331 - val_loss: 6.4561 - val_mae: 1.8633 - val_mse: 6.4561\n",
      "Epoch 237/1000\n",
      "250/250 [==============================] - 0s 83us/sample - loss: 4.9887 - mae: 1.5559 - mse: 4.9887 - val_loss: 6.7123 - val_mae: 1.8996 - val_mse: 6.7123\n",
      "Epoch 238/1000\n",
      "250/250 [==============================] - 0s 59us/sample - loss: 4.9521 - mae: 1.5506 - mse: 4.9521 - val_loss: 6.4761 - val_mae: 1.8719 - val_mse: 6.4761\n",
      "Epoch 239/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 5.1827 - mae: 1.5846 - mse: 5.1827 - val_loss: 7.0519 - val_mae: 1.9266 - val_mse: 7.0519\n",
      "Epoch 240/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 5.0706 - mae: 1.5813 - mse: 5.0706 - val_loss: 6.9504 - val_mae: 1.9015 - val_mse: 6.9504\n",
      "Epoch 241/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 4.9253 - mae: 1.5854 - mse: 4.9253 - val_loss: 6.3344 - val_mae: 1.8569 - val_mse: 6.3344\n",
      "Epoch 242/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 4.8596 - mae: 1.5084 - mse: 4.8596 - val_loss: 6.0676 - val_mae: 1.8249 - val_mse: 6.0676\n",
      "Epoch 243/1000\n",
      "250/250 [==============================] - 0s 80us/sample - loss: 5.1358 - mae: 1.5892 - mse: 5.1358 - val_loss: 7.2252 - val_mae: 1.9203 - val_mse: 7.2252\n",
      "Epoch 244/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 4.8749 - mae: 1.5208 - mse: 4.8749 - val_loss: 7.7554 - val_mae: 1.9702 - val_mse: 7.7554\n",
      "Epoch 245/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 5.2424 - mae: 1.5657 - mse: 5.2424 - val_loss: 6.8191 - val_mae: 1.8881 - val_mse: 6.8191\n",
      "Epoch 246/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 5.0576 - mae: 1.5388 - mse: 5.0576 - val_loss: 6.9963 - val_mae: 1.9016 - val_mse: 6.9963\n",
      "Epoch 247/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 4.8346 - mae: 1.5231 - mse: 4.8346 - val_loss: 6.0056 - val_mae: 1.8318 - val_mse: 6.0056\n",
      "Epoch 248/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 5.0239 - mae: 1.5746 - mse: 5.0239 - val_loss: 6.1561 - val_mae: 1.8353 - val_mse: 6.1561\n",
      "Epoch 249/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 5.0538 - mae: 1.5800 - mse: 5.0538 - val_loss: 7.6973 - val_mae: 1.9791 - val_mse: 7.6973\n",
      "Epoch 250/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 4.9343 - mae: 1.5439 - mse: 4.9343 - val_loss: 6.1422 - val_mae: 1.8203 - val_mse: 6.1422\n",
      "Epoch 251/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 4.8651 - mae: 1.5571 - mse: 4.8651 - val_loss: 8.0617 - val_mae: 2.0208 - val_mse: 8.0617\n",
      "Epoch 252/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 5.0067 - mae: 1.5288 - mse: 5.0067 - val_loss: 8.9063 - val_mae: 2.1275 - val_mse: 8.9063\n",
      "Epoch 253/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 4.9332 - mae: 1.5332 - mse: 4.9332 - val_loss: 6.8995 - val_mae: 1.8876 - val_mse: 6.8995\n",
      "Epoch 254/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 4.9553 - mae: 1.5318 - mse: 4.9553 - val_loss: 6.1624 - val_mae: 1.8234 - val_mse: 6.1624\n",
      "Epoch 255/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 5.0104 - mae: 1.5570 - mse: 5.0104 - val_loss: 7.8195 - val_mae: 1.9821 - val_mse: 7.8195\n",
      "Epoch 256/1000\n",
      "250/250 [==============================] - 0s 89us/sample - loss: 4.9962 - mae: 1.5639 - mse: 4.9962 - val_loss: 6.2707 - val_mae: 1.8354 - val_mse: 6.2707\n",
      "Epoch 257/1000\n",
      "250/250 [==============================] - 0s 59us/sample - loss: 5.0359 - mae: 1.5495 - mse: 5.0359 - val_loss: 7.4659 - val_mae: 1.9373 - val_mse: 7.4659\n",
      "Epoch 258/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 5.2215 - mae: 1.5952 - mse: 5.2215 - val_loss: 6.4646 - val_mae: 1.8544 - val_mse: 6.4646\n",
      "Epoch 259/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 4.8008 - mae: 1.5233 - mse: 4.8008 - val_loss: 6.1194 - val_mae: 1.8052 - val_mse: 6.1194\n",
      "Epoch 260/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 4.6698 - mae: 1.5011 - mse: 4.6698 - val_loss: 9.3822 - val_mae: 2.1873 - val_mse: 9.3822\n",
      "Epoch 261/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 5.0215 - mae: 1.5516 - mse: 5.0215 - val_loss: 5.9522 - val_mae: 1.8054 - val_mse: 5.9522\n",
      "Epoch 262/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 5.0210 - mae: 1.5605 - mse: 5.0210 - val_loss: 6.1661 - val_mae: 1.8104 - val_mse: 6.1661\n",
      "Epoch 263/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 4.9025 - mae: 1.5482 - mse: 4.9025 - val_loss: 7.5096 - val_mae: 1.9610 - val_mse: 7.5096\n",
      "Epoch 264/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 4.8742 - mae: 1.5245 - mse: 4.8742 - val_loss: 7.2522 - val_mae: 1.9185 - val_mse: 7.2522\n",
      "Epoch 265/1000\n",
      "250/250 [==============================] - 0s 82us/sample - loss: 4.8831 - mae: 1.5221 - mse: 4.8831 - val_loss: 6.0356 - val_mae: 1.8175 - val_mse: 6.0356\n",
      "Epoch 266/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 4.7229 - mae: 1.5207 - mse: 4.7229 - val_loss: 7.7715 - val_mae: 1.9616 - val_mse: 7.7715\n",
      "Epoch 267/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 4.9178 - mae: 1.5528 - mse: 4.9178 - val_loss: 8.2463 - val_mae: 2.0237 - val_mse: 8.2463\n",
      "Epoch 268/1000\n",
      "250/250 [==============================] - 0s 87us/sample - loss: 4.8596 - mae: 1.5220 - mse: 4.8596 - val_loss: 6.7792 - val_mae: 1.8745 - val_mse: 6.7792\n",
      "Epoch 269/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 4.9414 - mae: 1.5824 - mse: 4.9414 - val_loss: 8.2294 - val_mae: 2.0303 - val_mse: 8.2294\n",
      "Epoch 270/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 4.9629 - mae: 1.5698 - mse: 4.9629 - val_loss: 6.5132 - val_mae: 1.8518 - val_mse: 6.5132\n",
      "Epoch 271/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 4.9275 - mae: 1.5330 - mse: 4.9275 - val_loss: 7.7068 - val_mae: 1.9644 - val_mse: 7.7068\n",
      "Epoch 272/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 4.6862 - mae: 1.5188 - mse: 4.6862 - val_loss: 7.0347 - val_mae: 1.9136 - val_mse: 7.0347\n",
      "Epoch 273/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 4.8063 - mae: 1.5039 - mse: 4.8063 - val_loss: 6.7950 - val_mae: 1.8751 - val_mse: 6.7950\n",
      "Epoch 274/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 5.1170 - mae: 1.5807 - mse: 5.1170 - val_loss: 6.5331 - val_mae: 1.8555 - val_mse: 6.5331\n",
      "Epoch 275/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 4.8325 - mae: 1.5130 - mse: 4.8325 - val_loss: 6.3558 - val_mae: 1.8302 - val_mse: 6.3558\n",
      "Epoch 276/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 5.1299 - mae: 1.5541 - mse: 5.1299 - val_loss: 7.1750 - val_mae: 1.9319 - val_mse: 7.1750\n",
      "Epoch 277/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 4.7852 - mae: 1.5182 - mse: 4.7852 - val_loss: 5.9402 - val_mae: 1.7892 - val_mse: 5.9402\n",
      "Epoch 278/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 4.8172 - mae: 1.5326 - mse: 4.8172 - val_loss: 7.2546 - val_mae: 1.9052 - val_mse: 7.2546\n",
      "Epoch 279/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 4.9803 - mae: 1.5538 - mse: 4.9803 - val_loss: 7.6262 - val_mae: 1.9524 - val_mse: 7.6262\n",
      "Epoch 280/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 4.7633 - mae: 1.5180 - mse: 4.7633 - val_loss: 6.3630 - val_mae: 1.8317 - val_mse: 6.3630\n",
      "Epoch 281/1000\n",
      "250/250 [==============================] - 0s 59us/sample - loss: 4.8361 - mae: 1.5239 - mse: 4.8361 - val_loss: 7.1677 - val_mae: 1.9084 - val_mse: 7.1677\n",
      "Epoch 282/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 4.7515 - mae: 1.5443 - mse: 4.7515 - val_loss: 6.6627 - val_mae: 1.8570 - val_mse: 6.6627\n",
      "Epoch 283/1000\n",
      "250/250 [==============================] - 0s 60us/sample - loss: 5.0155 - mae: 1.5679 - mse: 5.0155 - val_loss: 6.6997 - val_mae: 1.8560 - val_mse: 6.6997\n",
      "Epoch 284/1000\n",
      "250/250 [==============================] - 0s 59us/sample - loss: 4.7535 - mae: 1.5189 - mse: 4.7535 - val_loss: 7.7869 - val_mae: 1.9789 - val_mse: 7.7869\n",
      "Epoch 285/1000\n",
      "250/250 [==============================] - 0s 58us/sample - loss: 4.8465 - mae: 1.5148 - mse: 4.8465 - val_loss: 6.7708 - val_mae: 1.8713 - val_mse: 6.7708\n",
      "Epoch 286/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 4.8656 - mae: 1.5221 - mse: 4.8656 - val_loss: 7.4982 - val_mae: 1.9412 - val_mse: 7.4982\n",
      "Epoch 287/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 4.8031 - mae: 1.5378 - mse: 4.8031 - val_loss: 8.0358 - val_mae: 1.9909 - val_mse: 8.0358\n",
      "Epoch 288/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 4.7334 - mae: 1.5084 - mse: 4.7334 - val_loss: 6.3813 - val_mae: 1.8160 - val_mse: 6.3813\n",
      "Epoch 289/1000\n",
      "250/250 [==============================] - 0s 60us/sample - loss: 4.8042 - mae: 1.5272 - mse: 4.8042 - val_loss: 7.4575 - val_mae: 1.9630 - val_mse: 7.4575\n",
      "Epoch 290/1000\n",
      "250/250 [==============================] - 0s 61us/sample - loss: 5.0702 - mae: 1.5487 - mse: 5.0702 - val_loss: 6.9723 - val_mae: 1.8967 - val_mse: 6.9723\n",
      "Epoch 291/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 4.7052 - mae: 1.5034 - mse: 4.7052 - val_loss: 7.0203 - val_mae: 1.9016 - val_mse: 7.0203\n",
      "Epoch 292/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 4.7370 - mae: 1.5126 - mse: 4.7370 - val_loss: 7.6811 - val_mae: 1.9614 - val_mse: 7.6811\n",
      "Epoch 293/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 4.8341 - mae: 1.5258 - mse: 4.8341 - val_loss: 6.6645 - val_mae: 1.8563 - val_mse: 6.6645\n",
      "Epoch 294/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 4.7667 - mae: 1.4988 - mse: 4.7667 - val_loss: 6.6381 - val_mae: 1.8490 - val_mse: 6.6381\n",
      "Epoch 295/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 4.7477 - mae: 1.5124 - mse: 4.7477 - val_loss: 8.3160 - val_mae: 2.0058 - val_mse: 8.3160\n",
      "Epoch 296/1000\n",
      "250/250 [==============================] - 0s 80us/sample - loss: 4.7632 - mae: 1.5123 - mse: 4.7632 - val_loss: 7.3572 - val_mae: 1.9251 - val_mse: 7.3572\n",
      "Epoch 297/1000\n",
      "250/250 [==============================] - 0s 86us/sample - loss: 4.7041 - mae: 1.5041 - mse: 4.7041 - val_loss: 6.0979 - val_mae: 1.8304 - val_mse: 6.0979\n",
      "Epoch 298/1000\n",
      "250/250 [==============================] - 0s 96us/sample - loss: 4.7644 - mae: 1.5140 - mse: 4.7644 - val_loss: 6.4607 - val_mae: 1.8420 - val_mse: 6.4607\n",
      "Epoch 299/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 4.8580 - mae: 1.5452 - mse: 4.8580 - val_loss: 8.2263 - val_mae: 2.0079 - val_mse: 8.2263\n",
      "Epoch 300/1000\n",
      "250/250 [==============================] - 0s 59us/sample - loss: 4.5496 - mae: 1.4787 - mse: 4.5496 - val_loss: 8.6479 - val_mae: 2.0711 - val_mse: 8.6479\n",
      "Epoch 301/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 4.7876 - mae: 1.5143 - mse: 4.7876 - val_loss: 8.4201 - val_mae: 2.0421 - val_mse: 8.4201\n",
      "Epoch 302/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 4.8028 - mae: 1.5035 - mse: 4.8028 - val_loss: 6.2923 - val_mae: 1.8006 - val_mse: 6.2923\n",
      "Epoch 303/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 4.7512 - mae: 1.5171 - mse: 4.7512 - val_loss: 7.7335 - val_mae: 1.9605 - val_mse: 7.7335\n",
      "Epoch 304/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 4.8197 - mae: 1.5218 - mse: 4.8197 - val_loss: 8.6183 - val_mae: 2.0594 - val_mse: 8.6183\n",
      "Epoch 305/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 4.7350 - mae: 1.4950 - mse: 4.7350 - val_loss: 8.0564 - val_mae: 2.0092 - val_mse: 8.0564\n",
      "Epoch 306/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 4.8658 - mae: 1.5036 - mse: 4.8658 - val_loss: 8.0643 - val_mae: 1.9875 - val_mse: 8.0643\n",
      "Epoch 307/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 4.9483 - mae: 1.5161 - mse: 4.9483 - val_loss: 8.8561 - val_mae: 2.0892 - val_mse: 8.8561\n",
      "Epoch 308/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 4.8181 - mae: 1.5032 - mse: 4.8181 - val_loss: 6.1940 - val_mae: 1.8087 - val_mse: 6.1940\n",
      "Epoch 309/1000\n",
      "250/250 [==============================] - 0s 60us/sample - loss: 4.5508 - mae: 1.4559 - mse: 4.5508 - val_loss: 8.1916 - val_mae: 2.0114 - val_mse: 8.1916\n",
      "Epoch 310/1000\n",
      "250/250 [==============================] - 0s 58us/sample - loss: 4.7048 - mae: 1.4731 - mse: 4.7048 - val_loss: 6.2690 - val_mae: 1.8190 - val_mse: 6.2690\n",
      "Epoch 311/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 4.6300 - mae: 1.4955 - mse: 4.6300 - val_loss: 6.2596 - val_mae: 1.7983 - val_mse: 6.2596\n",
      "Epoch 312/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 4.8677 - mae: 1.5225 - mse: 4.8677 - val_loss: 7.0478 - val_mae: 1.8744 - val_mse: 7.0478\n",
      "Epoch 313/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 4.8448 - mae: 1.5057 - mse: 4.8448 - val_loss: 7.4229 - val_mae: 1.9417 - val_mse: 7.4229\n",
      "Epoch 314/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 4.6015 - mae: 1.4842 - mse: 4.6015 - val_loss: 8.4079 - val_mae: 2.0244 - val_mse: 8.4079\n",
      "Epoch 315/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 4.6713 - mae: 1.4987 - mse: 4.6713 - val_loss: 6.4837 - val_mae: 1.8276 - val_mse: 6.4837\n",
      "Epoch 316/1000\n",
      "250/250 [==============================] - 0s 80us/sample - loss: 4.9003 - mae: 1.5245 - mse: 4.9003 - val_loss: 6.8213 - val_mae: 1.8533 - val_mse: 6.8213\n",
      "Epoch 317/1000\n",
      "250/250 [==============================] - 0s 82us/sample - loss: 4.5316 - mae: 1.4743 - mse: 4.5316 - val_loss: 6.7607 - val_mae: 1.8761 - val_mse: 6.7607\n",
      "Epoch 318/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 4.9472 - mae: 1.5337 - mse: 4.9472 - val_loss: 6.6667 - val_mae: 1.8488 - val_mse: 6.6667\n",
      "Epoch 319/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 4.5714 - mae: 1.4965 - mse: 4.5714 - val_loss: 6.9937 - val_mae: 1.8642 - val_mse: 6.9937\n",
      "Epoch 320/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 4.5833 - mae: 1.4810 - mse: 4.5833 - val_loss: 7.7766 - val_mae: 1.9818 - val_mse: 7.7766\n",
      "Epoch 321/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 4.7109 - mae: 1.5227 - mse: 4.7109 - val_loss: 9.6698 - val_mae: 2.1972 - val_mse: 9.6698\n",
      "Epoch 322/1000\n",
      "250/250 [==============================] - 0s 60us/sample - loss: 4.7458 - mae: 1.4813 - mse: 4.7458 - val_loss: 7.2683 - val_mae: 1.8865 - val_mse: 7.2683\n",
      "Epoch 323/1000\n",
      "250/250 [==============================] - 0s 61us/sample - loss: 4.6458 - mae: 1.4997 - mse: 4.6458 - val_loss: 7.5054 - val_mae: 1.9101 - val_mse: 7.5054\n",
      "Epoch 324/1000\n",
      "250/250 [==============================] - 0s 81us/sample - loss: 4.8186 - mae: 1.5153 - mse: 4.8186 - val_loss: 7.6943 - val_mae: 1.9355 - val_mse: 7.6943\n",
      "Epoch 325/1000\n",
      "250/250 [==============================] - 0s 80us/sample - loss: 4.6477 - mae: 1.4702 - mse: 4.6477 - val_loss: 6.7139 - val_mae: 1.8380 - val_mse: 6.7139\n",
      "Epoch 326/1000\n",
      "250/250 [==============================] - 0s 87us/sample - loss: 4.7491 - mae: 1.4892 - mse: 4.7491 - val_loss: 6.7801 - val_mae: 1.8487 - val_mse: 6.7801\n",
      "Epoch 327/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 4.6464 - mae: 1.4679 - mse: 4.6464 - val_loss: 7.6404 - val_mae: 1.9248 - val_mse: 7.6404\n",
      "Epoch 328/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 4.5595 - mae: 1.4871 - mse: 4.5595 - val_loss: 6.8425 - val_mae: 1.8543 - val_mse: 6.8425\n",
      "Epoch 329/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 4.7667 - mae: 1.5196 - mse: 4.7667 - val_loss: 7.6379 - val_mae: 1.9202 - val_mse: 7.6379\n",
      "Epoch 330/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 4.6446 - mae: 1.4843 - mse: 4.6446 - val_loss: 8.7706 - val_mae: 2.0910 - val_mse: 8.7706\n",
      "Epoch 331/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 4.7427 - mae: 1.5006 - mse: 4.7427 - val_loss: 6.8367 - val_mae: 1.8464 - val_mse: 6.8367\n",
      "Epoch 332/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 4.4775 - mae: 1.4559 - mse: 4.4775 - val_loss: 6.6195 - val_mae: 1.8284 - val_mse: 6.6195\n",
      "Epoch 333/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 4.9212 - mae: 1.5263 - mse: 4.9212 - val_loss: 6.2413 - val_mae: 1.7924 - val_mse: 6.2413\n",
      "Epoch 334/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 4.8269 - mae: 1.5347 - mse: 4.8269 - val_loss: 6.2717 - val_mae: 1.8159 - val_mse: 6.2717\n",
      "Epoch 335/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 4.6280 - mae: 1.4672 - mse: 4.6280 - val_loss: 6.8661 - val_mae: 1.8500 - val_mse: 6.8661\n",
      "Epoch 336/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 4.5373 - mae: 1.4900 - mse: 4.5373 - val_loss: 6.8422 - val_mae: 1.8500 - val_mse: 6.8422\n",
      "Epoch 337/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 4.6514 - mae: 1.5157 - mse: 4.6514 - val_loss: 7.6673 - val_mae: 1.9514 - val_mse: 7.6673\n",
      "Epoch 338/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 4.8054 - mae: 1.5455 - mse: 4.8054 - val_loss: 8.0655 - val_mae: 1.9844 - val_mse: 8.0655\n",
      "Epoch 339/1000\n",
      "250/250 [==============================] - 0s 80us/sample - loss: 4.4275 - mae: 1.4631 - mse: 4.4276 - val_loss: 8.5560 - val_mae: 2.0380 - val_mse: 8.5560\n",
      "Epoch 340/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 4.7230 - mae: 1.5062 - mse: 4.7230 - val_loss: 7.1017 - val_mae: 1.8748 - val_mse: 7.1017\n",
      "Epoch 341/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 4.5501 - mae: 1.4544 - mse: 4.5501 - val_loss: 6.3735 - val_mae: 1.8092 - val_mse: 6.3735\n",
      "Epoch 342/1000\n",
      "250/250 [==============================] - 0s 61us/sample - loss: 4.8190 - mae: 1.4915 - mse: 4.8190 - val_loss: 7.3103 - val_mae: 1.9092 - val_mse: 7.3103\n",
      "Epoch 343/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 4.4173 - mae: 1.4909 - mse: 4.4173 - val_loss: 9.1337 - val_mae: 2.1176 - val_mse: 9.1337\n",
      "Epoch 344/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 4.3848 - mae: 1.4665 - mse: 4.3848 - val_loss: 7.4975 - val_mae: 1.9064 - val_mse: 7.4975\n",
      "Epoch 345/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 4.7024 - mae: 1.4908 - mse: 4.7024 - val_loss: 7.7127 - val_mae: 1.9326 - val_mse: 7.7127\n",
      "Epoch 346/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 4.6184 - mae: 1.4685 - mse: 4.6184 - val_loss: 8.3070 - val_mae: 2.0027 - val_mse: 8.3070\n",
      "Epoch 347/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 4.5556 - mae: 1.4570 - mse: 4.5556 - val_loss: 6.4151 - val_mae: 1.8093 - val_mse: 6.4151\n",
      "Epoch 348/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 4.5251 - mae: 1.4819 - mse: 4.5251 - val_loss: 7.0987 - val_mae: 1.9363 - val_mse: 7.0987\n",
      "Epoch 349/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 4.6020 - mae: 1.4906 - mse: 4.6020 - val_loss: 6.9836 - val_mae: 1.8563 - val_mse: 6.9836\n",
      "Epoch 350/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 4.4995 - mae: 1.4487 - mse: 4.4995 - val_loss: 6.9268 - val_mae: 1.8468 - val_mse: 6.9268\n",
      "Epoch 351/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 4.6354 - mae: 1.4669 - mse: 4.6354 - val_loss: 8.5232 - val_mae: 2.0337 - val_mse: 8.5232\n",
      "Epoch 352/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 4.5899 - mae: 1.4441 - mse: 4.5899 - val_loss: 6.6643 - val_mae: 1.8532 - val_mse: 6.6643\n",
      "Epoch 353/1000\n",
      "250/250 [==============================] - 0s 79us/sample - loss: 4.5236 - mae: 1.4898 - mse: 4.5236 - val_loss: 7.8597 - val_mae: 1.9414 - val_mse: 7.8597\n",
      "Epoch 354/1000\n",
      "250/250 [==============================] - 0s 85us/sample - loss: 4.4980 - mae: 1.4561 - mse: 4.4980 - val_loss: 7.8689 - val_mae: 1.9569 - val_mse: 7.8689\n",
      "Epoch 355/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 4.4587 - mae: 1.4616 - mse: 4.4587 - val_loss: 6.5930 - val_mae: 1.8135 - val_mse: 6.5930\n",
      "Epoch 356/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 4.5984 - mae: 1.4716 - mse: 4.5984 - val_loss: 7.0581 - val_mae: 1.8675 - val_mse: 7.0581\n",
      "Epoch 357/1000\n",
      "250/250 [==============================] - 0s 96us/sample - loss: 4.4079 - mae: 1.4621 - mse: 4.4079 - val_loss: 8.1426 - val_mae: 1.9824 - val_mse: 8.1426\n",
      "Epoch 358/1000\n",
      "250/250 [==============================] - 0s 87us/sample - loss: 4.6700 - mae: 1.4722 - mse: 4.6700 - val_loss: 7.1422 - val_mae: 1.9051 - val_mse: 7.1422\n",
      "Epoch 359/1000\n",
      "250/250 [==============================] - 0s 61us/sample - loss: 4.5303 - mae: 1.4812 - mse: 4.5303 - val_loss: 8.0527 - val_mae: 1.9817 - val_mse: 8.0527\n",
      "Epoch 360/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 4.4667 - mae: 1.4619 - mse: 4.4667 - val_loss: 7.5386 - val_mae: 1.9248 - val_mse: 7.5386\n",
      "Epoch 361/1000\n",
      "250/250 [==============================] - 0s 78us/sample - loss: 4.6574 - mae: 1.4676 - mse: 4.6574 - val_loss: 6.6991 - val_mae: 1.8321 - val_mse: 6.6991\n",
      "Epoch 362/1000\n",
      "250/250 [==============================] - 0s 86us/sample - loss: 4.4852 - mae: 1.4572 - mse: 4.4852 - val_loss: 8.9318 - val_mae: 2.0903 - val_mse: 8.9318\n",
      "Epoch 363/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 4.5353 - mae: 1.4496 - mse: 4.5353 - val_loss: 8.3973 - val_mae: 2.0189 - val_mse: 8.3973\n",
      "Epoch 364/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 4.2593 - mae: 1.4449 - mse: 4.2593 - val_loss: 7.7374 - val_mae: 1.9245 - val_mse: 7.7374\n",
      "Epoch 365/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 4.3631 - mae: 1.4462 - mse: 4.3631 - val_loss: 10.0045 - val_mae: 2.2221 - val_mse: 10.0045\n",
      "Epoch 366/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 4.5198 - mae: 1.4722 - mse: 4.5198 - val_loss: 7.6189 - val_mae: 1.9074 - val_mse: 7.6189\n",
      "Epoch 367/1000\n",
      "250/250 [==============================] - 0s 79us/sample - loss: 4.5085 - mae: 1.4530 - mse: 4.5085 - val_loss: 8.0441 - val_mae: 1.9535 - val_mse: 8.0441\n",
      "Epoch 368/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 4.4030 - mae: 1.4494 - mse: 4.4030 - val_loss: 8.0965 - val_mae: 1.9813 - val_mse: 8.0965\n",
      "Epoch 369/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 4.4414 - mae: 1.4590 - mse: 4.4414 - val_loss: 6.8222 - val_mae: 1.8345 - val_mse: 6.8222\n",
      "Epoch 370/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 4.4797 - mae: 1.4849 - mse: 4.4797 - val_loss: 6.5524 - val_mae: 1.8135 - val_mse: 6.5524\n",
      "Epoch 371/1000\n",
      "250/250 [==============================] - 0s 60us/sample - loss: 4.3543 - mae: 1.4645 - mse: 4.3543 - val_loss: 10.4707 - val_mae: 2.2597 - val_mse: 10.4707\n",
      "Epoch 372/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 4.5690 - mae: 1.4751 - mse: 4.5690 - val_loss: 7.1078 - val_mae: 1.8508 - val_mse: 7.1078\n",
      "Epoch 373/1000\n",
      "250/250 [==============================] - 0s 85us/sample - loss: 4.3382 - mae: 1.4380 - mse: 4.3382 - val_loss: 8.6586 - val_mae: 2.0311 - val_mse: 8.6586\n",
      "Epoch 374/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 4.4412 - mae: 1.4407 - mse: 4.4412 - val_loss: 7.1593 - val_mae: 1.8514 - val_mse: 7.1593\n",
      "Epoch 375/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 4.4508 - mae: 1.4737 - mse: 4.4508 - val_loss: 8.6621 - val_mae: 2.0231 - val_mse: 8.6621\n",
      "Epoch 376/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 4.3884 - mae: 1.4404 - mse: 4.3884 - val_loss: 8.6384 - val_mae: 2.0301 - val_mse: 8.6384\n",
      "Epoch 377/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 4.2469 - mae: 1.4392 - mse: 4.2469 - val_loss: 6.1422 - val_mae: 1.7836 - val_mse: 6.1422\n",
      "Epoch 378/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 4.3884 - mae: 1.4478 - mse: 4.3884 - val_loss: 8.3617 - val_mae: 1.9815 - val_mse: 8.3617\n",
      "Epoch 379/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 4.3496 - mae: 1.4404 - mse: 4.3496 - val_loss: 7.0894 - val_mae: 1.8787 - val_mse: 7.0894\n",
      "Epoch 380/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 4.2454 - mae: 1.4118 - mse: 4.2454 - val_loss: 6.9405 - val_mae: 1.8269 - val_mse: 6.9405\n",
      "Epoch 381/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 4.6111 - mae: 1.5292 - mse: 4.6111 - val_loss: 8.0643 - val_mae: 1.9503 - val_mse: 8.0643\n",
      "Epoch 382/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 4.4715 - mae: 1.4630 - mse: 4.4715 - val_loss: 8.1942 - val_mae: 1.9709 - val_mse: 8.1942\n",
      "Epoch 383/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 4.4027 - mae: 1.4481 - mse: 4.4027 - val_loss: 8.6728 - val_mae: 2.0301 - val_mse: 8.6728\n",
      "Epoch 384/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 4.3332 - mae: 1.4522 - mse: 4.3332 - val_loss: 6.8477 - val_mae: 1.8174 - val_mse: 6.8477\n",
      "Epoch 385/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 4.5453 - mae: 1.4520 - mse: 4.5453 - val_loss: 6.8692 - val_mae: 1.8338 - val_mse: 6.8692\n",
      "Epoch 386/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 4.3562 - mae: 1.4597 - mse: 4.3562 - val_loss: 6.4732 - val_mae: 1.7992 - val_mse: 6.4732\n",
      "Epoch 387/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 4.3946 - mae: 1.4543 - mse: 4.3946 - val_loss: 6.7066 - val_mae: 1.8247 - val_mse: 6.7066\n",
      "Epoch 388/1000\n",
      "250/250 [==============================] - 0s 93us/sample - loss: 4.4729 - mae: 1.4448 - mse: 4.4729 - val_loss: 6.5262 - val_mae: 1.8019 - val_mse: 6.5262\n",
      "Epoch 389/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 4.2395 - mae: 1.4159 - mse: 4.2395 - val_loss: 7.9952 - val_mae: 1.9844 - val_mse: 7.9952\n",
      "Epoch 390/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 4.4463 - mae: 1.4656 - mse: 4.4463 - val_loss: 6.6174 - val_mae: 1.8208 - val_mse: 6.6174\n",
      "Epoch 391/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 4.3327 - mae: 1.4364 - mse: 4.3327 - val_loss: 8.1354 - val_mae: 1.9599 - val_mse: 8.1354\n",
      "Epoch 392/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 4.4894 - mae: 1.4531 - mse: 4.4894 - val_loss: 6.5369 - val_mae: 1.7951 - val_mse: 6.5369\n",
      "Epoch 393/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 4.3278 - mae: 1.4438 - mse: 4.3278 - val_loss: 8.9888 - val_mae: 2.0616 - val_mse: 8.9888\n",
      "Epoch 394/1000\n",
      "250/250 [==============================] - 0s 81us/sample - loss: 4.5932 - mae: 1.4690 - mse: 4.5932 - val_loss: 8.0017 - val_mae: 1.9458 - val_mse: 8.0017\n",
      "Epoch 395/1000\n",
      "250/250 [==============================] - 0s 78us/sample - loss: 4.2353 - mae: 1.4151 - mse: 4.2353 - val_loss: 7.2537 - val_mae: 1.8448 - val_mse: 7.2537\n",
      "Epoch 396/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 4.3970 - mae: 1.4813 - mse: 4.3970 - val_loss: 8.3997 - val_mae: 2.0007 - val_mse: 8.3997\n",
      "Epoch 397/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 4.5759 - mae: 1.4558 - mse: 4.5759 - val_loss: 8.0603 - val_mae: 1.9526 - val_mse: 8.0603\n",
      "Epoch 398/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 4.3367 - mae: 1.4305 - mse: 4.3367 - val_loss: 9.0241 - val_mae: 2.0699 - val_mse: 9.0241\n",
      "Epoch 399/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 4.2763 - mae: 1.4142 - mse: 4.2763 - val_loss: 6.8343 - val_mae: 1.8081 - val_mse: 6.8343\n",
      "Epoch 400/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 4.4469 - mae: 1.4517 - mse: 4.4469 - val_loss: 8.5427 - val_mae: 2.0034 - val_mse: 8.5427\n",
      "Epoch 401/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 4.2687 - mae: 1.4437 - mse: 4.2687 - val_loss: 7.9494 - val_mae: 1.9356 - val_mse: 7.9494\n",
      "Epoch 402/1000\n",
      "250/250 [==============================] - 0s 82us/sample - loss: 4.1983 - mae: 1.4223 - mse: 4.1983 - val_loss: 8.3915 - val_mae: 1.9997 - val_mse: 8.3915\n",
      "Epoch 403/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 4.3089 - mae: 1.4296 - mse: 4.3089 - val_loss: 7.5499 - val_mae: 1.8658 - val_mse: 7.5499\n",
      "Epoch 404/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 4.4396 - mae: 1.4343 - mse: 4.4396 - val_loss: 9.0322 - val_mae: 2.0678 - val_mse: 9.0322\n",
      "Epoch 405/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 4.4663 - mae: 1.4387 - mse: 4.4663 - val_loss: 6.9441 - val_mae: 1.8314 - val_mse: 6.9441\n",
      "Epoch 406/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 4.2188 - mae: 1.3889 - mse: 4.2188 - val_loss: 6.2551 - val_mae: 1.7901 - val_mse: 6.2551\n",
      "Epoch 407/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 4.4152 - mae: 1.4784 - mse: 4.4152 - val_loss: 7.5766 - val_mae: 1.9105 - val_mse: 7.5766\n",
      "Epoch 408/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 4.5317 - mae: 1.4353 - mse: 4.5317 - val_loss: 7.4810 - val_mae: 1.8629 - val_mse: 7.4810\n",
      "Epoch 409/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 4.2776 - mae: 1.4149 - mse: 4.2776 - val_loss: 8.5939 - val_mae: 2.0107 - val_mse: 8.5939\n",
      "Epoch 410/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 4.2174 - mae: 1.4072 - mse: 4.2174 - val_loss: 7.9940 - val_mae: 1.9382 - val_mse: 7.9940\n",
      "Epoch 411/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 4.3834 - mae: 1.4446 - mse: 4.3834 - val_loss: 9.0634 - val_mae: 2.0575 - val_mse: 9.0634\n",
      "Epoch 412/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 4.1989 - mae: 1.3900 - mse: 4.1989 - val_loss: 7.0137 - val_mae: 1.8270 - val_mse: 7.0137\n",
      "Epoch 413/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 4.1845 - mae: 1.4299 - mse: 4.1845 - val_loss: 6.3116 - val_mae: 1.7754 - val_mse: 6.3116\n",
      "Epoch 414/1000\n",
      "250/250 [==============================] - 0s 81us/sample - loss: 4.3601 - mae: 1.4231 - mse: 4.3601 - val_loss: 8.3595 - val_mae: 1.9881 - val_mse: 8.3595\n",
      "Epoch 415/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 4.3228 - mae: 1.4027 - mse: 4.3228 - val_loss: 6.3510 - val_mae: 1.8797 - val_mse: 6.3510\n",
      "Epoch 416/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 4.3545 - mae: 1.4817 - mse: 4.3545 - val_loss: 7.0727 - val_mae: 1.8133 - val_mse: 7.0727\n",
      "Epoch 417/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 4.2498 - mae: 1.4129 - mse: 4.2498 - val_loss: 7.7286 - val_mae: 1.9402 - val_mse: 7.7286\n",
      "Epoch 418/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 4.2748 - mae: 1.4140 - mse: 4.2748 - val_loss: 7.5236 - val_mae: 1.8569 - val_mse: 7.5236\n",
      "Epoch 419/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 4.2009 - mae: 1.4434 - mse: 4.2009 - val_loss: 7.0919 - val_mae: 1.8086 - val_mse: 7.0919\n",
      "Epoch 420/1000\n",
      "250/250 [==============================] - 0s 81us/sample - loss: 4.3307 - mae: 1.4456 - mse: 4.3307 - val_loss: 7.6991 - val_mae: 1.8803 - val_mse: 7.6991\n",
      "Epoch 421/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 4.3198 - mae: 1.4075 - mse: 4.3198 - val_loss: 6.4337 - val_mae: 1.8119 - val_mse: 6.4337\n",
      "Epoch 422/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 4.3676 - mae: 1.4530 - mse: 4.3676 - val_loss: 7.2322 - val_mae: 1.8449 - val_mse: 7.2322\n",
      "Epoch 423/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 4.1859 - mae: 1.4050 - mse: 4.1859 - val_loss: 7.0888 - val_mae: 1.8350 - val_mse: 7.0888\n",
      "Epoch 424/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 4.3100 - mae: 1.4115 - mse: 4.3100 - val_loss: 7.7683 - val_mae: 1.9226 - val_mse: 7.7683\n",
      "Epoch 425/1000\n",
      "250/250 [==============================] - 0s 61us/sample - loss: 4.3763 - mae: 1.4111 - mse: 4.3763 - val_loss: 7.8831 - val_mae: 1.9136 - val_mse: 7.8831\n",
      "Epoch 426/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 4.3111 - mae: 1.4356 - mse: 4.3111 - val_loss: 8.1544 - val_mae: 1.9354 - val_mse: 8.1544\n",
      "Epoch 427/1000\n",
      "250/250 [==============================] - 0s 78us/sample - loss: 4.3168 - mae: 1.4305 - mse: 4.3168 - val_loss: 7.9781 - val_mae: 1.9120 - val_mse: 7.9781\n",
      "Epoch 428/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 4.2893 - mae: 1.4021 - mse: 4.2893 - val_loss: 6.5048 - val_mae: 1.7772 - val_mse: 6.5048\n",
      "Epoch 429/1000\n",
      "250/250 [==============================] - 0s 82us/sample - loss: 4.4045 - mae: 1.4563 - mse: 4.4045 - val_loss: 7.6759 - val_mae: 1.8817 - val_mse: 7.6759\n",
      "Epoch 430/1000\n",
      "250/250 [==============================] - 0s 82us/sample - loss: 4.1462 - mae: 1.3884 - mse: 4.1462 - val_loss: 7.3167 - val_mae: 1.8204 - val_mse: 7.3167\n",
      "Epoch 431/1000\n",
      "250/250 [==============================] - 0s 78us/sample - loss: 4.1112 - mae: 1.3787 - mse: 4.1112 - val_loss: 8.5910 - val_mae: 1.9999 - val_mse: 8.5910\n",
      "Epoch 432/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 4.2780 - mae: 1.4226 - mse: 4.2780 - val_loss: 6.8908 - val_mae: 1.8722 - val_mse: 6.8908\n",
      "Epoch 433/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 4.2244 - mae: 1.4129 - mse: 4.2244 - val_loss: 7.1938 - val_mae: 1.8785 - val_mse: 7.1938\n",
      "Epoch 434/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 4.1156 - mae: 1.4061 - mse: 4.1156 - val_loss: 7.9582 - val_mae: 1.9376 - val_mse: 7.9582\n",
      "Epoch 435/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 4.2111 - mae: 1.4223 - mse: 4.2111 - val_loss: 6.6686 - val_mae: 1.8145 - val_mse: 6.6686\n",
      "Epoch 436/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 4.2567 - mae: 1.4260 - mse: 4.2567 - val_loss: 7.8366 - val_mae: 1.9018 - val_mse: 7.8366\n",
      "Epoch 437/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 4.0263 - mae: 1.4045 - mse: 4.0263 - val_loss: 8.2034 - val_mae: 1.9364 - val_mse: 8.2034\n",
      "Epoch 438/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 4.2772 - mae: 1.4193 - mse: 4.2772 - val_loss: 8.1658 - val_mae: 1.9363 - val_mse: 8.1658\n",
      "Epoch 439/1000\n",
      "250/250 [==============================] - 0s 81us/sample - loss: 4.1597 - mae: 1.3967 - mse: 4.1597 - val_loss: 6.3382 - val_mae: 1.7946 - val_mse: 6.3382\n",
      "Epoch 440/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 4.1035 - mae: 1.3881 - mse: 4.1035 - val_loss: 8.5908 - val_mae: 1.9982 - val_mse: 8.5908\n",
      "Epoch 441/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 4.3355 - mae: 1.4287 - mse: 4.3355 - val_loss: 7.7091 - val_mae: 1.9603 - val_mse: 7.7091\n",
      "Epoch 442/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 4.2755 - mae: 1.3780 - mse: 4.2755 - val_loss: 6.5498 - val_mae: 1.7931 - val_mse: 6.5498\n",
      "Epoch 443/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 4.1577 - mae: 1.3828 - mse: 4.1577 - val_loss: 6.7281 - val_mae: 1.8661 - val_mse: 6.7281\n",
      "Epoch 444/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 4.2056 - mae: 1.3963 - mse: 4.2056 - val_loss: 8.0106 - val_mae: 1.9771 - val_mse: 8.0106\n",
      "Epoch 445/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 4.2173 - mae: 1.4431 - mse: 4.2173 - val_loss: 8.7750 - val_mae: 2.0329 - val_mse: 8.7750\n",
      "Epoch 446/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 4.3295 - mae: 1.4452 - mse: 4.3295 - val_loss: 8.1460 - val_mae: 1.9137 - val_mse: 8.1460\n",
      "Epoch 447/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 4.3225 - mae: 1.4453 - mse: 4.3225 - val_loss: 7.8652 - val_mae: 1.9037 - val_mse: 7.8652\n",
      "Epoch 448/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 4.0612 - mae: 1.3909 - mse: 4.0612 - val_loss: 7.3993 - val_mae: 1.8325 - val_mse: 7.3993\n",
      "Epoch 449/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 4.0442 - mae: 1.3937 - mse: 4.0442 - val_loss: 7.5365 - val_mae: 1.8553 - val_mse: 7.5365\n",
      "Epoch 450/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 4.0819 - mae: 1.3986 - mse: 4.0819 - val_loss: 7.3916 - val_mae: 1.8334 - val_mse: 7.3916\n",
      "Epoch 451/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 4.0847 - mae: 1.3797 - mse: 4.0847 - val_loss: 6.8244 - val_mae: 1.8486 - val_mse: 6.8244\n",
      "Epoch 452/1000\n",
      "250/250 [==============================] - 0s 82us/sample - loss: 4.4893 - mae: 1.4657 - mse: 4.4893 - val_loss: 7.6388 - val_mae: 1.9245 - val_mse: 7.6388\n",
      "Epoch 453/1000\n",
      "250/250 [==============================] - 0s 85us/sample - loss: 4.0598 - mae: 1.3938 - mse: 4.0598 - val_loss: 7.2002 - val_mae: 1.8413 - val_mse: 7.2002\n",
      "Epoch 454/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 4.3021 - mae: 1.4222 - mse: 4.3021 - val_loss: 6.9188 - val_mae: 1.7969 - val_mse: 6.9188\n",
      "Epoch 455/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 4.0729 - mae: 1.3947 - mse: 4.0729 - val_loss: 6.8126 - val_mae: 1.8336 - val_mse: 6.8126\n",
      "Epoch 456/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 4.1039 - mae: 1.3890 - mse: 4.1039 - val_loss: 8.4917 - val_mae: 1.9752 - val_mse: 8.4917\n",
      "Epoch 457/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 4.0926 - mae: 1.3634 - mse: 4.0926 - val_loss: 7.0167 - val_mae: 1.8396 - val_mse: 7.0167\n",
      "Epoch 458/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 4.1933 - mae: 1.3929 - mse: 4.1933 - val_loss: 6.4899 - val_mae: 1.8213 - val_mse: 6.4899\n",
      "Epoch 459/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 4.1525 - mae: 1.4199 - mse: 4.1525 - val_loss: 7.3038 - val_mae: 1.8106 - val_mse: 7.3038\n",
      "Epoch 460/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 3.9273 - mae: 1.3552 - mse: 3.9273 - val_loss: 8.1218 - val_mae: 1.9786 - val_mse: 8.1218\n",
      "Epoch 461/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 4.2165 - mae: 1.4307 - mse: 4.2165 - val_loss: 8.9874 - val_mae: 2.0302 - val_mse: 8.9874\n",
      "Epoch 462/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 4.0527 - mae: 1.3670 - mse: 4.0527 - val_loss: 7.6324 - val_mae: 1.8411 - val_mse: 7.6324\n",
      "Epoch 463/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 4.0944 - mae: 1.3936 - mse: 4.0944 - val_loss: 7.5433 - val_mae: 1.8612 - val_mse: 7.5433\n",
      "Epoch 464/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 4.2296 - mae: 1.3838 - mse: 4.2296 - val_loss: 7.7180 - val_mae: 1.8784 - val_mse: 7.7180\n",
      "Epoch 465/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 4.0497 - mae: 1.3540 - mse: 4.0497 - val_loss: 7.3342 - val_mae: 1.9279 - val_mse: 7.3342\n",
      "Epoch 466/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 4.0967 - mae: 1.4005 - mse: 4.0967 - val_loss: 9.0035 - val_mae: 2.0311 - val_mse: 9.0035\n",
      "Epoch 467/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 4.1710 - mae: 1.3771 - mse: 4.1710 - val_loss: 7.1401 - val_mae: 1.8431 - val_mse: 7.1401\n",
      "Epoch 468/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 3.9812 - mae: 1.3688 - mse: 3.9812 - val_loss: 8.1327 - val_mae: 1.9255 - val_mse: 8.1327\n",
      "Epoch 469/1000\n",
      "250/250 [==============================] - 0s 60us/sample - loss: 3.9706 - mae: 1.3957 - mse: 3.9706 - val_loss: 8.5112 - val_mae: 1.9334 - val_mse: 8.5112\n",
      "Epoch 470/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 3.8994 - mae: 1.3660 - mse: 3.8994 - val_loss: 6.3552 - val_mae: 1.7543 - val_mse: 6.3552\n",
      "Epoch 471/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 4.1946 - mae: 1.4220 - mse: 4.1946 - val_loss: 7.5213 - val_mae: 1.8803 - val_mse: 7.5213\n",
      "Epoch 472/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 3.9254 - mae: 1.3552 - mse: 3.9254 - val_loss: 6.6302 - val_mae: 1.8637 - val_mse: 6.6302\n",
      "Epoch 473/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 4.1313 - mae: 1.4301 - mse: 4.1313 - val_loss: 8.5283 - val_mae: 1.9657 - val_mse: 8.5283\n",
      "Epoch 474/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 4.0134 - mae: 1.3717 - mse: 4.0134 - val_loss: 6.8819 - val_mae: 1.8150 - val_mse: 6.8819\n",
      "Epoch 475/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 4.0726 - mae: 1.4017 - mse: 4.0726 - val_loss: 6.8189 - val_mae: 1.7791 - val_mse: 6.8189\n",
      "Epoch 476/1000\n",
      "250/250 [==============================] - 0s 81us/sample - loss: 4.0335 - mae: 1.3646 - mse: 4.0335 - val_loss: 6.9746 - val_mae: 1.8633 - val_mse: 6.9746\n",
      "Epoch 477/1000\n",
      "250/250 [==============================] - 0s 82us/sample - loss: 4.1614 - mae: 1.4045 - mse: 4.1614 - val_loss: 7.9722 - val_mae: 1.8879 - val_mse: 7.9722\n",
      "Epoch 478/1000\n",
      "250/250 [==============================] - 0s 78us/sample - loss: 4.0565 - mae: 1.3640 - mse: 4.0565 - val_loss: 8.2797 - val_mae: 1.9208 - val_mse: 8.2797\n",
      "Epoch 479/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 4.1312 - mae: 1.3836 - mse: 4.1312 - val_loss: 7.3371 - val_mae: 1.8750 - val_mse: 7.3371\n",
      "Epoch 480/1000\n",
      "250/250 [==============================] - 0s 99us/sample - loss: 3.9519 - mae: 1.3727 - mse: 3.9519 - val_loss: 7.5702 - val_mae: 1.8769 - val_mse: 7.5702\n",
      "Epoch 481/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 3.9154 - mae: 1.3574 - mse: 3.9154 - val_loss: 8.1247 - val_mae: 1.8926 - val_mse: 8.1247\n",
      "Epoch 482/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 3.9671 - mae: 1.3327 - mse: 3.9671 - val_loss: 8.0365 - val_mae: 1.9493 - val_mse: 8.0365\n",
      "Epoch 483/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 3.9687 - mae: 1.3772 - mse: 3.9687 - val_loss: 7.7589 - val_mae: 1.8534 - val_mse: 7.7589\n",
      "Epoch 484/1000\n",
      "250/250 [==============================] - 0s 58us/sample - loss: 4.1557 - mae: 1.3864 - mse: 4.1557 - val_loss: 8.0277 - val_mae: 1.9401 - val_mse: 8.0277\n",
      "Epoch 485/1000\n",
      "250/250 [==============================] - 0s 58us/sample - loss: 3.9584 - mae: 1.3681 - mse: 3.9584 - val_loss: 6.4984 - val_mae: 1.8217 - val_mse: 6.4984\n",
      "Epoch 486/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 4.0934 - mae: 1.3967 - mse: 4.0934 - val_loss: 7.1489 - val_mae: 1.7929 - val_mse: 7.1489\n",
      "Epoch 487/1000\n",
      "250/250 [==============================] - 0s 97us/sample - loss: 3.9870 - mae: 1.3696 - mse: 3.9870 - val_loss: 7.5455 - val_mae: 1.8367 - val_mse: 7.5455\n",
      "Epoch 488/1000\n",
      "250/250 [==============================] - 0s 88us/sample - loss: 4.0957 - mae: 1.3559 - mse: 4.0957 - val_loss: 7.4072 - val_mae: 1.8129 - val_mse: 7.4072\n",
      "Epoch 489/1000\n",
      "250/250 [==============================] - 0s 103us/sample - loss: 4.1111 - mae: 1.3810 - mse: 4.1111 - val_loss: 8.6628 - val_mae: 1.9859 - val_mse: 8.6628\n",
      "Epoch 490/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 4.0101 - mae: 1.3511 - mse: 4.0101 - val_loss: 7.7702 - val_mae: 1.8561 - val_mse: 7.7702\n",
      "Epoch 491/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 3.9358 - mae: 1.3611 - mse: 3.9358 - val_loss: 8.7798 - val_mae: 1.9963 - val_mse: 8.7798\n",
      "Epoch 492/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 4.1035 - mae: 1.3527 - mse: 4.1035 - val_loss: 8.5739 - val_mae: 1.9664 - val_mse: 8.5739\n",
      "Epoch 493/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 4.0057 - mae: 1.3431 - mse: 4.0057 - val_loss: 8.6722 - val_mae: 1.9986 - val_mse: 8.6722\n",
      "Epoch 494/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 3.8870 - mae: 1.3538 - mse: 3.8870 - val_loss: 8.0050 - val_mae: 1.8963 - val_mse: 8.0050\n",
      "Epoch 495/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 3.9429 - mae: 1.3656 - mse: 3.9429 - val_loss: 7.0929 - val_mae: 1.8259 - val_mse: 7.0929\n",
      "Epoch 496/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 3.9584 - mae: 1.4051 - mse: 3.9584 - val_loss: 9.0161 - val_mae: 2.0222 - val_mse: 9.0161\n",
      "Epoch 497/1000\n",
      "250/250 [==============================] - 0s 79us/sample - loss: 3.9639 - mae: 1.3516 - mse: 3.9639 - val_loss: 7.0392 - val_mae: 1.8005 - val_mse: 7.0392\n",
      "Epoch 498/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 3.8739 - mae: 1.3606 - mse: 3.8739 - val_loss: 7.2375 - val_mae: 1.8674 - val_mse: 7.2375\n",
      "Epoch 499/1000\n",
      "250/250 [==============================] - 0s 89us/sample - loss: 4.0986 - mae: 1.3791 - mse: 4.0986 - val_loss: 8.2322 - val_mae: 1.9183 - val_mse: 8.2322\n",
      "Epoch 500/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 3.8572 - mae: 1.3670 - mse: 3.8572 - val_loss: 7.8094 - val_mae: 1.8733 - val_mse: 7.8094\n",
      "Epoch 501/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 4.1899 - mae: 1.3648 - mse: 4.1899 - val_loss: 8.1836 - val_mae: 1.9205 - val_mse: 8.1836\n",
      "Epoch 502/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 3.7658 - mae: 1.3310 - mse: 3.7658 - val_loss: 8.5749 - val_mae: 2.0207 - val_mse: 8.5749\n",
      "Epoch 503/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 4.0424 - mae: 1.3885 - mse: 4.0424 - val_loss: 7.3436 - val_mae: 1.8280 - val_mse: 7.3436\n",
      "Epoch 504/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 3.8852 - mae: 1.3515 - mse: 3.8852 - val_loss: 9.1121 - val_mae: 2.0355 - val_mse: 9.1121\n",
      "Epoch 505/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 4.0620 - mae: 1.3747 - mse: 4.0620 - val_loss: 10.0353 - val_mae: 2.1359 - val_mse: 10.0353\n",
      "Epoch 506/1000\n",
      "250/250 [==============================] - 0s 88us/sample - loss: 3.8827 - mae: 1.3771 - mse: 3.8827 - val_loss: 8.2406 - val_mae: 1.9012 - val_mse: 8.2406\n",
      "Epoch 507/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 3.8988 - mae: 1.3714 - mse: 3.8988 - val_loss: 8.6646 - val_mae: 1.9484 - val_mse: 8.6646\n",
      "Epoch 508/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 3.9294 - mae: 1.3573 - mse: 3.9294 - val_loss: 7.0756 - val_mae: 1.8202 - val_mse: 7.0756\n",
      "Epoch 509/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 3.9850 - mae: 1.4041 - mse: 3.9850 - val_loss: 7.7478 - val_mae: 1.8284 - val_mse: 7.7478\n",
      "Epoch 510/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 3.9400 - mae: 1.3672 - mse: 3.9400 - val_loss: 10.2802 - val_mae: 2.1604 - val_mse: 10.2802\n",
      "Epoch 511/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 4.0823 - mae: 1.3682 - mse: 4.0823 - val_loss: 7.7125 - val_mae: 1.8627 - val_mse: 7.7125\n",
      "Epoch 512/1000\n",
      "250/250 [==============================] - 0s 83us/sample - loss: 4.0138 - mae: 1.3645 - mse: 4.0138 - val_loss: 7.1334 - val_mae: 1.8039 - val_mse: 7.1334\n",
      "Epoch 513/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 4.1522 - mae: 1.3742 - mse: 4.1522 - val_loss: 8.7586 - val_mae: 1.9918 - val_mse: 8.7586\n",
      "Epoch 514/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 3.8570 - mae: 1.3416 - mse: 3.8570 - val_loss: 7.5249 - val_mae: 1.8468 - val_mse: 7.5249\n",
      "Epoch 515/1000\n",
      "250/250 [==============================] - 0s 79us/sample - loss: 3.8074 - mae: 1.3353 - mse: 3.8074 - val_loss: 8.0348 - val_mae: 1.9233 - val_mse: 8.0348\n",
      "Epoch 516/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 4.1167 - mae: 1.3692 - mse: 4.1167 - val_loss: 7.1110 - val_mae: 1.8078 - val_mse: 7.1110\n",
      "Epoch 517/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 3.8954 - mae: 1.3143 - mse: 3.8954 - val_loss: 8.7457 - val_mae: 1.9620 - val_mse: 8.7457\n",
      "Epoch 518/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 3.7533 - mae: 1.3136 - mse: 3.7533 - val_loss: 8.9640 - val_mae: 2.1033 - val_mse: 8.9640\n",
      "Epoch 519/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 3.8736 - mae: 1.3612 - mse: 3.8736 - val_loss: 8.4992 - val_mae: 1.9409 - val_mse: 8.4992\n",
      "Epoch 520/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 4.0409 - mae: 1.3630 - mse: 4.0409 - val_loss: 8.0119 - val_mae: 1.8891 - val_mse: 8.0119\n",
      "Epoch 521/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 3.8509 - mae: 1.3319 - mse: 3.8509 - val_loss: 8.7522 - val_mae: 2.0057 - val_mse: 8.7522\n",
      "Epoch 522/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 3.7060 - mae: 1.2880 - mse: 3.7060 - val_loss: 9.6330 - val_mae: 2.0722 - val_mse: 9.6330\n",
      "Epoch 523/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 3.8123 - mae: 1.3320 - mse: 3.8123 - val_loss: 7.4360 - val_mae: 1.9133 - val_mse: 7.4360\n",
      "Epoch 524/1000\n",
      "250/250 [==============================] - 0s 81us/sample - loss: 4.0047 - mae: 1.3882 - mse: 4.0047 - val_loss: 8.0455 - val_mae: 1.9702 - val_mse: 8.0455\n",
      "Epoch 525/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 3.9096 - mae: 1.3511 - mse: 3.9096 - val_loss: 8.2344 - val_mae: 1.9940 - val_mse: 8.2344\n",
      "Epoch 526/1000\n",
      "250/250 [==============================] - 0s 61us/sample - loss: 3.8985 - mae: 1.3266 - mse: 3.8985 - val_loss: 7.2318 - val_mae: 1.8150 - val_mse: 7.2318\n",
      "Epoch 527/1000\n",
      "250/250 [==============================] - 0s 59us/sample - loss: 3.7875 - mae: 1.3078 - mse: 3.7875 - val_loss: 9.4601 - val_mae: 2.0457 - val_mse: 9.4601\n",
      "Epoch 528/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 3.9016 - mae: 1.3684 - mse: 3.9016 - val_loss: 7.1919 - val_mae: 1.7998 - val_mse: 7.1919\n",
      "Epoch 529/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 3.8742 - mae: 1.3355 - mse: 3.8742 - val_loss: 7.1586 - val_mae: 1.7757 - val_mse: 7.1586\n",
      "Epoch 530/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 3.8042 - mae: 1.3031 - mse: 3.8042 - val_loss: 8.6425 - val_mae: 1.9724 - val_mse: 8.6425\n",
      "Epoch 531/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 3.8495 - mae: 1.3288 - mse: 3.8495 - val_loss: 8.7893 - val_mae: 1.9768 - val_mse: 8.7893\n",
      "Epoch 532/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 3.8052 - mae: 1.3303 - mse: 3.8052 - val_loss: 7.3736 - val_mae: 1.8226 - val_mse: 7.3736\n",
      "Epoch 533/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 3.8640 - mae: 1.3365 - mse: 3.8640 - val_loss: 8.6297 - val_mae: 1.9794 - val_mse: 8.6297\n",
      "Epoch 534/1000\n",
      "250/250 [==============================] - 0s 85us/sample - loss: 3.6806 - mae: 1.3068 - mse: 3.6806 - val_loss: 7.4299 - val_mae: 1.8953 - val_mse: 7.4299\n",
      "Epoch 535/1000\n",
      "250/250 [==============================] - 0s 91us/sample - loss: 3.8025 - mae: 1.3323 - mse: 3.8025 - val_loss: 7.5515 - val_mae: 1.8724 - val_mse: 7.5515\n",
      "Epoch 536/1000\n",
      "250/250 [==============================] - 0s 78us/sample - loss: 4.0292 - mae: 1.3857 - mse: 4.0292 - val_loss: 8.6105 - val_mae: 1.9557 - val_mse: 8.6105\n",
      "Epoch 537/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 3.8442 - mae: 1.3325 - mse: 3.8442 - val_loss: 7.6421 - val_mae: 1.8363 - val_mse: 7.6421\n",
      "Epoch 538/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 3.8339 - mae: 1.3021 - mse: 3.8339 - val_loss: 7.1965 - val_mae: 1.8394 - val_mse: 7.1965\n",
      "Epoch 539/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 3.8085 - mae: 1.3471 - mse: 3.8085 - val_loss: 8.3829 - val_mae: 1.9450 - val_mse: 8.3829\n",
      "Epoch 540/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 3.8352 - mae: 1.3354 - mse: 3.8352 - val_loss: 7.5599 - val_mae: 1.8622 - val_mse: 7.5599\n",
      "Epoch 541/1000\n",
      "250/250 [==============================] - 0s 83us/sample - loss: 3.7532 - mae: 1.3299 - mse: 3.7532 - val_loss: 8.8526 - val_mae: 2.0285 - val_mse: 8.8526\n",
      "Epoch 542/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 3.9500 - mae: 1.3558 - mse: 3.9500 - val_loss: 10.2748 - val_mae: 2.1855 - val_mse: 10.2748\n",
      "Epoch 543/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 4.0249 - mae: 1.3735 - mse: 4.0249 - val_loss: 7.8662 - val_mae: 1.9006 - val_mse: 7.8662\n",
      "Epoch 544/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 3.8572 - mae: 1.2964 - mse: 3.8572 - val_loss: 8.2050 - val_mae: 1.9166 - val_mse: 8.2050\n",
      "Epoch 545/1000\n",
      "250/250 [==============================] - 0s 79us/sample - loss: 3.7352 - mae: 1.3222 - mse: 3.7352 - val_loss: 7.6891 - val_mae: 1.9371 - val_mse: 7.6891\n",
      "Epoch 546/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 3.8913 - mae: 1.3450 - mse: 3.8913 - val_loss: 7.7386 - val_mae: 1.8417 - val_mse: 7.7386\n",
      "Epoch 547/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 3.7344 - mae: 1.3142 - mse: 3.7344 - val_loss: 7.8954 - val_mae: 1.8524 - val_mse: 7.8954\n",
      "Epoch 548/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 3.8193 - mae: 1.3368 - mse: 3.8193 - val_loss: 7.4678 - val_mae: 1.8215 - val_mse: 7.4678\n",
      "Epoch 549/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 3.7391 - mae: 1.3123 - mse: 3.7391 - val_loss: 7.9512 - val_mae: 1.8682 - val_mse: 7.9512\n",
      "Epoch 550/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 3.6718 - mae: 1.2820 - mse: 3.6718 - val_loss: 6.8492 - val_mae: 1.8027 - val_mse: 6.8492\n",
      "Epoch 551/1000\n",
      "250/250 [==============================] - 0s 61us/sample - loss: 3.8306 - mae: 1.3384 - mse: 3.8306 - val_loss: 7.9212 - val_mae: 1.8736 - val_mse: 7.9212\n",
      "Epoch 552/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 3.7763 - mae: 1.3053 - mse: 3.7763 - val_loss: 8.8611 - val_mae: 1.9997 - val_mse: 8.8611\n",
      "Epoch 553/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 3.7587 - mae: 1.3332 - mse: 3.7587 - val_loss: 7.2720 - val_mae: 1.8249 - val_mse: 7.2720\n",
      "Epoch 554/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 3.5775 - mae: 1.2929 - mse: 3.5775 - val_loss: 7.3164 - val_mae: 1.7624 - val_mse: 7.3164\n",
      "Epoch 555/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 3.8538 - mae: 1.3178 - mse: 3.8538 - val_loss: 7.0706 - val_mae: 1.7603 - val_mse: 7.0706\n",
      "Epoch 556/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 3.8163 - mae: 1.3515 - mse: 3.8163 - val_loss: 7.5207 - val_mae: 1.8439 - val_mse: 7.5207\n",
      "Epoch 557/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 3.6431 - mae: 1.3056 - mse: 3.6431 - val_loss: 7.1944 - val_mae: 1.8195 - val_mse: 7.1944\n",
      "Epoch 558/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 3.8002 - mae: 1.3196 - mse: 3.8002 - val_loss: 8.6331 - val_mae: 1.9873 - val_mse: 8.6331\n",
      "Epoch 559/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 3.7307 - mae: 1.2983 - mse: 3.7307 - val_loss: 7.4212 - val_mae: 1.8315 - val_mse: 7.4212\n",
      "Epoch 560/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 3.6978 - mae: 1.3026 - mse: 3.6978 - val_loss: 8.1727 - val_mae: 1.8852 - val_mse: 8.1727\n",
      "Epoch 561/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 3.9672 - mae: 1.3529 - mse: 3.9672 - val_loss: 8.0715 - val_mae: 1.9222 - val_mse: 8.0715\n",
      "Epoch 562/1000\n",
      "250/250 [==============================] - 0s 78us/sample - loss: 3.5713 - mae: 1.2980 - mse: 3.5713 - val_loss: 7.3072 - val_mae: 1.8101 - val_mse: 7.3072\n",
      "Epoch 563/1000\n",
      "250/250 [==============================] - 0s 86us/sample - loss: 3.7619 - mae: 1.2748 - mse: 3.7619 - val_loss: 8.5619 - val_mae: 1.9387 - val_mse: 8.5619\n",
      "Epoch 564/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 3.7618 - mae: 1.3430 - mse: 3.7618 - val_loss: 7.7639 - val_mae: 1.8803 - val_mse: 7.7639\n",
      "Epoch 565/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 3.7178 - mae: 1.3474 - mse: 3.7178 - val_loss: 8.7747 - val_mae: 1.9823 - val_mse: 8.7747\n",
      "Epoch 566/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 3.7469 - mae: 1.3047 - mse: 3.7469 - val_loss: 7.7551 - val_mae: 1.8264 - val_mse: 7.7551\n",
      "Epoch 567/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 3.7993 - mae: 1.2962 - mse: 3.7993 - val_loss: 7.5859 - val_mae: 1.8288 - val_mse: 7.5859\n",
      "Epoch 568/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 3.6063 - mae: 1.2976 - mse: 3.6063 - val_loss: 8.4918 - val_mae: 1.9735 - val_mse: 8.4918\n",
      "Epoch 569/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 3.8333 - mae: 1.3196 - mse: 3.8333 - val_loss: 8.2128 - val_mae: 1.9199 - val_mse: 8.2128\n",
      "Epoch 570/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 3.7263 - mae: 1.3240 - mse: 3.7263 - val_loss: 8.0052 - val_mae: 1.9832 - val_mse: 8.0052\n",
      "Epoch 571/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 3.7142 - mae: 1.3031 - mse: 3.7142 - val_loss: 9.7040 - val_mae: 2.0914 - val_mse: 9.7040\n",
      "Epoch 572/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 3.5644 - mae: 1.2840 - mse: 3.5644 - val_loss: 7.6416 - val_mae: 1.8717 - val_mse: 7.6416\n",
      "Epoch 573/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 3.5792 - mae: 1.2663 - mse: 3.5792 - val_loss: 8.7052 - val_mae: 1.9870 - val_mse: 8.7052\n",
      "Epoch 574/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 3.8023 - mae: 1.3352 - mse: 3.8023 - val_loss: 7.6438 - val_mae: 1.8391 - val_mse: 7.6438\n",
      "Epoch 575/1000\n",
      "250/250 [==============================] - 0s 87us/sample - loss: 3.6704 - mae: 1.2876 - mse: 3.6704 - val_loss: 8.6538 - val_mae: 1.9826 - val_mse: 8.6538\n",
      "Epoch 576/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 3.6923 - mae: 1.2987 - mse: 3.6923 - val_loss: 8.7641 - val_mae: 1.9945 - val_mse: 8.7641\n",
      "Epoch 577/1000\n",
      "250/250 [==============================] - 0s 79us/sample - loss: 3.6992 - mae: 1.3344 - mse: 3.6992 - val_loss: 8.4396 - val_mae: 1.9184 - val_mse: 8.4396\n",
      "Epoch 578/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 3.6879 - mae: 1.2848 - mse: 3.6879 - val_loss: 7.9568 - val_mae: 1.8593 - val_mse: 7.9568\n",
      "Epoch 579/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 3.6804 - mae: 1.2722 - mse: 3.6804 - val_loss: 9.1638 - val_mae: 2.0269 - val_mse: 9.1638\n",
      "Epoch 580/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 3.5765 - mae: 1.2730 - mse: 3.5765 - val_loss: 9.5640 - val_mae: 2.0603 - val_mse: 9.5640\n",
      "Epoch 581/1000\n",
      "250/250 [==============================] - 0s 83us/sample - loss: 3.5502 - mae: 1.2548 - mse: 3.5502 - val_loss: 8.2509 - val_mae: 1.9076 - val_mse: 8.2509\n",
      "Epoch 582/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 3.7869 - mae: 1.3326 - mse: 3.7869 - val_loss: 8.4958 - val_mae: 1.9381 - val_mse: 8.4958\n",
      "Epoch 583/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 3.7867 - mae: 1.3003 - mse: 3.7867 - val_loss: 7.8867 - val_mae: 1.9074 - val_mse: 7.8867\n",
      "Epoch 584/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 3.5535 - mae: 1.2624 - mse: 3.5535 - val_loss: 10.1399 - val_mae: 2.1504 - val_mse: 10.1399\n",
      "Epoch 585/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 3.8068 - mae: 1.3236 - mse: 3.8068 - val_loss: 8.6490 - val_mae: 1.9368 - val_mse: 8.6490\n",
      "Epoch 586/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 3.5260 - mae: 1.2683 - mse: 3.5260 - val_loss: 7.9429 - val_mae: 1.8933 - val_mse: 7.9429\n",
      "Epoch 587/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 3.7340 - mae: 1.2722 - mse: 3.7340 - val_loss: 7.5386 - val_mae: 1.8934 - val_mse: 7.5386\n",
      "Epoch 588/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 3.6289 - mae: 1.3086 - mse: 3.6289 - val_loss: 9.2377 - val_mae: 2.0385 - val_mse: 9.2377\n",
      "Epoch 589/1000\n",
      "250/250 [==============================] - 0s 79us/sample - loss: 3.6552 - mae: 1.3258 - mse: 3.6552 - val_loss: 9.5571 - val_mae: 2.0811 - val_mse: 9.5571\n",
      "Epoch 590/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 3.6564 - mae: 1.3099 - mse: 3.6564 - val_loss: 7.4090 - val_mae: 1.8473 - val_mse: 7.4090\n",
      "Epoch 591/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 3.6825 - mae: 1.3059 - mse: 3.6825 - val_loss: 7.6322 - val_mae: 1.8678 - val_mse: 7.6322\n",
      "Epoch 592/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 3.6103 - mae: 1.3051 - mse: 3.6103 - val_loss: 7.5420 - val_mae: 1.8442 - val_mse: 7.5420\n",
      "Epoch 593/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 3.5950 - mae: 1.2835 - mse: 3.5950 - val_loss: 8.0065 - val_mae: 1.9604 - val_mse: 8.0065\n",
      "Epoch 594/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 3.5395 - mae: 1.2666 - mse: 3.5395 - val_loss: 8.1911 - val_mae: 1.9234 - val_mse: 8.1911\n",
      "Epoch 595/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 3.7102 - mae: 1.3255 - mse: 3.7102 - val_loss: 7.2131 - val_mae: 1.8011 - val_mse: 7.2131\n",
      "Epoch 596/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 3.6052 - mae: 1.2628 - mse: 3.6052 - val_loss: 7.1432 - val_mae: 1.7942 - val_mse: 7.1432\n",
      "Epoch 597/1000\n",
      "250/250 [==============================] - 0s 61us/sample - loss: 3.5416 - mae: 1.2673 - mse: 3.5416 - val_loss: 8.2446 - val_mae: 1.9278 - val_mse: 8.2446\n",
      "Epoch 598/1000\n",
      "250/250 [==============================] - 0s 60us/sample - loss: 3.6081 - mae: 1.2797 - mse: 3.6081 - val_loss: 7.8135 - val_mae: 1.9187 - val_mse: 7.8135\n",
      "Epoch 599/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 3.4224 - mae: 1.2608 - mse: 3.4224 - val_loss: 8.7592 - val_mae: 1.9699 - val_mse: 8.7592\n",
      "Epoch 600/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 3.6702 - mae: 1.2977 - mse: 3.6702 - val_loss: 7.1481 - val_mae: 1.8683 - val_mse: 7.1481\n",
      "Epoch 601/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 3.4738 - mae: 1.2796 - mse: 3.4738 - val_loss: 7.8450 - val_mae: 1.9243 - val_mse: 7.8450\n",
      "Epoch 602/1000\n",
      "250/250 [==============================] - 0s 88us/sample - loss: 3.7834 - mae: 1.2814 - mse: 3.7834 - val_loss: 7.7337 - val_mae: 1.9235 - val_mse: 7.7337\n",
      "Epoch 603/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 3.6129 - mae: 1.2762 - mse: 3.6129 - val_loss: 8.0540 - val_mae: 1.9632 - val_mse: 8.0540\n",
      "Epoch 604/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 3.6835 - mae: 1.3065 - mse: 3.6835 - val_loss: 8.0418 - val_mae: 1.9901 - val_mse: 8.0418\n",
      "Epoch 605/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 3.4571 - mae: 1.2656 - mse: 3.4571 - val_loss: 7.0872 - val_mae: 1.8796 - val_mse: 7.0872\n",
      "Epoch 606/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 3.5998 - mae: 1.2938 - mse: 3.5998 - val_loss: 7.1899 - val_mae: 1.8122 - val_mse: 7.1899\n",
      "Epoch 607/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 3.4675 - mae: 1.2531 - mse: 3.4675 - val_loss: 7.3087 - val_mae: 1.8030 - val_mse: 7.3087\n",
      "Epoch 608/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 3.4629 - mae: 1.2790 - mse: 3.4629 - val_loss: 7.6008 - val_mae: 1.8829 - val_mse: 7.6008\n",
      "Epoch 609/1000\n",
      "250/250 [==============================] - 0s 61us/sample - loss: 3.6796 - mae: 1.3054 - mse: 3.6796 - val_loss: 8.7733 - val_mae: 2.0393 - val_mse: 8.7733\n",
      "Epoch 610/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 3.6371 - mae: 1.2822 - mse: 3.6371 - val_loss: 7.8772 - val_mae: 1.8855 - val_mse: 7.8772\n",
      "Epoch 611/1000\n",
      "250/250 [==============================] - 0s 101us/sample - loss: 3.5928 - mae: 1.2630 - mse: 3.5928 - val_loss: 9.1640 - val_mae: 2.0341 - val_mse: 9.1639\n",
      "Epoch 612/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 3.4538 - mae: 1.2576 - mse: 3.4538 - val_loss: 10.2012 - val_mae: 2.1732 - val_mse: 10.2012\n",
      "Epoch 613/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 3.5179 - mae: 1.2526 - mse: 3.5179 - val_loss: 8.1915 - val_mae: 1.9939 - val_mse: 8.1915\n",
      "Epoch 614/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 3.5067 - mae: 1.2908 - mse: 3.5067 - val_loss: 8.7013 - val_mae: 1.9720 - val_mse: 8.7013\n",
      "Epoch 615/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 3.5823 - mae: 1.2748 - mse: 3.5823 - val_loss: 10.5128 - val_mae: 2.1920 - val_mse: 10.5128\n",
      "Epoch 616/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 3.6231 - mae: 1.2927 - mse: 3.6231 - val_loss: 8.6246 - val_mae: 1.9414 - val_mse: 8.6246\n",
      "Epoch 617/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 3.4087 - mae: 1.2390 - mse: 3.4087 - val_loss: 8.5970 - val_mae: 1.9745 - val_mse: 8.5970\n",
      "Epoch 618/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 3.5181 - mae: 1.2716 - mse: 3.5181 - val_loss: 8.0740 - val_mae: 1.9205 - val_mse: 8.0740\n",
      "Epoch 619/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 3.4577 - mae: 1.2697 - mse: 3.4577 - val_loss: 9.6756 - val_mae: 2.1142 - val_mse: 9.6756\n",
      "Epoch 620/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 3.6878 - mae: 1.2513 - mse: 3.6878 - val_loss: 8.9688 - val_mae: 1.9886 - val_mse: 8.9688\n",
      "Epoch 621/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 3.5598 - mae: 1.2778 - mse: 3.5598 - val_loss: 6.9828 - val_mae: 1.8186 - val_mse: 6.9828\n",
      "Epoch 622/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 3.5001 - mae: 1.2384 - mse: 3.5001 - val_loss: 8.6892 - val_mae: 1.9640 - val_mse: 8.6892\n",
      "Epoch 623/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 3.4875 - mae: 1.2421 - mse: 3.4875 - val_loss: 7.2751 - val_mae: 1.7858 - val_mse: 7.2751\n",
      "Epoch 624/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 3.4455 - mae: 1.2604 - mse: 3.4455 - val_loss: 8.8133 - val_mae: 1.9871 - val_mse: 8.8133\n",
      "Epoch 625/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 3.5873 - mae: 1.2785 - mse: 3.5873 - val_loss: 7.7344 - val_mae: 1.8922 - val_mse: 7.7344\n",
      "Epoch 626/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 3.3982 - mae: 1.2557 - mse: 3.3982 - val_loss: 7.8898 - val_mae: 1.8998 - val_mse: 7.8898\n",
      "Epoch 627/1000\n",
      "250/250 [==============================] - 0s 96us/sample - loss: 3.4785 - mae: 1.2584 - mse: 3.4785 - val_loss: 8.9553 - val_mae: 2.0156 - val_mse: 8.9553\n",
      "Epoch 628/1000\n",
      "250/250 [==============================] - 0s 61us/sample - loss: 3.5275 - mae: 1.2792 - mse: 3.5275 - val_loss: 6.8120 - val_mae: 1.7739 - val_mse: 6.8120\n",
      "Epoch 629/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 3.2901 - mae: 1.2874 - mse: 3.2901 - val_loss: 9.1775 - val_mae: 2.0429 - val_mse: 9.1775\n",
      "Epoch 630/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 3.6400 - mae: 1.2686 - mse: 3.6400 - val_loss: 7.2327 - val_mae: 1.8405 - val_mse: 7.2327\n",
      "Epoch 631/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 3.4535 - mae: 1.2779 - mse: 3.4535 - val_loss: 7.6837 - val_mae: 2.0027 - val_mse: 7.6837\n",
      "Epoch 632/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 3.4320 - mae: 1.2701 - mse: 3.4320 - val_loss: 9.1302 - val_mae: 2.0442 - val_mse: 9.1302\n",
      "Epoch 633/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 3.5304 - mae: 1.2697 - mse: 3.5304 - val_loss: 8.4583 - val_mae: 1.9696 - val_mse: 8.4583\n",
      "Epoch 634/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 3.4909 - mae: 1.2622 - mse: 3.4909 - val_loss: 9.7975 - val_mae: 2.1091 - val_mse: 9.7975\n",
      "Epoch 635/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 3.5340 - mae: 1.2481 - mse: 3.5340 - val_loss: 8.3465 - val_mae: 1.9372 - val_mse: 8.3465\n",
      "Epoch 636/1000\n",
      "250/250 [==============================] - 0s 59us/sample - loss: 3.4700 - mae: 1.2495 - mse: 3.4700 - val_loss: 7.7921 - val_mae: 1.8783 - val_mse: 7.7921\n",
      "Epoch 637/1000\n",
      "250/250 [==============================] - 0s 60us/sample - loss: 3.3466 - mae: 1.2273 - mse: 3.3466 - val_loss: 8.7305 - val_mae: 1.9918 - val_mse: 8.7305\n",
      "Epoch 638/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 3.4189 - mae: 1.2464 - mse: 3.4189 - val_loss: 8.8873 - val_mae: 1.9955 - val_mse: 8.8873\n",
      "Epoch 639/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 3.4843 - mae: 1.2585 - mse: 3.4843 - val_loss: 8.7078 - val_mae: 2.0753 - val_mse: 8.7078\n",
      "Epoch 640/1000\n",
      "250/250 [==============================] - 0s 82us/sample - loss: 3.4273 - mae: 1.2543 - mse: 3.4273 - val_loss: 7.1587 - val_mae: 1.8710 - val_mse: 7.1587\n",
      "Epoch 641/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 3.4919 - mae: 1.2670 - mse: 3.4919 - val_loss: 6.8989 - val_mae: 1.7916 - val_mse: 6.8989\n",
      "Epoch 642/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 3.5081 - mae: 1.3040 - mse: 3.5081 - val_loss: 9.1807 - val_mae: 2.0112 - val_mse: 9.1807\n",
      "Epoch 643/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 3.4603 - mae: 1.2576 - mse: 3.4603 - val_loss: 10.7414 - val_mae: 2.2187 - val_mse: 10.7414\n",
      "Epoch 644/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 3.5327 - mae: 1.2991 - mse: 3.5327 - val_loss: 7.8282 - val_mae: 1.9548 - val_mse: 7.8282\n",
      "Epoch 645/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 3.4728 - mae: 1.2591 - mse: 3.4728 - val_loss: 9.8668 - val_mae: 2.1182 - val_mse: 9.8668\n",
      "Epoch 646/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 3.2710 - mae: 1.2135 - mse: 3.2710 - val_loss: 9.2856 - val_mae: 2.0245 - val_mse: 9.2856\n",
      "Epoch 647/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 3.3546 - mae: 1.2329 - mse: 3.3546 - val_loss: 9.6988 - val_mae: 2.1846 - val_mse: 9.6988\n",
      "Epoch 648/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 3.7168 - mae: 1.2828 - mse: 3.7168 - val_loss: 6.6980 - val_mae: 1.8110 - val_mse: 6.6980\n",
      "Epoch 649/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 3.4047 - mae: 1.2565 - mse: 3.4047 - val_loss: 7.5136 - val_mae: 1.8392 - val_mse: 7.5136\n",
      "Epoch 650/1000\n",
      "250/250 [==============================] - 0s 81us/sample - loss: 3.4169 - mae: 1.2479 - mse: 3.4169 - val_loss: 7.7497 - val_mae: 1.8690 - val_mse: 7.7497\n",
      "Epoch 651/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 3.3623 - mae: 1.2190 - mse: 3.3623 - val_loss: 7.4443 - val_mae: 1.8416 - val_mse: 7.4443\n",
      "Epoch 652/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 3.6166 - mae: 1.2545 - mse: 3.6166 - val_loss: 9.2905 - val_mae: 2.0483 - val_mse: 9.2905\n",
      "Epoch 653/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 3.7298 - mae: 1.3019 - mse: 3.7298 - val_loss: 7.8125 - val_mae: 1.8856 - val_mse: 7.8125\n",
      "Epoch 654/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 3.2085 - mae: 1.2100 - mse: 3.2085 - val_loss: 7.1594 - val_mae: 1.7868 - val_mse: 7.1594\n",
      "Epoch 655/1000\n",
      "250/250 [==============================] - 0s 61us/sample - loss: 3.5389 - mae: 1.2577 - mse: 3.5389 - val_loss: 7.6265 - val_mae: 1.8680 - val_mse: 7.6265\n",
      "Epoch 656/1000\n",
      "250/250 [==============================] - 0s 79us/sample - loss: 3.5301 - mae: 1.2727 - mse: 3.5301 - val_loss: 7.4858 - val_mae: 1.8573 - val_mse: 7.4858\n",
      "Epoch 657/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 3.3497 - mae: 1.2646 - mse: 3.3497 - val_loss: 8.4825 - val_mae: 2.0251 - val_mse: 8.4825\n",
      "Epoch 658/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 3.3150 - mae: 1.2111 - mse: 3.3150 - val_loss: 7.5582 - val_mae: 1.8993 - val_mse: 7.5582\n",
      "Epoch 659/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 3.3487 - mae: 1.2328 - mse: 3.3487 - val_loss: 8.9458 - val_mae: 2.0217 - val_mse: 8.9458\n",
      "Epoch 660/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 3.2723 - mae: 1.2534 - mse: 3.2723 - val_loss: 7.2124 - val_mae: 1.8340 - val_mse: 7.2124\n",
      "Epoch 661/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 3.3077 - mae: 1.2092 - mse: 3.3077 - val_loss: 7.4690 - val_mae: 1.9090 - val_mse: 7.4690\n",
      "Epoch 662/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 3.3767 - mae: 1.2350 - mse: 3.3767 - val_loss: 7.8861 - val_mae: 1.8808 - val_mse: 7.8861\n",
      "Epoch 663/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 3.3092 - mae: 1.2240 - mse: 3.3092 - val_loss: 8.9661 - val_mae: 2.0457 - val_mse: 8.9661\n",
      "Epoch 664/1000\n",
      "250/250 [==============================] - 0s 80us/sample - loss: 3.4340 - mae: 1.2598 - mse: 3.4340 - val_loss: 8.4393 - val_mae: 1.9355 - val_mse: 8.4393\n",
      "Epoch 665/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 3.4189 - mae: 1.2329 - mse: 3.4189 - val_loss: 8.3062 - val_mae: 1.9299 - val_mse: 8.3062\n",
      "Epoch 666/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 3.3854 - mae: 1.2064 - mse: 3.3854 - val_loss: 6.9061 - val_mae: 1.8074 - val_mse: 6.9061\n",
      "Epoch 667/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 3.2404 - mae: 1.2670 - mse: 3.2404 - val_loss: 7.6205 - val_mae: 1.9228 - val_mse: 7.6205\n",
      "Epoch 668/1000\n",
      "250/250 [==============================] - 0s 83us/sample - loss: 3.2946 - mae: 1.2311 - mse: 3.2946 - val_loss: 8.5455 - val_mae: 1.9719 - val_mse: 8.5455\n",
      "Epoch 669/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 3.1698 - mae: 1.2035 - mse: 3.1698 - val_loss: 7.2262 - val_mae: 1.7972 - val_mse: 7.2262\n",
      "Epoch 670/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 3.3962 - mae: 1.2381 - mse: 3.3962 - val_loss: 6.9800 - val_mae: 1.8319 - val_mse: 6.9800\n",
      "Epoch 671/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 3.3341 - mae: 1.2335 - mse: 3.3341 - val_loss: 7.2077 - val_mae: 1.8936 - val_mse: 7.2077\n",
      "Epoch 672/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 3.3719 - mae: 1.2275 - mse: 3.3719 - val_loss: 10.1998 - val_mae: 2.1730 - val_mse: 10.1998\n",
      "Epoch 673/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 3.4229 - mae: 1.2324 - mse: 3.4229 - val_loss: 7.7752 - val_mae: 1.9204 - val_mse: 7.7752\n",
      "Epoch 674/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 3.4757 - mae: 1.2381 - mse: 3.4757 - val_loss: 8.4502 - val_mae: 1.9868 - val_mse: 8.4502\n",
      "Epoch 675/1000\n",
      "250/250 [==============================] - 0s 89us/sample - loss: 3.3497 - mae: 1.2264 - mse: 3.3497 - val_loss: 8.9862 - val_mae: 2.0285 - val_mse: 8.9862\n",
      "Epoch 676/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 3.2790 - mae: 1.2222 - mse: 3.2790 - val_loss: 7.8875 - val_mae: 1.9440 - val_mse: 7.8875\n",
      "Epoch 677/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 3.4137 - mae: 1.2254 - mse: 3.4137 - val_loss: 7.0877 - val_mae: 1.8494 - val_mse: 7.0877\n",
      "Epoch 678/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 3.3904 - mae: 1.2218 - mse: 3.3904 - val_loss: 7.3749 - val_mae: 1.9152 - val_mse: 7.3749\n",
      "Epoch 679/1000\n",
      "250/250 [==============================] - 0s 106us/sample - loss: 3.2058 - mae: 1.2132 - mse: 3.2058 - val_loss: 8.4173 - val_mae: 1.9559 - val_mse: 8.4173\n",
      "Epoch 680/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 3.7871 - mae: 1.3215 - mse: 3.7871 - val_loss: 7.9465 - val_mae: 1.9497 - val_mse: 7.9465\n",
      "Epoch 681/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 3.2720 - mae: 1.2175 - mse: 3.2720 - val_loss: 7.0573 - val_mae: 1.8842 - val_mse: 7.0573\n",
      "Epoch 682/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 3.2336 - mae: 1.2013 - mse: 3.2336 - val_loss: 7.2936 - val_mae: 1.8466 - val_mse: 7.2936\n",
      "Epoch 683/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 3.2542 - mae: 1.2194 - mse: 3.2542 - val_loss: 8.1062 - val_mae: 1.9688 - val_mse: 8.1062\n",
      "Epoch 684/1000\n",
      "250/250 [==============================] - 0s 91us/sample - loss: 3.1460 - mae: 1.2038 - mse: 3.1460 - val_loss: 9.3795 - val_mae: 2.0809 - val_mse: 9.3795\n",
      "Epoch 685/1000\n",
      "250/250 [==============================] - 0s 79us/sample - loss: 3.4603 - mae: 1.2296 - mse: 3.4603 - val_loss: 8.2629 - val_mae: 1.9186 - val_mse: 8.2629\n",
      "Epoch 686/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 3.1887 - mae: 1.2273 - mse: 3.1887 - val_loss: 9.1211 - val_mae: 2.0755 - val_mse: 9.1211\n",
      "Epoch 687/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 3.2856 - mae: 1.2190 - mse: 3.2856 - val_loss: 8.0881 - val_mae: 1.9043 - val_mse: 8.0881\n",
      "Epoch 688/1000\n",
      "250/250 [==============================] - 0s 91us/sample - loss: 3.2333 - mae: 1.1862 - mse: 3.2333 - val_loss: 8.8139 - val_mae: 2.0103 - val_mse: 8.8139\n",
      "Epoch 689/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 3.2306 - mae: 1.2113 - mse: 3.2306 - val_loss: 8.8218 - val_mae: 2.0031 - val_mse: 8.8218\n",
      "Epoch 690/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 3.1876 - mae: 1.2056 - mse: 3.1876 - val_loss: 8.5306 - val_mae: 1.9964 - val_mse: 8.5306\n",
      "Epoch 691/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 3.3451 - mae: 1.2419 - mse: 3.3451 - val_loss: 9.5440 - val_mae: 2.1189 - val_mse: 9.5440\n",
      "Epoch 692/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 3.3110 - mae: 1.2034 - mse: 3.3110 - val_loss: 6.8135 - val_mae: 1.7839 - val_mse: 6.8135\n",
      "Epoch 693/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 3.4904 - mae: 1.2638 - mse: 3.4904 - val_loss: 8.2149 - val_mae: 1.9366 - val_mse: 8.2149\n",
      "Epoch 694/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 3.2534 - mae: 1.2206 - mse: 3.2534 - val_loss: 7.8647 - val_mae: 1.8999 - val_mse: 7.8647\n",
      "Epoch 695/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 3.1206 - mae: 1.1822 - mse: 3.1206 - val_loss: 7.4762 - val_mae: 1.8319 - val_mse: 7.4762\n",
      "Epoch 696/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 3.2056 - mae: 1.2094 - mse: 3.2056 - val_loss: 8.8075 - val_mae: 1.9867 - val_mse: 8.8075\n",
      "Epoch 697/1000\n",
      "250/250 [==============================] - 0s 78us/sample - loss: 3.4370 - mae: 1.2577 - mse: 3.4370 - val_loss: 7.6428 - val_mae: 1.9025 - val_mse: 7.6428\n",
      "Epoch 698/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 3.1786 - mae: 1.2154 - mse: 3.1786 - val_loss: 8.4001 - val_mae: 1.9325 - val_mse: 8.4001\n",
      "Epoch 699/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 3.2330 - mae: 1.1928 - mse: 3.2330 - val_loss: 7.2861 - val_mae: 1.8583 - val_mse: 7.2861\n",
      "Epoch 700/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 3.3757 - mae: 1.2503 - mse: 3.3757 - val_loss: 8.3663 - val_mae: 1.9477 - val_mse: 8.3663\n",
      "Epoch 701/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 3.1125 - mae: 1.1669 - mse: 3.1125 - val_loss: 6.8010 - val_mae: 1.8624 - val_mse: 6.8010\n",
      "Epoch 702/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 3.5194 - mae: 1.2529 - mse: 3.5194 - val_loss: 7.4521 - val_mae: 1.9124 - val_mse: 7.4521\n",
      "Epoch 703/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 3.5245 - mae: 1.2504 - mse: 3.5245 - val_loss: 7.7633 - val_mae: 1.9310 - val_mse: 7.7633\n",
      "Epoch 704/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 3.0647 - mae: 1.1639 - mse: 3.0647 - val_loss: 7.9434 - val_mae: 1.9077 - val_mse: 7.9434\n",
      "Epoch 705/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 3.1462 - mae: 1.2010 - mse: 3.1462 - val_loss: 8.5874 - val_mae: 1.9644 - val_mse: 8.5874\n",
      "Epoch 706/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 3.2443 - mae: 1.2007 - mse: 3.2443 - val_loss: 8.8485 - val_mae: 2.0204 - val_mse: 8.8485\n",
      "Epoch 707/1000\n",
      "250/250 [==============================] - 0s 92us/sample - loss: 3.2015 - mae: 1.2115 - mse: 3.2015 - val_loss: 7.6261 - val_mae: 1.8586 - val_mse: 7.6261\n",
      "Epoch 708/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 3.1239 - mae: 1.1831 - mse: 3.1239 - val_loss: 7.4143 - val_mae: 1.8251 - val_mse: 7.4143\n",
      "Epoch 709/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 3.1124 - mae: 1.1769 - mse: 3.1124 - val_loss: 8.7932 - val_mae: 2.0841 - val_mse: 8.7932\n",
      "Epoch 710/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 3.3856 - mae: 1.2617 - mse: 3.3856 - val_loss: 7.2108 - val_mae: 1.8645 - val_mse: 7.2108\n",
      "Epoch 711/1000\n",
      "250/250 [==============================] - 0s 89us/sample - loss: 3.0866 - mae: 1.1915 - mse: 3.0866 - val_loss: 7.6173 - val_mae: 1.8740 - val_mse: 7.6173\n",
      "Epoch 712/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 3.0095 - mae: 1.1742 - mse: 3.0095 - val_loss: 8.0772 - val_mae: 1.9328 - val_mse: 8.0772\n",
      "Epoch 713/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 3.4436 - mae: 1.2298 - mse: 3.4436 - val_loss: 8.3421 - val_mae: 1.9484 - val_mse: 8.3421\n",
      "Epoch 714/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 3.2819 - mae: 1.2243 - mse: 3.2819 - val_loss: 7.7239 - val_mae: 1.8974 - val_mse: 7.7239\n",
      "Epoch 715/1000\n",
      "250/250 [==============================] - 0s 80us/sample - loss: 3.2878 - mae: 1.2335 - mse: 3.2878 - val_loss: 9.2993 - val_mae: 2.0771 - val_mse: 9.2993\n",
      "Epoch 716/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 3.2604 - mae: 1.2139 - mse: 3.2604 - val_loss: 8.3602 - val_mae: 1.9371 - val_mse: 8.3602\n",
      "Epoch 717/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 3.0841 - mae: 1.1806 - mse: 3.0841 - val_loss: 8.0435 - val_mae: 1.9281 - val_mse: 8.0435\n",
      "Epoch 718/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 3.3291 - mae: 1.2003 - mse: 3.3291 - val_loss: 7.9462 - val_mae: 1.9037 - val_mse: 7.9462\n",
      "Epoch 719/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 3.0612 - mae: 1.1669 - mse: 3.0612 - val_loss: 7.2944 - val_mae: 1.8761 - val_mse: 7.2944\n",
      "Epoch 720/1000\n",
      "250/250 [==============================] - 0s 61us/sample - loss: 3.1672 - mae: 1.2116 - mse: 3.1672 - val_loss: 7.2857 - val_mae: 1.8785 - val_mse: 7.2857\n",
      "Epoch 721/1000\n",
      "250/250 [==============================] - 0s 79us/sample - loss: 3.1513 - mae: 1.1723 - mse: 3.1513 - val_loss: 6.7381 - val_mae: 1.8165 - val_mse: 6.7381\n",
      "Epoch 722/1000\n",
      "250/250 [==============================] - 0s 81us/sample - loss: 3.0840 - mae: 1.1755 - mse: 3.0840 - val_loss: 6.8193 - val_mae: 1.8449 - val_mse: 6.8193\n",
      "Epoch 723/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 3.1244 - mae: 1.2413 - mse: 3.1244 - val_loss: 8.6134 - val_mae: 1.9743 - val_mse: 8.6134\n",
      "Epoch 724/1000\n",
      "250/250 [==============================] - 0s 88us/sample - loss: 3.1523 - mae: 1.1865 - mse: 3.1523 - val_loss: 9.4750 - val_mae: 2.0806 - val_mse: 9.4750\n",
      "Epoch 725/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 3.2292 - mae: 1.1894 - mse: 3.2292 - val_loss: 8.1866 - val_mae: 1.9430 - val_mse: 8.1866\n",
      "Epoch 726/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 3.0809 - mae: 1.1584 - mse: 3.0809 - val_loss: 8.2381 - val_mae: 1.9261 - val_mse: 8.2381\n",
      "Epoch 727/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 3.1787 - mae: 1.1824 - mse: 3.1787 - val_loss: 9.1600 - val_mae: 2.0620 - val_mse: 9.1600\n",
      "Epoch 728/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 3.2911 - mae: 1.2315 - mse: 3.2911 - val_loss: 7.6657 - val_mae: 1.9266 - val_mse: 7.6657\n",
      "Epoch 729/1000\n",
      "250/250 [==============================] - 0s 61us/sample - loss: 3.0388 - mae: 1.1678 - mse: 3.0388 - val_loss: 8.0710 - val_mae: 1.9318 - val_mse: 8.0710\n",
      "Epoch 730/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 3.0796 - mae: 1.1631 - mse: 3.0796 - val_loss: 9.8052 - val_mae: 2.1284 - val_mse: 9.8052\n",
      "Epoch 731/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 3.3987 - mae: 1.2079 - mse: 3.3987 - val_loss: 9.0965 - val_mae: 2.0561 - val_mse: 9.0965\n",
      "Epoch 732/1000\n",
      "250/250 [==============================] - 0s 60us/sample - loss: 3.0766 - mae: 1.2097 - mse: 3.0766 - val_loss: 8.8832 - val_mae: 2.0120 - val_mse: 8.8832\n",
      "Epoch 733/1000\n",
      "250/250 [==============================] - 0s 86us/sample - loss: 3.1325 - mae: 1.2164 - mse: 3.1325 - val_loss: 8.1845 - val_mae: 1.9074 - val_mse: 8.1845\n",
      "Epoch 734/1000\n",
      "250/250 [==============================] - 0s 86us/sample - loss: 3.1472 - mae: 1.1767 - mse: 3.1472 - val_loss: 8.1016 - val_mae: 1.9220 - val_mse: 8.1016\n",
      "Epoch 735/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 3.1429 - mae: 1.1719 - mse: 3.1429 - val_loss: 7.3448 - val_mae: 1.8219 - val_mse: 7.3448\n",
      "Epoch 736/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 3.1756 - mae: 1.2088 - mse: 3.1756 - val_loss: 7.7825 - val_mae: 1.8793 - val_mse: 7.7825\n",
      "Epoch 737/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 3.1687 - mae: 1.1961 - mse: 3.1687 - val_loss: 7.8080 - val_mae: 1.8867 - val_mse: 7.8080\n",
      "Epoch 738/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 3.0052 - mae: 1.1633 - mse: 3.0052 - val_loss: 7.9827 - val_mae: 1.9564 - val_mse: 7.9827\n",
      "Epoch 739/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 3.1539 - mae: 1.1902 - mse: 3.1539 - val_loss: 8.8792 - val_mae: 2.0192 - val_mse: 8.8792\n",
      "Epoch 740/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 3.1102 - mae: 1.1565 - mse: 3.1102 - val_loss: 6.8942 - val_mae: 1.8072 - val_mse: 6.8942\n",
      "Epoch 741/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 3.3685 - mae: 1.2317 - mse: 3.3685 - val_loss: 8.8359 - val_mae: 2.0894 - val_mse: 8.8359\n",
      "Epoch 742/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 3.0141 - mae: 1.1470 - mse: 3.0141 - val_loss: 7.4252 - val_mae: 1.9184 - val_mse: 7.4252\n",
      "Epoch 743/1000\n",
      "250/250 [==============================] - 0s 79us/sample - loss: 3.0550 - mae: 1.1396 - mse: 3.0550 - val_loss: 8.7725 - val_mae: 1.9920 - val_mse: 8.7725\n",
      "Epoch 744/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 2.9960 - mae: 1.1636 - mse: 2.9960 - val_loss: 8.0746 - val_mae: 1.9491 - val_mse: 8.0746\n",
      "Epoch 745/1000\n",
      "250/250 [==============================] - 0s 79us/sample - loss: 3.1989 - mae: 1.1899 - mse: 3.1989 - val_loss: 8.8006 - val_mae: 1.9928 - val_mse: 8.8006\n",
      "Epoch 746/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 3.2859 - mae: 1.2102 - mse: 3.2859 - val_loss: 10.0732 - val_mae: 2.1792 - val_mse: 10.0732\n",
      "Epoch 747/1000\n",
      "250/250 [==============================] - 0s 91us/sample - loss: 3.0974 - mae: 1.1948 - mse: 3.0974 - val_loss: 7.9784 - val_mae: 1.9392 - val_mse: 7.9784\n",
      "Epoch 748/1000\n",
      "250/250 [==============================] - 0s 60us/sample - loss: 3.0016 - mae: 1.1797 - mse: 3.0016 - val_loss: 8.0487 - val_mae: 1.9198 - val_mse: 8.0487\n",
      "Epoch 749/1000\n",
      "250/250 [==============================] - 0s 57us/sample - loss: 2.9177 - mae: 1.1338 - mse: 2.9177 - val_loss: 7.7687 - val_mae: 1.8625 - val_mse: 7.7687\n",
      "Epoch 750/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 2.9634 - mae: 1.1588 - mse: 2.9634 - val_loss: 8.0965 - val_mae: 1.9401 - val_mse: 8.0965\n",
      "Epoch 751/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 3.0223 - mae: 1.1613 - mse: 3.0223 - val_loss: 7.9887 - val_mae: 1.9073 - val_mse: 7.9887\n",
      "Epoch 752/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 3.0135 - mae: 1.1615 - mse: 3.0135 - val_loss: 9.4782 - val_mae: 2.1149 - val_mse: 9.4782\n",
      "Epoch 753/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 2.9806 - mae: 1.1901 - mse: 2.9806 - val_loss: 6.9272 - val_mae: 1.8654 - val_mse: 6.9272\n",
      "Epoch 754/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 3.0437 - mae: 1.1785 - mse: 3.0437 - val_loss: 8.8467 - val_mae: 2.0510 - val_mse: 8.8467\n",
      "Epoch 755/1000\n",
      "250/250 [==============================] - 0s 78us/sample - loss: 3.1066 - mae: 1.2197 - mse: 3.1066 - val_loss: 9.0444 - val_mae: 2.0464 - val_mse: 9.0444\n",
      "Epoch 756/1000\n",
      "250/250 [==============================] - 0s 78us/sample - loss: 3.0353 - mae: 1.1795 - mse: 3.0353 - val_loss: 7.6446 - val_mae: 1.8805 - val_mse: 7.6446\n",
      "Epoch 757/1000\n",
      "250/250 [==============================] - 0s 78us/sample - loss: 2.8960 - mae: 1.1720 - mse: 2.8960 - val_loss: 8.8374 - val_mae: 2.0418 - val_mse: 8.8374\n",
      "Epoch 758/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 2.9595 - mae: 1.1685 - mse: 2.9595 - val_loss: 9.1990 - val_mae: 2.1144 - val_mse: 9.1990\n",
      "Epoch 759/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 2.8743 - mae: 1.1452 - mse: 2.8743 - val_loss: 7.1081 - val_mae: 1.8439 - val_mse: 7.1081\n",
      "Epoch 760/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 2.7410 - mae: 1.1372 - mse: 2.7410 - val_loss: 8.9914 - val_mae: 2.0340 - val_mse: 8.9914\n",
      "Epoch 761/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 3.0233 - mae: 1.1685 - mse: 3.0233 - val_loss: 7.1233 - val_mae: 1.8901 - val_mse: 7.1233\n",
      "Epoch 762/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 3.1628 - mae: 1.1766 - mse: 3.1628 - val_loss: 7.4909 - val_mae: 1.8713 - val_mse: 7.4909\n",
      "Epoch 763/1000\n",
      "250/250 [==============================] - 0s 61us/sample - loss: 2.9098 - mae: 1.1230 - mse: 2.9098 - val_loss: 8.0248 - val_mae: 1.9111 - val_mse: 8.0248\n",
      "Epoch 764/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 2.9601 - mae: 1.1369 - mse: 2.9601 - val_loss: 7.1795 - val_mae: 1.8400 - val_mse: 7.1795\n",
      "Epoch 765/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 2.9529 - mae: 1.1631 - mse: 2.9529 - val_loss: 7.7519 - val_mae: 1.8820 - val_mse: 7.7519\n",
      "Epoch 766/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 2.9007 - mae: 1.1090 - mse: 2.9007 - val_loss: 8.9205 - val_mae: 2.0174 - val_mse: 8.9205\n",
      "Epoch 767/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 3.0030 - mae: 1.1708 - mse: 3.0030 - val_loss: 8.7240 - val_mae: 2.0099 - val_mse: 8.7240\n",
      "Epoch 768/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 2.9999 - mae: 1.1750 - mse: 2.9999 - val_loss: 9.0416 - val_mae: 2.0561 - val_mse: 9.0416\n",
      "Epoch 769/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 3.0604 - mae: 1.1653 - mse: 3.0604 - val_loss: 8.3724 - val_mae: 1.9704 - val_mse: 8.3724\n",
      "Epoch 770/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 2.8965 - mae: 1.1415 - mse: 2.8965 - val_loss: 10.0581 - val_mae: 2.1920 - val_mse: 10.0581\n",
      "Epoch 771/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 2.9641 - mae: 1.1401 - mse: 2.9641 - val_loss: 9.8368 - val_mae: 2.1581 - val_mse: 9.8368\n",
      "Epoch 772/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 2.9220 - mae: 1.1348 - mse: 2.9220 - val_loss: 7.4358 - val_mae: 1.8855 - val_mse: 7.4358\n",
      "Epoch 773/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 3.1101 - mae: 1.1692 - mse: 3.1101 - val_loss: 9.3074 - val_mae: 2.1054 - val_mse: 9.3074\n",
      "Epoch 774/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 2.8532 - mae: 1.1363 - mse: 2.8532 - val_loss: 7.9884 - val_mae: 1.9567 - val_mse: 7.9884\n",
      "Epoch 775/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 2.9823 - mae: 1.1598 - mse: 2.9823 - val_loss: 8.0561 - val_mae: 1.9626 - val_mse: 8.0561\n",
      "Epoch 776/1000\n",
      "250/250 [==============================] - 0s 79us/sample - loss: 2.9925 - mae: 1.1741 - mse: 2.9925 - val_loss: 8.4868 - val_mae: 1.9937 - val_mse: 8.4868\n",
      "Epoch 777/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 2.8582 - mae: 1.1397 - mse: 2.8582 - val_loss: 10.5808 - val_mae: 2.2470 - val_mse: 10.5808\n",
      "Epoch 778/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 2.9010 - mae: 1.1263 - mse: 2.9010 - val_loss: 7.9351 - val_mae: 1.9235 - val_mse: 7.9351\n",
      "Epoch 779/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 2.9342 - mae: 1.1296 - mse: 2.9342 - val_loss: 8.9846 - val_mae: 2.0509 - val_mse: 8.9846\n",
      "Epoch 780/1000\n",
      "250/250 [==============================] - 0s 79us/sample - loss: 3.0381 - mae: 1.1756 - mse: 3.0381 - val_loss: 8.4395 - val_mae: 1.9741 - val_mse: 8.4395\n",
      "Epoch 781/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 2.9415 - mae: 1.1671 - mse: 2.9415 - val_loss: 7.5678 - val_mae: 1.9255 - val_mse: 7.5678\n",
      "Epoch 782/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 3.0568 - mae: 1.1706 - mse: 3.0568 - val_loss: 7.1683 - val_mae: 1.8806 - val_mse: 7.1683\n",
      "Epoch 783/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 2.9662 - mae: 1.1297 - mse: 2.9662 - val_loss: 8.8904 - val_mae: 2.0173 - val_mse: 8.8904\n",
      "Epoch 784/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 3.0218 - mae: 1.1706 - mse: 3.0218 - val_loss: 8.0091 - val_mae: 1.9171 - val_mse: 8.0091\n",
      "Epoch 785/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 2.8771 - mae: 1.0999 - mse: 2.8771 - val_loss: 6.8512 - val_mae: 1.8034 - val_mse: 6.8512\n",
      "Epoch 786/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 2.9306 - mae: 1.1500 - mse: 2.9306 - val_loss: 8.3404 - val_mae: 1.9684 - val_mse: 8.3404\n",
      "Epoch 787/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 3.1271 - mae: 1.1866 - mse: 3.1271 - val_loss: 8.0795 - val_mae: 1.9319 - val_mse: 8.0795\n",
      "Epoch 788/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 2.9305 - mae: 1.1313 - mse: 2.9305 - val_loss: 7.3064 - val_mae: 1.8716 - val_mse: 7.3064\n",
      "Epoch 789/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 2.7763 - mae: 1.0980 - mse: 2.7763 - val_loss: 9.0273 - val_mae: 2.0280 - val_mse: 9.0273\n",
      "Epoch 790/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 2.8332 - mae: 1.1234 - mse: 2.8332 - val_loss: 7.3493 - val_mae: 1.8864 - val_mse: 7.3493\n",
      "Epoch 791/1000\n",
      "250/250 [==============================] - 0s 89us/sample - loss: 3.0920 - mae: 1.1620 - mse: 3.0920 - val_loss: 8.7084 - val_mae: 2.0321 - val_mse: 8.7084\n",
      "Epoch 792/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 2.8930 - mae: 1.1139 - mse: 2.8930 - val_loss: 7.4598 - val_mae: 1.9142 - val_mse: 7.4598\n",
      "Epoch 793/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 2.9185 - mae: 1.1479 - mse: 2.9185 - val_loss: 7.3776 - val_mae: 1.8848 - val_mse: 7.3776\n",
      "Epoch 794/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 2.8953 - mae: 1.1647 - mse: 2.8953 - val_loss: 7.2803 - val_mae: 1.9012 - val_mse: 7.2803\n",
      "Epoch 795/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 2.8441 - mae: 1.1103 - mse: 2.8441 - val_loss: 7.9916 - val_mae: 1.9057 - val_mse: 7.9916\n",
      "Epoch 796/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 3.0140 - mae: 1.1368 - mse: 3.0140 - val_loss: 7.3606 - val_mae: 1.8668 - val_mse: 7.3606\n",
      "Epoch 797/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 3.1549 - mae: 1.1993 - mse: 3.1549 - val_loss: 8.2180 - val_mae: 1.9572 - val_mse: 8.2180\n",
      "Epoch 798/1000\n",
      "250/250 [==============================] - 0s 82us/sample - loss: 2.7532 - mae: 1.0929 - mse: 2.7532 - val_loss: 7.6663 - val_mae: 1.8733 - val_mse: 7.6663\n",
      "Epoch 799/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 2.8901 - mae: 1.1436 - mse: 2.8901 - val_loss: 9.8505 - val_mae: 2.1771 - val_mse: 9.8505\n",
      "Epoch 800/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 2.9662 - mae: 1.1277 - mse: 2.9662 - val_loss: 9.9306 - val_mae: 2.1493 - val_mse: 9.9306\n",
      "Epoch 801/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 2.9424 - mae: 1.1147 - mse: 2.9424 - val_loss: 6.9744 - val_mae: 1.8509 - val_mse: 6.9744\n",
      "Epoch 802/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 2.9394 - mae: 1.1464 - mse: 2.9394 - val_loss: 7.7341 - val_mae: 1.9354 - val_mse: 7.7341\n",
      "Epoch 803/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 2.7956 - mae: 1.1002 - mse: 2.7956 - val_loss: 8.3930 - val_mae: 1.9783 - val_mse: 8.3930\n",
      "Epoch 804/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 3.1251 - mae: 1.1925 - mse: 3.1251 - val_loss: 8.2292 - val_mae: 1.9386 - val_mse: 8.2292\n",
      "Epoch 805/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 2.7401 - mae: 1.1083 - mse: 2.7401 - val_loss: 8.7382 - val_mae: 2.0139 - val_mse: 8.7382\n",
      "Epoch 806/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 2.9022 - mae: 1.1321 - mse: 2.9022 - val_loss: 8.6784 - val_mae: 2.0238 - val_mse: 8.6784\n",
      "Epoch 807/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 2.9048 - mae: 1.1423 - mse: 2.9048 - val_loss: 6.7933 - val_mae: 1.8193 - val_mse: 6.7933\n",
      "Epoch 808/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 2.8790 - mae: 1.1446 - mse: 2.8790 - val_loss: 7.4529 - val_mae: 1.8853 - val_mse: 7.4529\n",
      "Epoch 809/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 2.9637 - mae: 1.1548 - mse: 2.9637 - val_loss: 7.7844 - val_mae: 1.8972 - val_mse: 7.7844\n",
      "Epoch 810/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 2.7738 - mae: 1.0824 - mse: 2.7738 - val_loss: 7.6206 - val_mae: 1.9023 - val_mse: 7.6206\n",
      "Epoch 811/1000\n",
      "250/250 [==============================] - 0s 60us/sample - loss: 2.9003 - mae: 1.1371 - mse: 2.9003 - val_loss: 8.3902 - val_mae: 1.9626 - val_mse: 8.3902\n",
      "Epoch 812/1000\n",
      "250/250 [==============================] - 0s 59us/sample - loss: 2.8371 - mae: 1.0891 - mse: 2.8371 - val_loss: 7.2150 - val_mae: 1.9443 - val_mse: 7.2150\n",
      "Epoch 813/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 2.9789 - mae: 1.1317 - mse: 2.9789 - val_loss: 9.9397 - val_mae: 2.1792 - val_mse: 9.9397\n",
      "Epoch 814/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 2.9083 - mae: 1.1184 - mse: 2.9083 - val_loss: 7.3364 - val_mae: 1.8766 - val_mse: 7.3364\n",
      "Epoch 815/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 2.9946 - mae: 1.1330 - mse: 2.9946 - val_loss: 8.4380 - val_mae: 2.0392 - val_mse: 8.4380\n",
      "Epoch 816/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 2.7643 - mae: 1.0889 - mse: 2.7643 - val_loss: 7.1767 - val_mae: 1.8772 - val_mse: 7.1767\n",
      "Epoch 817/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 2.7408 - mae: 1.1035 - mse: 2.7408 - val_loss: 8.0223 - val_mae: 1.9619 - val_mse: 8.0223\n",
      "Epoch 818/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 2.8146 - mae: 1.0995 - mse: 2.8146 - val_loss: 7.1140 - val_mae: 1.9171 - val_mse: 7.1140\n",
      "Epoch 819/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 2.7968 - mae: 1.1152 - mse: 2.7968 - val_loss: 10.6672 - val_mae: 2.2547 - val_mse: 10.6672\n",
      "Epoch 820/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 3.0819 - mae: 1.2083 - mse: 3.0819 - val_loss: 10.2347 - val_mae: 2.1832 - val_mse: 10.2347\n",
      "Epoch 821/1000\n",
      "250/250 [==============================] - 0s 83us/sample - loss: 2.8176 - mae: 1.1236 - mse: 2.8176 - val_loss: 8.5633 - val_mae: 1.9990 - val_mse: 8.5633\n",
      "Epoch 822/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 2.8600 - mae: 1.1071 - mse: 2.8600 - val_loss: 8.6176 - val_mae: 2.0005 - val_mse: 8.6176\n",
      "Epoch 823/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 2.9766 - mae: 1.1459 - mse: 2.9766 - val_loss: 8.8538 - val_mae: 2.0171 - val_mse: 8.8538\n",
      "Epoch 824/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 2.8573 - mae: 1.0932 - mse: 2.8573 - val_loss: 9.2338 - val_mae: 2.0720 - val_mse: 9.2338\n",
      "Epoch 825/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 2.6870 - mae: 1.1008 - mse: 2.6870 - val_loss: 7.2262 - val_mae: 1.8269 - val_mse: 7.2262\n",
      "Epoch 826/1000\n",
      "250/250 [==============================] - 0s 82us/sample - loss: 2.7206 - mae: 1.1102 - mse: 2.7206 - val_loss: 9.1813 - val_mae: 2.0596 - val_mse: 9.1813\n",
      "Epoch 827/1000\n",
      "250/250 [==============================] - 0s 86us/sample - loss: 2.9485 - mae: 1.1819 - mse: 2.9485 - val_loss: 7.6824 - val_mae: 1.9167 - val_mse: 7.6824\n",
      "Epoch 828/1000\n",
      "250/250 [==============================] - 0s 76us/sample - loss: 2.7999 - mae: 1.1069 - mse: 2.7999 - val_loss: 8.8610 - val_mae: 2.0777 - val_mse: 8.8610\n",
      "Epoch 829/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 2.7341 - mae: 1.1163 - mse: 2.7341 - val_loss: 9.3633 - val_mae: 2.1223 - val_mse: 9.3633\n",
      "Epoch 830/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 2.8667 - mae: 1.1520 - mse: 2.8667 - val_loss: 10.7546 - val_mae: 2.2569 - val_mse: 10.7546\n",
      "Epoch 831/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 2.9129 - mae: 1.1333 - mse: 2.9129 - val_loss: 8.2076 - val_mae: 1.9656 - val_mse: 8.2076\n",
      "Epoch 832/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 2.7418 - mae: 1.0565 - mse: 2.7418 - val_loss: 7.0186 - val_mae: 1.8288 - val_mse: 7.0186\n",
      "Epoch 833/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 2.7972 - mae: 1.1032 - mse: 2.7972 - val_loss: 9.0156 - val_mae: 2.0591 - val_mse: 9.0156\n",
      "Epoch 834/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 3.0954 - mae: 1.1631 - mse: 3.0954 - val_loss: 8.5131 - val_mae: 1.9867 - val_mse: 8.5131\n",
      "Epoch 835/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 2.6659 - mae: 1.0822 - mse: 2.6659 - val_loss: 7.0546 - val_mae: 1.8402 - val_mse: 7.0546\n",
      "Epoch 836/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 2.8476 - mae: 1.1375 - mse: 2.8476 - val_loss: 8.4888 - val_mae: 2.0038 - val_mse: 8.4888\n",
      "Epoch 837/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 2.8150 - mae: 1.1112 - mse: 2.8150 - val_loss: 7.4358 - val_mae: 1.9026 - val_mse: 7.4358\n",
      "Epoch 838/1000\n",
      "250/250 [==============================] - 0s 102us/sample - loss: 2.7488 - mae: 1.0794 - mse: 2.7488 - val_loss: 9.3638 - val_mae: 2.0888 - val_mse: 9.3638\n",
      "Epoch 839/1000\n",
      "250/250 [==============================] - 0s 61us/sample - loss: 2.7801 - mae: 1.1393 - mse: 2.7801 - val_loss: 8.3688 - val_mae: 1.9875 - val_mse: 8.3688\n",
      "Epoch 840/1000\n",
      "250/250 [==============================] - 0s 60us/sample - loss: 2.7800 - mae: 1.1007 - mse: 2.7800 - val_loss: 7.8730 - val_mae: 1.9694 - val_mse: 7.8730\n",
      "Epoch 841/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 2.8507 - mae: 1.1056 - mse: 2.8507 - val_loss: 7.1310 - val_mae: 1.8400 - val_mse: 7.1310\n",
      "Epoch 842/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 2.8713 - mae: 1.0948 - mse: 2.8713 - val_loss: 7.6049 - val_mae: 1.9448 - val_mse: 7.6049\n",
      "Epoch 843/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 3.0093 - mae: 1.1778 - mse: 3.0093 - val_loss: 8.1465 - val_mae: 1.9566 - val_mse: 8.1465\n",
      "Epoch 844/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 2.8214 - mae: 1.1332 - mse: 2.8214 - val_loss: 8.3800 - val_mae: 1.9831 - val_mse: 8.3800\n",
      "Epoch 845/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 2.8305 - mae: 1.1009 - mse: 2.8305 - val_loss: 9.8230 - val_mae: 2.2236 - val_mse: 9.8230\n",
      "Epoch 846/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 2.7936 - mae: 1.1002 - mse: 2.7936 - val_loss: 7.9285 - val_mae: 1.9362 - val_mse: 7.9285\n",
      "Epoch 847/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 2.8073 - mae: 1.1102 - mse: 2.8073 - val_loss: 8.2487 - val_mae: 1.9456 - val_mse: 8.2487\n",
      "Epoch 848/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 2.8364 - mae: 1.1067 - mse: 2.8364 - val_loss: 7.4534 - val_mae: 1.8963 - val_mse: 7.4534\n",
      "Epoch 849/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 2.6673 - mae: 1.0657 - mse: 2.6673 - val_loss: 7.9299 - val_mae: 1.9403 - val_mse: 7.9299\n",
      "Epoch 850/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 2.7117 - mae: 1.0825 - mse: 2.7117 - val_loss: 7.3937 - val_mae: 1.8733 - val_mse: 7.3937\n",
      "Epoch 851/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 2.9358 - mae: 1.1269 - mse: 2.9358 - val_loss: 7.6037 - val_mae: 1.9620 - val_mse: 7.6037\n",
      "Epoch 852/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 2.7338 - mae: 1.1046 - mse: 2.7338 - val_loss: 7.2663 - val_mae: 1.8595 - val_mse: 7.2663\n",
      "Epoch 853/1000\n",
      "250/250 [==============================] - 0s 79us/sample - loss: 2.8294 - mae: 1.0935 - mse: 2.8294 - val_loss: 8.2080 - val_mae: 1.9782 - val_mse: 8.2080\n",
      "Epoch 854/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 2.7842 - mae: 1.1146 - mse: 2.7842 - val_loss: 8.5519 - val_mae: 2.0183 - val_mse: 8.5519\n",
      "Epoch 855/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 2.6286 - mae: 1.0466 - mse: 2.6286 - val_loss: 9.8654 - val_mae: 2.1968 - val_mse: 9.8654\n",
      "Epoch 856/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 2.7156 - mae: 1.1408 - mse: 2.7156 - val_loss: 8.4976 - val_mae: 1.9698 - val_mse: 8.4976\n",
      "Epoch 857/1000\n",
      "250/250 [==============================] - 0s 78us/sample - loss: 2.8151 - mae: 1.0980 - mse: 2.8151 - val_loss: 8.4583 - val_mae: 1.9924 - val_mse: 8.4583\n",
      "Epoch 858/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 2.7220 - mae: 1.0965 - mse: 2.7220 - val_loss: 7.1010 - val_mae: 1.8558 - val_mse: 7.1010\n",
      "Epoch 859/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 2.7421 - mae: 1.0833 - mse: 2.7421 - val_loss: 7.4291 - val_mae: 1.9083 - val_mse: 7.4291\n",
      "Epoch 860/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 2.6953 - mae: 1.0978 - mse: 2.6953 - val_loss: 9.9527 - val_mae: 2.2551 - val_mse: 9.9527\n",
      "Epoch 861/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 2.8570 - mae: 1.1387 - mse: 2.8570 - val_loss: 7.5984 - val_mae: 1.9768 - val_mse: 7.5984\n",
      "Epoch 862/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 2.7939 - mae: 1.0976 - mse: 2.7939 - val_loss: 8.3854 - val_mae: 2.0356 - val_mse: 8.3854\n",
      "Epoch 863/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 2.6462 - mae: 1.0281 - mse: 2.6462 - val_loss: 7.8863 - val_mae: 1.9400 - val_mse: 7.8863\n",
      "Epoch 864/1000\n",
      "250/250 [==============================] - 0s 78us/sample - loss: 2.8061 - mae: 1.0989 - mse: 2.8061 - val_loss: 11.7537 - val_mae: 2.3937 - val_mse: 11.7537\n",
      "Epoch 865/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 2.8546 - mae: 1.0846 - mse: 2.8546 - val_loss: 8.4953 - val_mae: 2.0739 - val_mse: 8.4953\n",
      "Epoch 866/1000\n",
      "250/250 [==============================] - 0s 61us/sample - loss: 2.6167 - mae: 1.0677 - mse: 2.6167 - val_loss: 7.8446 - val_mae: 1.9471 - val_mse: 7.8446\n",
      "Epoch 867/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 2.6752 - mae: 1.0964 - mse: 2.6752 - val_loss: 8.6198 - val_mae: 2.0342 - val_mse: 8.6198\n",
      "Epoch 868/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 2.7441 - mae: 1.1161 - mse: 2.7441 - val_loss: 7.5587 - val_mae: 1.9158 - val_mse: 7.5587\n",
      "Epoch 869/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 2.6992 - mae: 1.0743 - mse: 2.6992 - val_loss: 7.9336 - val_mae: 1.9548 - val_mse: 7.9336\n",
      "Epoch 870/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 2.6420 - mae: 1.0864 - mse: 2.6420 - val_loss: 10.6883 - val_mae: 2.2704 - val_mse: 10.6883\n",
      "Epoch 871/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 2.6810 - mae: 1.0768 - mse: 2.6810 - val_loss: 6.9269 - val_mae: 1.8531 - val_mse: 6.9269\n",
      "Epoch 872/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 2.8622 - mae: 1.1387 - mse: 2.8622 - val_loss: 10.0077 - val_mae: 2.1859 - val_mse: 10.0077\n",
      "Epoch 873/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 2.8423 - mae: 1.0995 - mse: 2.8423 - val_loss: 7.7985 - val_mae: 1.9348 - val_mse: 7.7985\n",
      "Epoch 874/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 2.6650 - mae: 1.0740 - mse: 2.6650 - val_loss: 8.7408 - val_mae: 2.0778 - val_mse: 8.7408\n",
      "Epoch 875/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 2.7772 - mae: 1.0967 - mse: 2.7772 - val_loss: 8.9674 - val_mae: 2.0738 - val_mse: 8.9674\n",
      "Epoch 876/1000\n",
      "250/250 [==============================] - 0s 85us/sample - loss: 2.6438 - mae: 1.0516 - mse: 2.6438 - val_loss: 8.7780 - val_mae: 2.0483 - val_mse: 8.7780\n",
      "Epoch 877/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 2.7990 - mae: 1.1024 - mse: 2.7990 - val_loss: 7.9127 - val_mae: 1.9570 - val_mse: 7.9127\n",
      "Epoch 878/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 2.6773 - mae: 1.0615 - mse: 2.6773 - val_loss: 7.5087 - val_mae: 1.8993 - val_mse: 7.5087\n",
      "Epoch 879/1000\n",
      "250/250 [==============================] - 0s 85us/sample - loss: 2.5866 - mae: 1.0841 - mse: 2.5866 - val_loss: 7.9909 - val_mae: 1.9780 - val_mse: 7.9909\n",
      "Epoch 880/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 2.5534 - mae: 1.0612 - mse: 2.5534 - val_loss: 8.2734 - val_mae: 1.9955 - val_mse: 8.2734\n",
      "Epoch 881/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 2.6864 - mae: 1.0856 - mse: 2.6864 - val_loss: 10.1491 - val_mae: 2.1940 - val_mse: 10.1491\n",
      "Epoch 882/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 2.6387 - mae: 1.0522 - mse: 2.6387 - val_loss: 6.7887 - val_mae: 1.8125 - val_mse: 6.7887\n",
      "Epoch 883/1000\n",
      "250/250 [==============================] - 0s 61us/sample - loss: 2.8194 - mae: 1.1447 - mse: 2.8194 - val_loss: 10.9598 - val_mae: 2.3156 - val_mse: 10.9598\n",
      "Epoch 884/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 2.7119 - mae: 1.0611 - mse: 2.7119 - val_loss: 7.9999 - val_mae: 1.9739 - val_mse: 7.9999\n",
      "Epoch 885/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 2.5758 - mae: 1.0457 - mse: 2.5758 - val_loss: 8.7175 - val_mae: 2.0425 - val_mse: 8.7175\n",
      "Epoch 886/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 2.8491 - mae: 1.1382 - mse: 2.8491 - val_loss: 7.9928 - val_mae: 1.9474 - val_mse: 7.9928\n",
      "Epoch 887/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 2.5093 - mae: 1.0347 - mse: 2.5093 - val_loss: 7.0178 - val_mae: 1.8478 - val_mse: 7.0178\n",
      "Epoch 888/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 2.8178 - mae: 1.0769 - mse: 2.8178 - val_loss: 6.9740 - val_mae: 1.9084 - val_mse: 6.9740\n",
      "Epoch 889/1000\n",
      "250/250 [==============================] - 0s 60us/sample - loss: 2.6875 - mae: 1.0984 - mse: 2.6875 - val_loss: 9.1267 - val_mae: 2.0903 - val_mse: 9.1267\n",
      "Epoch 890/1000\n",
      "250/250 [==============================] - 0s 61us/sample - loss: 2.6208 - mae: 1.0812 - mse: 2.6208 - val_loss: 7.0698 - val_mae: 1.8633 - val_mse: 7.0698\n",
      "Epoch 891/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 2.6084 - mae: 1.0838 - mse: 2.6084 - val_loss: 9.2503 - val_mae: 2.0975 - val_mse: 9.2503\n",
      "Epoch 892/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 2.6583 - mae: 1.0913 - mse: 2.6583 - val_loss: 9.5029 - val_mae: 2.1217 - val_mse: 9.5029\n",
      "Epoch 893/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 2.4431 - mae: 1.0001 - mse: 2.4431 - val_loss: 8.8594 - val_mae: 2.0370 - val_mse: 8.8594\n",
      "Epoch 894/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 2.6800 - mae: 1.0705 - mse: 2.6800 - val_loss: 7.7750 - val_mae: 1.9888 - val_mse: 7.7750\n",
      "Epoch 895/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 2.5836 - mae: 1.0606 - mse: 2.5836 - val_loss: 8.5250 - val_mae: 2.0704 - val_mse: 8.5250\n",
      "Epoch 896/1000\n",
      "250/250 [==============================] - 0s 81us/sample - loss: 2.6264 - mae: 1.0616 - mse: 2.6264 - val_loss: 7.9971 - val_mae: 1.9765 - val_mse: 7.9971\n",
      "Epoch 897/1000\n",
      "250/250 [==============================] - 0s 87us/sample - loss: 2.8441 - mae: 1.1490 - mse: 2.8441 - val_loss: 8.0823 - val_mae: 1.9546 - val_mse: 8.0823\n",
      "Epoch 898/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 2.7753 - mae: 1.0652 - mse: 2.7753 - val_loss: 8.5055 - val_mae: 2.0066 - val_mse: 8.5055\n",
      "Epoch 899/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 2.5538 - mae: 1.0207 - mse: 2.5538 - val_loss: 9.6964 - val_mae: 2.1759 - val_mse: 9.6964\n",
      "Epoch 900/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 2.6018 - mae: 1.0450 - mse: 2.6018 - val_loss: 9.2363 - val_mae: 2.0974 - val_mse: 9.2363\n",
      "Epoch 901/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 2.5594 - mae: 1.0497 - mse: 2.5594 - val_loss: 8.4518 - val_mae: 1.9951 - val_mse: 8.4518\n",
      "Epoch 902/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 2.7849 - mae: 1.0710 - mse: 2.7849 - val_loss: 8.2267 - val_mae: 2.0312 - val_mse: 8.2267\n",
      "Epoch 903/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 2.5718 - mae: 1.0828 - mse: 2.5718 - val_loss: 7.2302 - val_mae: 1.8700 - val_mse: 7.2302\n",
      "Epoch 904/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 2.5037 - mae: 1.0614 - mse: 2.5037 - val_loss: 7.3517 - val_mae: 1.9271 - val_mse: 7.3517\n",
      "Epoch 905/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 2.7081 - mae: 1.1115 - mse: 2.7081 - val_loss: 8.9709 - val_mae: 2.0912 - val_mse: 8.9709\n",
      "Epoch 906/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 2.7534 - mae: 1.0799 - mse: 2.7534 - val_loss: 7.8606 - val_mae: 1.9305 - val_mse: 7.8606\n",
      "Epoch 907/1000\n",
      "250/250 [==============================] - 0s 61us/sample - loss: 2.5489 - mae: 1.0272 - mse: 2.5489 - val_loss: 9.3084 - val_mae: 2.1027 - val_mse: 9.3084\n",
      "Epoch 908/1000\n",
      "250/250 [==============================] - 0s 80us/sample - loss: 2.4742 - mae: 1.0216 - mse: 2.4742 - val_loss: 8.0271 - val_mae: 1.9952 - val_mse: 8.0271\n",
      "Epoch 909/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 2.8094 - mae: 1.1213 - mse: 2.8094 - val_loss: 7.6309 - val_mae: 1.9279 - val_mse: 7.6309\n",
      "Epoch 910/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 2.4878 - mae: 1.0706 - mse: 2.4878 - val_loss: 7.4203 - val_mae: 1.9450 - val_mse: 7.4203\n",
      "Epoch 911/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 2.6132 - mae: 1.0915 - mse: 2.6132 - val_loss: 8.0503 - val_mae: 1.9924 - val_mse: 8.0503\n",
      "Epoch 912/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 2.5722 - mae: 1.0646 - mse: 2.5722 - val_loss: 7.2546 - val_mae: 1.9405 - val_mse: 7.2546\n",
      "Epoch 913/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 2.8501 - mae: 1.1215 - mse: 2.8501 - val_loss: 6.8395 - val_mae: 1.8397 - val_mse: 6.8395\n",
      "Epoch 914/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 2.6685 - mae: 1.0670 - mse: 2.6685 - val_loss: 7.3685 - val_mae: 1.8704 - val_mse: 7.3685\n",
      "Epoch 915/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 2.5101 - mae: 1.0696 - mse: 2.5101 - val_loss: 7.0383 - val_mae: 1.8798 - val_mse: 7.0383\n",
      "Epoch 916/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 2.4938 - mae: 1.0438 - mse: 2.4938 - val_loss: 8.6974 - val_mae: 2.0273 - val_mse: 8.6974\n",
      "Epoch 917/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 2.5175 - mae: 1.0353 - mse: 2.5175 - val_loss: 7.8984 - val_mae: 1.9921 - val_mse: 7.8984\n",
      "Epoch 918/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 2.5739 - mae: 1.0579 - mse: 2.5739 - val_loss: 7.8460 - val_mae: 1.9414 - val_mse: 7.8460\n",
      "Epoch 919/1000\n",
      "250/250 [==============================] - 0s 79us/sample - loss: 2.6511 - mae: 1.0956 - mse: 2.6511 - val_loss: 7.6015 - val_mae: 1.9582 - val_mse: 7.6015\n",
      "Epoch 920/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 2.4663 - mae: 1.0280 - mse: 2.4663 - val_loss: 8.8507 - val_mae: 2.0672 - val_mse: 8.8507\n",
      "Epoch 921/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 2.7463 - mae: 1.1005 - mse: 2.7463 - val_loss: 8.6843 - val_mae: 2.0752 - val_mse: 8.6843\n",
      "Epoch 922/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 2.5716 - mae: 1.0575 - mse: 2.5716 - val_loss: 8.3324 - val_mae: 2.0193 - val_mse: 8.3324\n",
      "Epoch 923/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 2.5428 - mae: 1.0637 - mse: 2.5428 - val_loss: 11.3433 - val_mae: 2.3430 - val_mse: 11.3433\n",
      "Epoch 924/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 2.7033 - mae: 1.0928 - mse: 2.7033 - val_loss: 7.6301 - val_mae: 1.9681 - val_mse: 7.6301\n",
      "Epoch 925/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 2.5785 - mae: 1.0511 - mse: 2.5785 - val_loss: 8.0182 - val_mae: 1.9541 - val_mse: 8.0182\n",
      "Epoch 926/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 2.5520 - mae: 1.0484 - mse: 2.5520 - val_loss: 9.3801 - val_mae: 2.0959 - val_mse: 9.3801\n",
      "Epoch 927/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 2.4206 - mae: 1.0103 - mse: 2.4206 - val_loss: 7.4166 - val_mae: 1.9794 - val_mse: 7.4166\n",
      "Epoch 928/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 2.6282 - mae: 1.0572 - mse: 2.6282 - val_loss: 9.7373 - val_mae: 2.1532 - val_mse: 9.7373\n",
      "Epoch 929/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 2.6550 - mae: 1.0888 - mse: 2.6550 - val_loss: 7.1815 - val_mae: 1.8783 - val_mse: 7.1815\n",
      "Epoch 930/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 2.5857 - mae: 1.0465 - mse: 2.5857 - val_loss: 6.9104 - val_mae: 1.8666 - val_mse: 6.9104\n",
      "Epoch 931/1000\n",
      "250/250 [==============================] - 0s 63us/sample - loss: 2.5398 - mae: 1.0477 - mse: 2.5398 - val_loss: 7.9158 - val_mae: 1.9630 - val_mse: 7.9158\n",
      "Epoch 932/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 2.3721 - mae: 1.0265 - mse: 2.3721 - val_loss: 7.8401 - val_mae: 1.9975 - val_mse: 7.8401\n",
      "Epoch 933/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 2.6354 - mae: 1.0878 - mse: 2.6354 - val_loss: 7.2578 - val_mae: 1.8888 - val_mse: 7.2578\n",
      "Epoch 934/1000\n",
      "250/250 [==============================] - 0s 61us/sample - loss: 2.5547 - mae: 1.0543 - mse: 2.5547 - val_loss: 8.3592 - val_mae: 2.0522 - val_mse: 8.3592\n",
      "Epoch 935/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 2.5313 - mae: 1.0641 - mse: 2.5313 - val_loss: 7.9869 - val_mae: 1.9467 - val_mse: 7.9869\n",
      "Epoch 936/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 2.5575 - mae: 1.0507 - mse: 2.5575 - val_loss: 8.6668 - val_mae: 2.0393 - val_mse: 8.6668\n",
      "Epoch 937/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 2.8266 - mae: 1.1047 - mse: 2.8266 - val_loss: 8.2701 - val_mae: 1.9957 - val_mse: 8.2701\n",
      "Epoch 938/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 2.4449 - mae: 1.0274 - mse: 2.4449 - val_loss: 8.6355 - val_mae: 2.0343 - val_mse: 8.6355\n",
      "Epoch 939/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 2.4046 - mae: 1.0064 - mse: 2.4046 - val_loss: 9.2026 - val_mae: 2.1021 - val_mse: 9.2026\n",
      "Epoch 940/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 2.5550 - mae: 1.0467 - mse: 2.5550 - val_loss: 7.2234 - val_mae: 1.8856 - val_mse: 7.2234\n",
      "Epoch 941/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 2.5641 - mae: 1.0449 - mse: 2.5641 - val_loss: 7.1068 - val_mae: 1.8277 - val_mse: 7.1068\n",
      "Epoch 942/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 2.5096 - mae: 1.0639 - mse: 2.5096 - val_loss: 9.2237 - val_mae: 2.0903 - val_mse: 9.2237\n",
      "Epoch 943/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 2.5508 - mae: 1.0747 - mse: 2.5508 - val_loss: 8.1106 - val_mae: 1.9749 - val_mse: 8.1106\n",
      "Epoch 944/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 2.6675 - mae: 1.0679 - mse: 2.6675 - val_loss: 9.0240 - val_mae: 2.0617 - val_mse: 9.0240\n",
      "Epoch 945/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 2.4097 - mae: 1.0376 - mse: 2.4097 - val_loss: 8.5469 - val_mae: 2.0451 - val_mse: 8.5469\n",
      "Epoch 946/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 2.4338 - mae: 1.0588 - mse: 2.4338 - val_loss: 9.0605 - val_mae: 2.0941 - val_mse: 9.0605\n",
      "Epoch 947/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 2.6326 - mae: 1.0800 - mse: 2.6326 - val_loss: 9.3450 - val_mae: 2.1197 - val_mse: 9.3450\n",
      "Epoch 948/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 2.5316 - mae: 1.0133 - mse: 2.5316 - val_loss: 7.8225 - val_mae: 1.9246 - val_mse: 7.8225\n",
      "Epoch 949/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 2.4131 - mae: 1.0456 - mse: 2.4131 - val_loss: 7.4089 - val_mae: 1.8922 - val_mse: 7.4089\n",
      "Epoch 950/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 2.5834 - mae: 1.0774 - mse: 2.5834 - val_loss: 7.4350 - val_mae: 1.8856 - val_mse: 7.4350\n",
      "Epoch 951/1000\n",
      "250/250 [==============================] - 0s 83us/sample - loss: 2.5401 - mae: 1.0457 - mse: 2.5401 - val_loss: 8.6968 - val_mae: 2.0520 - val_mse: 8.6968\n",
      "Epoch 952/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 2.5332 - mae: 1.0741 - mse: 2.5332 - val_loss: 7.9401 - val_mae: 1.9823 - val_mse: 7.9401\n",
      "Epoch 953/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 2.1207 - mae: 0.9620 - mse: 2.1207 - val_loss: 6.9151 - val_mae: 1.9343 - val_mse: 6.9151\n",
      "Epoch 954/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 2.5236 - mae: 1.0565 - mse: 2.5236 - val_loss: 7.9476 - val_mae: 1.9566 - val_mse: 7.9476\n",
      "Epoch 955/1000\n",
      "250/250 [==============================] - 0s 66us/sample - loss: 2.4835 - mae: 1.0315 - mse: 2.4835 - val_loss: 8.6513 - val_mae: 2.0424 - val_mse: 8.6513\n",
      "Epoch 956/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 2.3433 - mae: 0.9789 - mse: 2.3433 - val_loss: 7.4535 - val_mae: 1.9430 - val_mse: 7.4535\n",
      "Epoch 957/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 2.3301 - mae: 1.0171 - mse: 2.3301 - val_loss: 8.9537 - val_mae: 2.0532 - val_mse: 8.9537\n",
      "Epoch 958/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 2.6355 - mae: 1.0573 - mse: 2.6355 - val_loss: 9.5141 - val_mae: 2.1581 - val_mse: 9.5141\n",
      "Epoch 959/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 2.3932 - mae: 1.0159 - mse: 2.3932 - val_loss: 7.3414 - val_mae: 1.9504 - val_mse: 7.3414\n",
      "Epoch 960/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 2.4642 - mae: 1.0641 - mse: 2.4642 - val_loss: 8.3583 - val_mae: 2.0266 - val_mse: 8.3583\n",
      "Epoch 961/1000\n",
      "250/250 [==============================] - 0s 64us/sample - loss: 2.3452 - mae: 1.0262 - mse: 2.3452 - val_loss: 7.8990 - val_mae: 1.9775 - val_mse: 7.8990\n",
      "Epoch 962/1000\n",
      "250/250 [==============================] - 0s 86us/sample - loss: 2.4418 - mae: 1.0310 - mse: 2.4418 - val_loss: 7.6755 - val_mae: 1.9348 - val_mse: 7.6755\n",
      "Epoch 963/1000\n",
      "250/250 [==============================] - 0s 107us/sample - loss: 2.6236 - mae: 1.0845 - mse: 2.6236 - val_loss: 8.5664 - val_mae: 1.9999 - val_mse: 8.5664\n",
      "Epoch 964/1000\n",
      "250/250 [==============================] - 0s 87us/sample - loss: 2.6147 - mae: 1.1071 - mse: 2.6147 - val_loss: 7.5127 - val_mae: 1.9451 - val_mse: 7.5127\n",
      "Epoch 965/1000\n",
      "250/250 [==============================] - 0s 62us/sample - loss: 2.3880 - mae: 1.0174 - mse: 2.3880 - val_loss: 8.2496 - val_mae: 1.9933 - val_mse: 8.2496\n",
      "Epoch 966/1000\n",
      "250/250 [==============================] - 0s 107us/sample - loss: 2.4591 - mae: 1.0285 - mse: 2.4591 - val_loss: 7.2875 - val_mae: 1.9572 - val_mse: 7.2875\n",
      "Epoch 967/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 2.4623 - mae: 1.0468 - mse: 2.4623 - val_loss: 7.0069 - val_mae: 1.8703 - val_mse: 7.0069\n",
      "Epoch 968/1000\n",
      "250/250 [==============================] - 0s 81us/sample - loss: 2.4272 - mae: 1.0119 - mse: 2.4272 - val_loss: 8.6812 - val_mae: 2.0506 - val_mse: 8.6812\n",
      "Epoch 969/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 2.4269 - mae: 1.0200 - mse: 2.4269 - val_loss: 7.9896 - val_mae: 1.9934 - val_mse: 7.9896\n",
      "Epoch 970/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 2.3982 - mae: 1.0084 - mse: 2.3982 - val_loss: 9.4559 - val_mae: 2.1520 - val_mse: 9.4559\n",
      "Epoch 971/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 2.5770 - mae: 1.0298 - mse: 2.5770 - val_loss: 9.1304 - val_mae: 2.1607 - val_mse: 9.1304\n",
      "Epoch 972/1000\n",
      "250/250 [==============================] - 0s 79us/sample - loss: 2.3284 - mae: 1.0293 - mse: 2.3284 - val_loss: 7.3490 - val_mae: 1.9041 - val_mse: 7.3490\n",
      "Epoch 973/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 2.3933 - mae: 1.0356 - mse: 2.3933 - val_loss: 9.0671 - val_mae: 2.1208 - val_mse: 9.0671\n",
      "Epoch 974/1000\n",
      "250/250 [==============================] - 0s 80us/sample - loss: 2.5174 - mae: 1.0419 - mse: 2.5174 - val_loss: 7.8942 - val_mae: 1.9463 - val_mse: 7.8942\n",
      "Epoch 975/1000\n",
      "250/250 [==============================] - 0s 69us/sample - loss: 2.4086 - mae: 1.0151 - mse: 2.4086 - val_loss: 8.7255 - val_mae: 2.0538 - val_mse: 8.7255\n",
      "Epoch 976/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 2.4403 - mae: 1.0515 - mse: 2.4403 - val_loss: 7.6506 - val_mae: 1.9395 - val_mse: 7.6506\n",
      "Epoch 977/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 2.6426 - mae: 1.0557 - mse: 2.6426 - val_loss: 7.6247 - val_mae: 1.9332 - val_mse: 7.6247\n",
      "Epoch 978/1000\n",
      "250/250 [==============================] - 0s 60us/sample - loss: 2.3108 - mae: 0.9845 - mse: 2.3108 - val_loss: 7.1510 - val_mae: 1.8875 - val_mse: 7.1510\n",
      "Epoch 979/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 2.6520 - mae: 1.0983 - mse: 2.6520 - val_loss: 8.1954 - val_mae: 1.9775 - val_mse: 8.1954\n",
      "Epoch 980/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 2.4335 - mae: 1.0304 - mse: 2.4335 - val_loss: 7.6340 - val_mae: 1.9045 - val_mse: 7.6340\n",
      "Epoch 981/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 2.4641 - mae: 1.0166 - mse: 2.4641 - val_loss: 7.6401 - val_mae: 1.9215 - val_mse: 7.6401\n",
      "Epoch 982/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 2.2859 - mae: 0.9922 - mse: 2.2859 - val_loss: 8.4243 - val_mae: 2.0174 - val_mse: 8.4243\n",
      "Epoch 983/1000\n",
      "250/250 [==============================] - 0s 92us/sample - loss: 2.4325 - mae: 1.0436 - mse: 2.4325 - val_loss: 9.7591 - val_mae: 2.1976 - val_mse: 9.7591\n",
      "Epoch 984/1000\n",
      "250/250 [==============================] - 0s 68us/sample - loss: 2.4644 - mae: 1.0285 - mse: 2.4644 - val_loss: 7.5230 - val_mae: 1.9097 - val_mse: 7.5230\n",
      "Epoch 985/1000\n",
      "250/250 [==============================] - 0s 85us/sample - loss: 2.4600 - mae: 0.9715 - mse: 2.4600 - val_loss: 7.5185 - val_mae: 1.9287 - val_mse: 7.5185\n",
      "Epoch 986/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 2.3450 - mae: 1.0046 - mse: 2.3450 - val_loss: 7.4209 - val_mae: 1.9132 - val_mse: 7.4209\n",
      "Epoch 987/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 2.3379 - mae: 1.0014 - mse: 2.3379 - val_loss: 10.4893 - val_mae: 2.2359 - val_mse: 10.4893\n",
      "Epoch 988/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 2.4489 - mae: 1.0427 - mse: 2.4489 - val_loss: 7.9479 - val_mae: 1.9636 - val_mse: 7.9479\n",
      "Epoch 989/1000\n",
      "250/250 [==============================] - 0s 65us/sample - loss: 2.4614 - mae: 0.9842 - mse: 2.4614 - val_loss: 9.2774 - val_mae: 2.1517 - val_mse: 9.2774\n",
      "Epoch 990/1000\n",
      "250/250 [==============================] - 0s 71us/sample - loss: 2.3465 - mae: 1.0157 - mse: 2.3465 - val_loss: 8.7327 - val_mae: 2.0691 - val_mse: 8.7327\n",
      "Epoch 991/1000\n",
      "250/250 [==============================] - 0s 60us/sample - loss: 2.4646 - mae: 1.0707 - mse: 2.4646 - val_loss: 9.1271 - val_mae: 2.0998 - val_mse: 9.1271\n",
      "Epoch 992/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 2.4156 - mae: 1.0210 - mse: 2.4156 - val_loss: 10.3534 - val_mae: 2.2283 - val_mse: 10.3534\n",
      "Epoch 993/1000\n",
      "250/250 [==============================] - 0s 74us/sample - loss: 2.3619 - mae: 0.9823 - mse: 2.3619 - val_loss: 8.2463 - val_mae: 1.9920 - val_mse: 8.2463\n",
      "Epoch 994/1000\n",
      "250/250 [==============================] - 0s 98us/sample - loss: 2.4263 - mae: 1.0304 - mse: 2.4263 - val_loss: 7.4273 - val_mae: 1.9110 - val_mse: 7.4273\n",
      "Epoch 995/1000\n",
      "250/250 [==============================] - 0s 73us/sample - loss: 2.4459 - mae: 1.0162 - mse: 2.4459 - val_loss: 8.7501 - val_mae: 2.0315 - val_mse: 8.7501\n",
      "Epoch 996/1000\n",
      "250/250 [==============================] - 0s 72us/sample - loss: 2.3006 - mae: 1.0166 - mse: 2.3006 - val_loss: 8.0797 - val_mae: 1.9595 - val_mse: 8.0797\n",
      "Epoch 997/1000\n",
      "250/250 [==============================] - 0s 67us/sample - loss: 2.3551 - mae: 0.9913 - mse: 2.3551 - val_loss: 9.8724 - val_mae: 2.1962 - val_mse: 9.8724\n",
      "Epoch 998/1000\n",
      "250/250 [==============================] - 0s 77us/sample - loss: 2.6218 - mae: 1.0695 - mse: 2.6218 - val_loss: 8.5066 - val_mae: 2.0909 - val_mse: 8.5066\n",
      "Epoch 999/1000\n",
      "250/250 [==============================] - 0s 75us/sample - loss: 2.1893 - mae: 0.9642 - mse: 2.1893 - val_loss: 8.0633 - val_mae: 1.9964 - val_mse: 8.0633\n",
      "Epoch 1000/1000\n",
      "250/250 [==============================] - 0s 70us/sample - loss: 2.3222 - mae: 0.9830 - mse: 2.3222 - val_loss: 7.7049 - val_mae: 1.9261 - val_mse: 7.7049\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1000\n",
    "\n",
    "### START CODING HERE ### \n",
    "# fit nn_reg1 on X_train, y_train, epochs=EPOCHS, validation_split=0.2, verbose=1\n",
    "nn_reg1_history = nn_reg1.fit(X_train, y_train, epochs=EPOCHS, validation_split=0.2, verbose=1)\n",
    "### END CODING HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "joFiwWyM21MJ",
    "outputId": "a3d9a3e3-058f-4bcd-8a47-372a41dfc28e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "79/79 - 0s - loss: 8.7345 - mae: 2.2004 - mse: 8.7345\n",
      "Testing set Mean Abs Error:  2.20 MPG\n"
     ]
    }
   ],
   "source": [
    "### START CODING HERE ### \n",
    "# Evaluate nn_reg1 using .evaluate() method on X_test, y_test and verbose=2\n",
    "loss1, mae1, mse1 = nn_reg1.evaluate(X_test, y_test, verbose=2)\n",
    "### END CODING HERE ###\n",
    "\n",
    "print(\"Testing set Mean Abs Error: {:5.2f} MPG\".format(mae1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "jLNWfAHm21ML",
    "outputId": "ea22fb63-a20b-484d-9c34-0c7ea2482e68"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mae</th>\n",
       "      <th>val_mse</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>2.300627</td>\n",
       "      <td>1.016647</td>\n",
       "      <td>2.300627</td>\n",
       "      <td>8.079683</td>\n",
       "      <td>1.959532</td>\n",
       "      <td>8.079683</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2.355072</td>\n",
       "      <td>0.991261</td>\n",
       "      <td>2.355072</td>\n",
       "      <td>9.872364</td>\n",
       "      <td>2.196214</td>\n",
       "      <td>9.872364</td>\n",
       "      <td>996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2.621769</td>\n",
       "      <td>1.069454</td>\n",
       "      <td>2.621769</td>\n",
       "      <td>8.506604</td>\n",
       "      <td>2.090942</td>\n",
       "      <td>8.506604</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2.189309</td>\n",
       "      <td>0.964201</td>\n",
       "      <td>2.189309</td>\n",
       "      <td>8.063314</td>\n",
       "      <td>1.996383</td>\n",
       "      <td>8.063314</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2.322169</td>\n",
       "      <td>0.983005</td>\n",
       "      <td>2.322169</td>\n",
       "      <td>7.704936</td>\n",
       "      <td>1.926082</td>\n",
       "      <td>7.704936</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss       mae       mse  val_loss   val_mae   val_mse  epoch\n",
       "995  2.300627  1.016647  2.300627  8.079683  1.959532  8.079683    995\n",
       "996  2.355072  0.991261  2.355072  9.872364  2.196214  9.872364    996\n",
       "997  2.621769  1.069454  2.621769  8.506604  2.090942  8.506604    997\n",
       "998  2.189309  0.964201  2.189309  8.063314  1.996383  8.063314    998\n",
       "999  2.322169  0.983005  2.322169  7.704936  1.926082  7.704936    999"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your numbers might be a little different due to the randomness!\n",
    "hist1 = pd.DataFrame(nn_reg1_history.history)\n",
    "hist1['epoch'] = nn_reg1_history.epoch\n",
    "hist1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "colab_type": "code",
    "id": "nN510bmp21MP",
    "outputId": "a3f3135d-f421-4bff-89a5-b2b87b169a95"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEKCAYAAAAYd05sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3hUVfrA8e+ZyaRAIEDoBAhNehGi\ngCjNhgVssIoN27Lq2tdVdN2fqLtrXXsv2AV7WRRUEAUVwYCI9F5CJ4QE0jNzfn+cO5nJZJKZJDOZ\nZPJ+nidP5t65c++ZuTPvPfdUpbVGCCFE9LFFOgFCCCHCQwK8EEJEKQnwQggRpSTACyFElJIAL4QQ\nUUoCvBBCRKmYcO5cKbUNOAI4gRKtdVo4jyeEEMIjrAHeMkZrfbAWjiOEEMKLFNEIIUSUUuHsyaqU\n2gpkARp4SWv9sp9tpgJTARo3bjykV69eNTrm4ayDNMvfCS2PgdjGNdqXEELUdcuWLTuotW7l77lw\nB/gOWutdSqnWwLfAjVrrhRVtn5aWptPT02t0zM8+eptzV91AyRVziUkdXqN9CSFEXaeUWlZR/WZY\ni2i01rus//uBT4Hjw3k8AGWzA+ByloT7UEIIUaeFLcArpRorpZq4HwOnAavCdbxSNlNvrJ3OsB9K\nCCHqsnC2omkDfKqUch/nPa313DAeD/Dk4J3O4nAfSggh6rSwBXit9RZgYLj2XyG75OCFiITi4mIy\nMjIoKCiIdFKiUnx8PCkpKTgcjqBfUxvt4GuVUlYZvEvK4IWoTRkZGTRp0oTU1FSsO3cRIlprMjMz\nycjIoEuXLkG/LurawasYc81ylUgRjRC1qaCggOTkZAnuYaCUIjk5ucp3R9EX4K1KVpdLimiEqG0S\n3MOnOp9t1AV4m1XJqqWSVQjRwEVdgFd2UwGhJQcvRIOSmZnJoEGDGDRoEG3btqVDhw6ly0VFRUHt\n48orr2T9+vVBH/PVV1+lVatWpccZNGhQlV4fblFXyWqzm2uWdHQSomFJTk5mxYoVAEyfPp3ExERu\nv/32MttordFaY7P5z9u+/vrrVT7uJZdcwpNPPlnh8yUlJcTEeEJtoDR4czqd2O32KqfJLepy8O6O\nTi5pJimEADZt2kSfPn245JJL6Nu3L3v27GHq1KmkpaXRt29f7r///tJtTzzxRFasWEFJSQnNmjVj\n2rRpDBw4kOHDh7N///6gjzlv3jxGjx7N2WefTf/+/f2m4Z133qF///7069ePu+++G6D0uLfccgsD\nBgxg6dKlNXrvUZiDt9qIuqQMXohIue9/q1mzOyek++zTvin3ju9brdeuW7eOt956i7Q0M2TLQw89\nRIsWLSgpKWHMmDFMnDiRPn36lHlNdnY2o0aN4qGHHuK2225jxowZTJs2rdy+3333Xb7//vvSZXdQ\nTk9PZ82aNXTq1IlNmzaVSUNGRgb33HMP6enpJCUlccoppzB79mzGjRtHdnY2I0eOrPSuIFhRl4O3\n2WUsGiFEWd26dSsN7gAzZ85k8ODBDB48mLVr17JmzZpyr0lISOCMM84AYMiQIWzbts3vvi+55BJW\nrFhR+hcbGwvA8OHD6dSpk980LFmyhLFjx9KyZUscDgcXX3wxCxeacRhjY2M577zzQvK+oy4H724m\nKZWsQkROdXPa4dK4sWfo8I0bN/LUU0+xdOlSmjVrxqWXXuq3fbk7UAPY7XZKSqqWafQ+pr/liiQk\nJISsuWkU5uDdQxVIDl4IUV5OTg5NmjShadOm7Nmzh6+//rrW0zB06FAWLFhAZmYmJSUlzJo1i1Gj\nRoX8ONGXg7dLDl4IUbHBgwfTp08fevXqRefOnRkxYkSN9udbBv/SSy8FfE1KSgoPPPAAo0ePRmvN\n+PHjOeuss6p8lxBIWCf8qKpQTPjx09odjHi/PzuH3EnH8XeHKGVCiEDWrl1L7969I52MqObvM47Y\nhB+RYLNJEY0QQkAUBngV424mKUU0QoiGLeoCfIzdjksraQcvhGjwoi7A22yKEmxSySqEaPCiLsDb\nlcKJHWTCDyFEAxd9Ad6mKMEulaxCiAYvKgO8ExtKyuCFaFDGjBlTrtPSk08+yXXXXVfp6xITE/2u\nt9vtZYYBfuihh0KW1toSdR2dSnPwUkQjRIMyefJkZs2axemnn166btasWTzyyCPV2l9CQkLp8MMV\n8R3O13do4IoEu11NRV0O3qZMgJdmkkI0LBMnTuTLL78sndxj27Zt7N69m5NOOomjR49y8sknM3jw\nYPr378/nn39e7eOkpqZy5513MnjwYD788ENGjx7NLbfcQlpaGk899RTbtm1j7NixDBgwgJNPPpkd\nO3YAcMUVV3DttdcydOhQ7rjjjpC850CiLgcfY+XgleTghYicOdNg7x+h3Wfb/nBGxcUkLVq04Pjj\nj2fOnDmcc845zJo1iz/96U8opYiPj+fTTz+ladOmHDx4kGHDhjFhwoRKB/XKz89n0KBBpct33XUX\nF154IWAmF1m+fDkAL774IkVFRbh74Y8fP54pU6YwZcoUZsyYwU033cRnn30GQEZGBj///HONJvGo\niqgL8Habwqlt0opGiAbIXUzjDvCvvfYaYGZRuvvuu1m4cCE2m41du3axb98+2rZtW+G+KiuicQd6\nf8uLFy/mk08+AeCyyy4rk1ufNGlSrQV3iMIAb7MpirDjkAAvRORUktMOp3POOYdbb72V5cuXk5eX\nx5AhQwAzINiBAwdYtmwZDoeD1NRUv0MEB6u6QwEHu12oRF0ZvGkHb5MyeCEaoMTERMaMGcNVV13F\n5MmTS9dnZ2fTunVrHA4HCxYsYPv27WFLwwknnMCsWbMAc2E56aSTwnasQKIuB29a0cSgtOTghWiI\nJk+ezHnnnVcaZMHMujR+/Hj69+9PWloavXr1Crgf3zL4cePGBdVU8plnnuHKK6/k0UcfpVWrVtWa\nyDtUojTASxm8EA3Vueeei+8w6C1btmTx4sV+tz969Kjf9U6n/1IA36n7vMeCB+jcuTPfffddude9\n8cYb/hMcRlFaRGNHaSmiEUI0bFEX4G02KMEmzSSFEA1e1AX4GJvN5OAlwAtR6+rSDHHRpjqfbdQF\neJsNirUU0QhR2+Lj48nMzJQgHwZaazIzM4mPj6/S66KvkrW0DF5y8ELUppSUFDIyMjhw4ECkkxKV\n4uPjSUlJqdJroi/AW61olLSDF6JWORwOunTpEulkCC9RV0SjlMKlJAcvhBBhD/BKKbtS6jel1Oxw\nH8tNK6lkFUKI2sjB3wysrYXjlHKqGKlkFUI0eGEN8EqpFOAs4NVwHseXVjHYpIhGCNHAhTsH/yRw\nB+CqaAOl1FSlVLpSKj1Ute/aJs0khRAibAFeKXU2sF9rvayy7bTWL2ut07TWaa1atQrJsbWyY5MA\nL4Ro4MKZgx8BTFBKbQNmAWOVUu+E8XiltIrBJpWsQogGLmwBXmt9l9Y6RWudClwEfKe1vjRcxyvD\nFoMNycELIRq2qGsHD6BtMVJEI4Ro8GqlJ6vW+nvg+9o4FgC2GOwS4IUQDVyU5uDtUkQjhGjwojLA\nK1sMMThBRrUTQjRgURngsVklTzLgmBCiAYvKAK/tDvNAmkoKIRqwqAzwttIcvAR4IUTDFZUBXkmA\nF0KIyptJKqVaBLEPl9b6cIjSExp2CfBCCBGoHfxu609Vso0d6BSyFIWATQK8EEIEDPBrtdbHVraB\nUuq3EKYnNNyVrM7iyKZDCCEiKFAZ/PAg9hHMNrUrxsw8rksKI5wQIYSInEpz8FrrAu9lpVQHTJEM\nwG6tdYnvNnVCTBwAzqL86JtVXAghghSokvUuwKG1vt9atRg4DMQCbwIPhjd51aOsHHxJUYEEeCFE\ngxWoiGYS8F+v5Uyt9QCgL2YqvrrJYQX4wvwIJ0QIISInYDt4rXWu1+JT1jonkBCuRNWUzSqiKSmS\nAC+EaLgCBfhEpZTDvaC1fgNAKRUHNA1jumrEFmty8M7iulc9IIQQtSVQgP8IeEkp1ci9QinVGHjR\neq5OsjvMzUVJQW6ALYUQInoFCvD/BPYDO5RSy5RSy4FtwD7ruTopJs4K8EWSgxdCNFyBmkk6gWlK\nqfuA7tbqTVrrOl247QnwdTqZQggRVpXm4JVSPZRSnwO/AncDh+p6cAdwWAHeJTl4IUQDFqiIZgYw\nG7gAWA48E/YUhUBsnKkycBXX+WuREEKETaB+QE201q9Yjx+1yuDrvFh3Dl5a0QghGrBAAT5eKXUs\nntEkE7yXtdZ1MuDHxcVSrO0S4IUQDVqgAL8XeLyCZQ2MDUeiaireYaeAWCiRAC+EaLgCtaIZXUvp\nCKn4GBuFONDFMpqkEKLhCjTY2PmVPa+1/iS0yQmNOIedbBySgxdCNGiBimg+AlZYf1B2ZicN1MkA\nHx9jY792oJySgxdCNFyBAvz5wEXAAOBzYKbWelPYU1VDMXYbRTiwyYQfQogGrNJ28Frrz7TWFwGj\ngM3Af5VSPyqlRtVK6mqgWMVic0oRjRCi4Qo4XLClAMgGcoBEID5sKQqRIhUnAV4I0aAFqmQdiymi\nOR6YBzyltU6vjYTVVJEtnhhnXqSTIYQQEROoDH4esBL4EYgDLldKXe5+Umt9UxjTViNFtngcrsxI\nJ0MIISImUIC/CtNapt4ptsXjcEkRjRCi4QrU0emNWkpHyJXYE4iVdvBCiAYs0HDB0wPtIJhtIsEV\nk0Cs5OCFEA1YoCKaa5RSOZU8rzCVsNPLPaFUPLAQU3YfA3yktb63mumsMldMAvEUgssFtmAbCwkh\nRPQIFOBfAZoEsY0/hcBYrfVRa+LuH5VSc7TWv1Q1kdUSY4YMxlkItoRaOaQQQtQlgcrg76vujrXW\nGjhqLTqsv1qrsLU7Ys0DZzE4JMALIRqesJZdKKXsSqkVmIm7v9VaL/GzzVSlVLpSKv3AgQMhO3aZ\nAC+EEA1QWAO81tqptR4EpADHK6X6+dnmZa11mtY6rVWrViE7tsMK8MUyZLAQooEKGOCtXPitNTmI\n1vowsAAYV5P9VIXdYUZTyM+XljRCiIYpYIDXWjuByVXdsVKqlVKqmfU4ATgVWFflFFZTbKzJwecV\nSoAXQjRMgVrRuP2klHoWeB/Ida8MMCdrO+BNpZQdcyH5QGs9u9oprSJHbBwABQUS4IUQDVOwAX6Q\n9f9+r3WVzsmqtV4JHFvNdNWYOwcvAV4I0VAFFeC11mPCnZBQc8SZMvhCKaIRQjRQQbWiUUolKaUe\ndzdnVEr9VymVFO7E1UR8rAnwkoMXQjRUwTaTnAEcAf5k/eUAr4crUaEQG2fK4AuLpJmkEKJhCrYM\nvpvW+gKv5fusDkx1Vny8ycG3ypgHnBfZxAghRAQEm4PPV0qd6F5QSo0A8sOTpNCIt3Lwvbe/E+GU\nCCFEZASbg78WeMur3D0LmBKeJIVGvJaiGSFEwxYwwCulbEBPrfVApVRTAK11ZUMI1wm2pm0jnQQh\nhIioYHqyuoA7rMc59SG4A9CqJ7tozfomQyOdEiGEiIhgy+DnKaVuV0p1VEq1cP+FNWUhkGlrCSVF\nkU6GEEJERLBl8Bda///qtU4DXUObnNBSMQ60UwK8EKJhCrYM/lKt9U+1kJ6Qstlj0YV1urGPEEKE\nTbBl8M/WQlpCzuaIRbkkBy+EaJiCLYOfr5S6QCmlwpqaELPFxGJzFWNmDxRCiIYl2AD/F+BDoFAp\nlaOUOqKUqvOtaWwxcTgoIa/IGemkCCFErQt2NMkm4U5IONgdsTiUk+z8YhrHBVufLIQQ0aHSHLxS\n6lKvxyN8nrshXIkKlRiHycHnFMjE20KIhidQEc1tXo+f8XnuqhCnJeRiYuNNgM8viXRShBCi1gUK\n8KqCx/6W6xxHbBxxFJOTLzl4IUTDEyjA6woe+1uucxwJiSRQRE6+DDwmhGh4AtU89lJKrcTk1rtZ\nj7GW63QvVoDYhKbYlCYv92ikkyKEELUuUIDvXSupCJO4RokA5OceiXBKhBC1zlkMD7SEU6bDibdG\nOjURUWmA11pvr62EhIM93rTubLP7W0BGlRSiQSm2hilZ+N8GG+CD7ehUr03Y+WikkyCEqHVWNaF2\n1Ww3zhJ4cwJsXVTzJNWy6A7wuQcjnQIhRKS4rB7svgG+uADmTYeivOD2c3QfbP0BPv1LSJNXG6oc\n4JVSzZVSA8KRmJA71vTT+kX3j3BChBC1zmX1f/EN8L++Aj8+AT8/HeSO6nyDwQoFFeCVUt8rpZpa\nk3wsB15RSj0e3qSFQKMW7G/ck1xXDCXOGt6mifDK2RPpFIj6QmvzF4g7wDsLYe5dkH/YWrZGmC0O\ncihx97FU/SvwCDbFSdZUfecDb2mthwKnhC9ZoaMdjWhEITkF0pu1ztryPTzeC9b+L9IpEbVhz0r4\n5YWqvWbmZHj9LPN42etwXzM4eqDyYzzu1Qjwl+dhwb/NY3egDrZs3n2hCFXfTmcxFNTOWI3BBvgY\npVQ74E/A7DCmJ+RUbGMSVAG/bjtU9olDW4PLBdRXRblwYEOkUxHYr6/B+rnm8c6lkU1LNCgugA1f\nRzoVlXvpJJg7rWqvWf8VbP/RPE5/3fzP3ul5fvWnJqNQuv2c8vtwWj3aS3PiQf7+XSHOHL5/KTzU\n0VTcbv4utPv2EWyAvx/4Gtistf5VKdUV2Bi+ZIVOcvNmNKKQPzKyPSv3rISnB1U9F1GfvH8ZPHec\np6Ip3I4egDVfVO01Obvhy9tgSRSfh1DL2QOFlfTr+OYeeO9PkLGs9tIUrPwsOLLXs1zdDJa7iCUm\nzrPuwyvgrXPM492/wf7V5V/nDuyHtlTt+O7jhSIDv3MpbLAyNFt/gLfPg80LQrBj/4IK8FrrD7XW\nA7TW11nLW7TWF4QtVSFkT2xJK9sRMnO9hivI2mr+b693sxAGb4v1pXFWMg5PcT5MT4Ifn6z58d6b\nBB9cVnnw8VUubVFyR5V70HyuG74J7X53rzBFWS+NrHibTCvf9epY2LrQ/zZH94evyd8Pj8Cu5f6f\ne2kU/LenZ9lf5mPVJ2XTvX1x2e9ncT6UWL/ligL0y6Nhzefl16e/Zv4ve6P86/OzTIbP3z4r+w1V\n1Wunll/39rmmKWYYBFvJ2lUp9T+l1AGl1H6l1OdWLr7uS0qhOTls+HUeLpdPZYmzyPwQf4jidvLO\nSsbhKbDuan55vubHybRyRVW6na2HAf3dSaYFRmV2/2b+L3kx+P2WFMLqzyrPVb48yvx350D98S5X\nXvyc/21eOw3ePDv4tAVLa1PO/coY/88f9uk36fQzneZHV8Kb4z3Lr4+Defd6lguPeF7n7/VVSq/X\nZzVnmik22vZj+e3cAf7wDlgxs/J97l9n7ibcr1kxE149JfBFoiC78uerKdgimveAD4B2QHvM7E4B\n3mkd0e5YACbYf2btXnfFhnWvVWiNURPoB1svWe/R3xcr9yBsnBfaw2krN1aVnIhvDq4u14nsWgZv\nnQsbvzFtqCvj/sztjuD3P/9++HBKxbnuYHl/hvZY/9u472D9ff5VOQdbF5mA5lZSEPxrAVzVyBkX\n53mOE4oA7yw2xUbuO8+Cw+W3807nZ9eWf76kyPO9/+TPpj5g7x+e7TN+LVs05U9+VtXTH4RgA3wj\nrfXbWusS6+8dID4sKQq1HqaxTxEOVuy0Tp57atkSq5mU9vmiz7sPfnu3lhIYZv5+BG9OgHcv8Nzq\nhoI7WPj70b54ksnFlEtbPRrG+dPrPMVeYFpBVBSM3XcxtirMInZ4h/mff6jy7dw+/rPX8Vww/wHI\nziibK60owLv5nv+nB8ET/YI7Ppi7gOe9hgDxbnb4YCdY+go83hfyKnhPlZ3/ii40xfmeDko1DfBo\nc8fw356ei3FxAez5vWwLHd/juKzPePvP5nv/RB9TcQyez9S7fgDMZ1uZSAR4pVQLq+37HKXUNKVU\nqlKqs1LqDuCrsKQoDHSTdjS35bF932GrLawV4N1fFN+czI+Pw+fXm5M8PQl+f796B967yrw+kq1Z\nZk4uv85dAVWUW/39LnmpbBGAO7A4i0wO98vbPc/tXWlyMeXSdmH1j1/bin16PX58jSlKyM0sv637\nIldZgN+/Fv7bC47sM8vuTMfu3+BgEO0X/vjA83jPClj0GHx4paetNwQO8O7iu13LICMdsrZBTkbZ\nbUqK4KenzG/FXTR0aIsnh+q25Xv44HLPcmE2fHW72d+OXyo4fiUBvqLnivM8GTNnEXzzz6pVUnoH\n7h2LPU1z3RenX181dRwvnujZzrdJo6sYNs2H18+A+1tA7gHYv8ZKU6H/9AcqugxTgA+UxViGKSh1\n1x9799XVwF3hSFSoqbimTLR9D8tHmm5ap9xnnnD/aCtqD7tvlfn/29swsBrBaNVH5v+62dDqNvOj\nSGwDia2rvi+3PSshYykcd02Q26+o+Dl3gK/KbfnLo03g2LnELA//q7UPdxFNsafI66zHKt6P1iag\nBHJgA+z8BebeDSfdBivegwvfNj+qdgMhPin4tNeEb4Dfa42c/WhXuOob6OSVk3XfrldWRPPzs3Bk\nD2z8Gjqd4KkU/Okp8zfdp0y2sqIvdzDJ8GlmGqiIqMTKmb4ytuJtlr4E3/6f+QO4fSM8fWz57dwt\nWPyJT/L/HSuppKORswhi/FygvNO6+jPz2wy6RyrwWHfPY++L1EaraelO62J01KtI5YPLyu6jINtc\nEH3lHzZ3AOC/uKqy31kkcvBa6y5a667W/zJ/QM/KXquU6qiUWqCUWqOUWq2UujmkKa8K3woMd6VN\naYCroClhkVVGH5sYmnS8eGL5H9PBTZ4vRWWcJfDBFHMr+OXfyn5ZnMXVK24p8honX2tPMUFFtDY5\nTHdw9+a+C/K9nfWulHKWeNoq+8vRHNxgcsTF+Z6c1qsnwxc3QtERmH+faSXyzT0m9/xRELNG5uwp\nXymZd8jkyqpyZ+V7jrw//xmnwUdXe5bdn0FlOXh3cItJgGeHBD7+ugo6ga353Bzfn+VvwsOp5nHm\nZnikK2R5VXQGE1QKfeZS8Ffc4nKCsle8j4PrTXGNr1nWlM8lheWbdQZT/PLb2+XXhbrYz99v8/He\n8P1/yq9/uLPnwlCcb1oEefvtnYqP46/sPwSqUEgISikFjAUuBs4G2lSyeQnwN631cqVUE2CZUupb\nrfWaaqe2uioq1wxU3umueIlrUs0DezWcdd8CenfOKDxqftz9LoAuI03zshE3Q3K38rs6tAXWfOZZ\nLikEh1UN8uKJcGBd2VxfRRctb+4LnFKmZyDA1d9Cx+PLb6t1+VYQAF/9HTbNo7RFjPcPszi/bKXU\nosfg+wdhyv+g/eDy+9r4Dbw7EWx2U6QzPRsK/fT4cwdm7wq+ijzey/z3/mwe6eJ5fPkX0HWU/9fm\nHza366fcC8W+xVk+ubFVH8FEqxmeO7dfWQ7aHTh8y2or8uEV/vfhXSziT36WKSZ08y7aeX4o/F8l\nv4HCI/DDQ2XX+bswZ26u/Ps2u4Khevevhj8+go+ti+OFXvVeH1wOLarRUC/Y4QeCkbkZnvHzPQ2m\npdjb58LAi8uu89dCxy2SlaxKqWFKqaeB7cDnwEKgV2Wv0Vrv0Vovtx4fAdYCHWqW3Gqa9GbgbeZM\nM+Wi3uXx7rI394nJ2W3aNi95ySwX5ZpyT99xVLQ2ZXl5XqNZHt1X/pjuXPemefC/m02Oa8Y4/+nz\n/VJ5tzc/YAW6tbNhh5/cdUXyrPJj71zPAZ+guXWhqXT65Xl4amD5fSx9uWwO2bsowfeO4PsHzf+c\n3f4/D4Ddyz3l9RV96bOt/eZklL9zeWmkyS3mZ8EbfpoC+t4mu1uU+G7z9T9Mjuzwdv93ChWlHzwX\nTpvDBJwn+plzvHWh2XfhEU8Ofv79Fe8HzOe53E9OFWCWn/qVQHyLevz1W9i51KRz9afln/NXb7Oi\nkpxpIB973fkcWOt5vG2R+T1UVXZG4G2C5e+uoyp+f6/sclElM8tVVBFdQ5Xm4JVS/wEmATswzSLv\nA9K11lX65JVSqcCxQLnoo5SaCkwF6NSpU1V2G7xeZ0JcU/+5QbclL8CGOXDdz551c+80/4/sNhVR\n3sUrx0+FVR/D6k/A0QjOftwE6dHTTNnml3/z2rn2FDnEJHhWuytkvC8qufv9p8/3h1WYA4mtyq57\n/xLzv+OwsuufHAC3rITlb5WtwPvfTea/952Md7B+ZyJs+tY8Tgry3Hjn4DM3+d9mzp3B3ZK6ixcq\nM3canO3VzHXP7+avUbIJEt4ObDC9PL3Z48xQCd3GeHLTK96Dxc8GPrav6UkwcDIkpXjWZW4yd23v\nWP0CT/6/skE9s4IK1fwsE2gXP2d6PPpTnW7uK2eVXfbXfO+1U+HMx/wH/yI/6yprl18VoWjVtTGE\nnctC3cN6XSWjvORWMq5ODQTKwV8D7ANeAN7WWmdSxd4pSqlE4GPgFmvAsjK01i9rrdO01mmtWrUq\nv4NQ6T0h8DZZ2+CHh/0/51t27izyag5nN02mfp9pbkd9K1jm3+8Jog6v1qXu7byv7P7KMn95EV7z\naWZYmFNxxdtOn1YLh7ebi8gXNwaukPrxCTi803SGcQd38OSaA/G+ELk7/PgKZXnjsjc97be9A5Lv\nrbrLBd89UD7HvivdtOb57l/mlvz9S8s2h6yq32fCQqvj3JE95e+8Vn5Q/jX+PJxqLkYVBffq8q3Y\n9m7m6O2r2/0XKfgL+of83AVVR6Ae1Sl+ig59eXeKClaTdlV/jbdg0hVIZXeENRAowLcD/gWMBzYr\npd4GEpRSQZXdK6UcmOD+rtb6k0Dbh9WEIGvaf3oquO2Kck1AAHMr677VzNoOm+eX3949/kR+lmln\nf2Rf2cGR3GxWgC/IMdtsmue5k/C28DF4IBk+vyG49M66OPA2bk/281+RGoz3JnkeH95Z8Xahop1m\ndMHHesCDXjnnz68vu932n2Ctn7Fyfn3V/N/8nSlvXfs/+OPD0KRt/VflhyvwLQKrqsTKqr1CzN/3\n2F/bf3drs0CGXV/584E6PoXqTsFX0/Zw/qvmrqUiNy6H7n6GGQBTf1blY3Ywf2AuMEfCM1x2oFY0\nTq31XK31FKAb8BnwE7BLKfVeZa+1KmRfA9ZqrSM/drzNXvkJrKr8LM9tfGGOpzz70Gb/PR29y5N/\nfBz+e4z/yidnkQk6zw0120WIS94AACAASURBVLxTwZA/7ts9fy0J/HFfYGpTmCqOylk/J/AtbqCu\n+cEGqapyN6cMlUs/DrxNs86Q0Dy0x3Xz7gDY1s+8Pz3PrPi1cU1rduxBVcikuN0R5N3FgEnQxquT\nl6Ox5/Gtq03DB3eTYChb1Nqso9e2a0yF/iUfVX68kgJoZ3V+6j2+ehXKQQh6BHutdaHW+mOt9USg\nBxAoYowALgPGKqVWWH+VnP1acPyfzZX69PJNnFa4/LRccbP7aengr3a9Mv4qrCry5d9MuX9VKTv0\nPb/qrwuXiuoTvKVdHXibQEJZ7hpqB9aHdn8xCXB1EMNM2AK0gW/jM8tZ/z8F1/PWXW806Q24dhHE\n+fRDOOGmil+bYLXU6pAGt1XjTuaU6RU/52jkf32jFmWDfL+J5bfxndCjwxC4yOtC1ijZ/PfuONbO\nq8FBqtWLtU0/SLJy5T28cvtDroABF5nHrfvA2Hvg+l/g/JfMheDMR4O7cFdDtaYo0VrnaK3fCrDN\nj1prZY1COcj6i3zv1wGTzJV4Stl2xVcU3VFm+ffRMzwL9wRRPtamH5z6gLkah8PxU6H/pMq3GXYd\nnPYv6DIKrv0JYqvbvDNEKiqD95ZyXPjTEU6Bgm1Flai+Bl8O03ZCYtuKt2nTD1p2h9jG5Z/rc64n\nI+JyBr64tvUZkmDcg/6brnq7xesux50b981VN62kPLv9sXDO83DRe9Ckbdlca6sKGuWdaN3ltu7j\nKb70R9nhsgoyUY1aeB67m7KWSXN7879xS/O/73ll74BirHqzFlbz2kbJcLFXZXVyN5j4Ooz3Kd5t\nbm0//ik451m44DXTiGPk301nx7gmZS8EYVD/5qAKlS4j4ep5FN6ylg3XZeCKb84unVz69J/nWpV0\njVuBUrju3otu3gXG3ON/fx2GwIib4MJ3rLbVo836UHWSUnY44xHzJel2sv9tOh5vchBTvjA/YHf7\n7jNCMFpmoG7vAKcGaPLnT2VNx2rblAo6E1WkWWdo2cP/c8EUB7pzfh2Hwdj/g/im0MenMcAN6Z7i\ngJNuM/8dfoaBGngRXGNdbPy1dPHVuk/Z5fiksi2gWvUq2y795pVliyLcuXHfStfmqTDhGbjkY7jB\np/NSq55w7CXQpI3pezHBKuJs0w/+usQc42KrEtpdRNJxmCnyuH5x5e+nRRfoNhYu/hDOe8ncSZzt\nVWnbqjeceFv51zkaw7nWaKrJ3UwRy/AbPD2kO53gGUaiaXuY+gNcv6R8EVi/8yElrey6636GO62+\nI3YH9J/o2VctqVJHp6jT8TjigGOApyYfyymvP0pzjtLZto/9NGdK0Z1s4Rjav7SYhFg7y7Me5veT\nTkV1GWl6XfY4zZSTQ9mOHl1HQdv+ZnTAs580lYD9JnqGewWTM/EdxXLMP0yOaP9a0+EHzJd/3yrT\nbbtRC/Ml6XU2rP/S0z67UbJpYtb5xLL7O/8V8wNs0sY0GVz7hflh76+gr9m9h02Z/hc3muVzXzC9\nRvMy4a5dpmVFRW2TL/vU/BgLj5jc5IsjzPoWXctWjl3+uenWntDcXDx7TzD7BXPbu+d3//uvieOn\nmvb63s563PzYvOtBmncxuSt3K5gTbzP1JRVxOcsXDfQ4Hdr0gSFXmpmVvFsieYtrCn96yzS1Pe4a\nzw//9P+Y75X7/Lfs4Sk6aWy1MvNXHHHMOE9LDH8tXfqca76z7nPfvDP886Apnsg9YAKQO8A3bm0C\nLpjMxOb5ZnswAbE413OBaONzoQBzN+I29QdwJJjvsG9QdHcEczdPbd7Z/E3PNk2Ol71R9qJS+l7O\nMRX4e/8wxUT7VnsulsdU0Kv3r37Gw7ljqzkPdq8w6C5iaZ5qmt/2Obfsa9p7DRo2+HLT9LgisY2A\nCoqOaonSQY5DopQ6AUjF66IQqJimqtLS0nR6up8xHmrJ7sP5zF65m1lLd3LfOX257DX/U8jdN6Ev\nFx3fEZcLEnK2wuxbTK4hKUA/rke6mmB5ycdmlMuti0zlX4/TYPKssregh3eYytqz/mvaQo+4uXyP\n2rxD5kcSTE9bZ7Fp+ZPQzDTpTEoxzeA+u848f+G70NuqiHT3fJyebYLzzl89Y/Gsn2uaFQ69ztNO\n+J8Hy/fadO9j6vdm4LEhU0wLlcnvmyEHjrvG8+PNSDcXoFUfmx/tlXPNOODehv3VtDRY7W6MpfAM\nk6TNeDDuLvuXfOQJkO734XKagaHAFKNNfMM0GfQeJuAfe00Lmi9uNLnmu3fD/c2hUUu4Y7P5vH95\n3nMBuPAds69n0kxRTOcRcMWXnmDtcpnXu3U/xdzuD/2LuRj6G2sFTNB19yyenm2O992/TGWfu439\ndJ+y7+nZptnsA8nmrvGkv5n/c/5uPqPph815f/0Ms/1NKzxFDm7PDDHt9m9I99yZOItNk9N4q0hm\n3xqzjftOw+WCfX/At/ea8znhGf/vyZ+jB8zYMGc8Yj4Tb0W5ZjCw7j7Ng0vLy2uQE9651DQ/Hfn3\n6u+jDlFKLdNap/l9LpgAbzWP7AasANxZVa21rqRGpeoiHeB9fb16L395u+Kpz2LtNlbffzoA6duy\nOFpYwkk9WhLvqKCs8Mg+07u1Td9wJLd6/P1gFj8HyT0qzg1lbTc5rcIjJki27V9+m7WzzYWnomEA\n/FnzuemiPm2nCSw5GSa4JDQ3t/dgZvjJWGqGdVjzGdy5zZMznJ4ESR3h1lWmt+zvM837cAejjHRo\n1skz2FvWtrK9c6dnm+Mufs5cgOISzdj59lhPgANzsfC+GO9ZacYIGjUNxviMv1dSZN7Thjnmbi7t\nyuA+ix2/mOIrd4ArPFL2Qr7hazNE8D5rALvbreEblr5iLjRt+phz+/6lkHYVdD/Zs5+9q6Dz8PLH\nXPWxuSu8e4+V+6wFJUUmc1DLRRfRJBQBfi3QRweb3a+muhbgAfKKSvgwPYN7v/Azx6MfFw/txH/O\nMwHP5dLYbPLFDYviAjOIlXdrhqP7TXFAsGMH5R0y49IMudLUH8TXoBnfnpXmIuRvbJk1X5gih8s+\nMRWNobRrucnV12SEUlGvhSLAfwjcpLUOT2t8S10M8G5HCoppEu/ghw0HmLlkB3NXVz5Dy6uXp3HL\n+ys4WljCbaceQ592TdmZlcfk4zsx7smF3HdOP0YdE8aeuyI4WdtN5VlVZl8Sog4JRYBfAAwClgKl\nA0ZorYPo/x+8uhzgfblcmt8zDpORlc+cVXv46o8AU3JZkhvHkplrKrOGdG7OGf3acvnwVGwKvl2z\nj3H92qLkdlUIEaRQBHi/Bala65AOlFGfAryvi15ezC9bqj8i3KQhKXy4LINHJw5gUpqflgNCCOFH\njQN8banPAT4rt4il2w7RoVkCZz/jGaQpMS6Go4VVmIgaGNSxGXeM60mrxDhaJsbRvHEsq3Zlcyi3\niIEdm7E3u4CebSPciUkIUSeEIgc/DHgG6A3EAnYgV2tdw8ElyqrPAd7Xodwi/tiVzQndkjn3uZ+4\ndFhnJh/fiREPfceuwzWflOD8wR04/9gU9h8p4LxjO5CTX0KjODsOe8PtuyZEQxSKAJ8OXAR8CKQB\nlwPHaK1DOidrNAX4ijy3YBOPfr2e60Z3Y9WubLq2bMybi01vt29uHclpTyykZ5smrN8XRG9Ey2XD\nOvP2L2YfN4zpzqKNB3j+0iH8uvUQzRo5uOL1X1l698m0blq2B2Tm0ULyipx0bBHZzhhCiOoLSYDX\nWqcppVZqrQdY637TWoe0zVdDCPBaa7LyimnR2NPJZfbK3TRvFMuI7i3JL3IS77DhtCpxh3RuwfQv\nVvPGz9tCcvzp4/uQlVfMTSf3oO+9cykodrHtobPYk53Pf75ax6COzejQLIFx/dqWpkUqfYWou0IR\n4BcCpwCvAnuBPcAVWms/c7hVX0MI8NX1R0Y2458tPwHDP87szb+/WuvnFZU7tlMzftthJt4YP7A9\nh/OKWLTRM8Xg9aO78fz3m7nrjF6c0a8dSY0cJCU4OFpYQmJc8CNc5BQU0zRemiAKES6hCPCdMTM7\nxQK3AknA81rrCuZkqx4J8JVbtj2LGJtiQEoS+3IKaZtkilwOHi2kqMTFLe+vYOnWQ8TabRQ5XSE9\ndsvEOF66bDAXvLCY5y4ezCl9WhMXYyc7r5htmbnMXrmb60d3p7l1Z7JkSyZ3ffoHWw7kMvvGE+nX\nISnAEQytNXNW7eXUPm2kPkGIIISkFY1SKgHopLUO8QDXHhLgayavqIQDRwrpnNyYz1fs4uZZKyrd\nPjW5Edsy86p9vBO6JfPz5swy607p3Zr56/aXmdt66siu3H1m76D2uXDDAS6fsZTrR3fjjnGVzusu\nhKDyAB/s1HvjgccwOfguSqlBwP2h7ugkaqZRbAydk80pPWdQB5IbxzF/3T7+fFJX5q/dxz8/X817\n1wxlUKdmJDjs7MspZNiD8zl3UHsGdWxGyyZx3PCeGcP95pN7kJwYy1PzNpZ2zPLlG9wB5q0tPw75\nywu38PLCLQzvmkzrpnEcl9qCYV1b0DIxjmaNYtmemcvyHVmcO6gDeUWmSemGfZ5hhA8cKeRQblFp\n09Cdh/Lo2KIRh/OK2J6Zx8COzWr2wQkRpYItolkGjAW+d1esKqX+0Fr7GWWq+iQHX/u2Z+bSLimB\n2BhTHHK0sITGsfYyFavFThdbDuTi0poznlpEbIyNE7u35Lt1QczYFMDonq34fr1nur2hXVqwZOsh\nmsTHcPeZvfllSyafrzCzW629fxyLtxzkqjfSefXyNJ5ZsInfdx5m3QPjsClFsdNFVl4RKc3Ltwpa\nviOLX7ce4i+jKpm5S4h6KBRl8L9orYd5t5zxblETKhLg676DRwuJd9g5UlDM8Ae/4/rR3ZiU1pFD\nuUWkNE/gqjd+pUfrRD5bUY0pB0Nk20NnAaY8f/bKPby7ZHtpL+PN/zmTohIXNhvExZgRIfOKSmgU\n27CnRhD1VygC/GvAfGAacAFwE+DQWl8byoRKgK9fdh7KI6V5gt9mlLsO59OuaTwa2LT/KN1aNWbx\nlkzu+Ggl947vy7h+bVm69RAvL9xM66bxxMfYmfGTZ+7MK05IDVnTUG9/GdWVl37YQmpyI77/+xju\n/98aZvy0lf9OGsgFQ1JCfjwhwi0UAb4R8A/gNMwMC18DD2itC0KZUAnw4u5P/+C9JTvY/J8zue2D\nFdhtih83HmT/kcLAL64i74HfAK4+sQt/GdmVR75ez53jelHsdHHCQ9/x2pQ02jdLwGFXnPL4Qt6f\nOoyhXc30jlm5ReQUFNM52Uwxdyi3iMS4mNIiLyHCTcaiEfWGy6XJL3bSuIK29te+vazMUM2zpg7D\nblOs2Z3Dpv1HS3v0pnVuTvr2rBql5brR3Xjh+81+n7tyRCoXDE7hz2+lsye7gBcvHcLpfdvQ5a6v\nGNe3LaN6tqJt03jG9JJx2kV4VTvAK6W+qGzHDXm4YBEZJU4XJS5NrN2GBuxeE6porXn7l+0cl9qC\n3u2asmDdfjYfOMqVI7qwYN1+GsXaufjVJWFL24wr0rjqjbLf30V3jKFZIwezlu4kNsbGoo0HObZT\nMy4d1pmkBOkAJmquJgH+ALATmAkswRTPlJLhgkV9s/NQHp8s30W31o3JPFrE8G7J/LjxIGN7teay\nGUvYeSifJy8cxC3vmz4EsTE2ikpMp7GOLRK4aWwP/vPVWrLyimuUjnF925KU4OD99J3YbYqRPVri\n1DCmZyu6t05k9+F8jkttQddWiTV+z5X5ceNBBnduJpXM9VhNArwdOBWYDAwAvgRmaq2Dm7+uiiTA\ni0jal1PAFyt2c81JXVBKkV/kJCHWzqHcIl74fhO3n96TuBizPPiBb8OenrgYG89MPpYP0jPo064J\nrZvGM3FICiUuXeFwEYs2HuDD9AweOLcfMTbFzKU7uHJElzJ3Om47D+Vx0iMLOGdQe566KMRTCYpa\nE6qerHGYQP8ocJ/W+tnQJdGQAC/qiw37jvDI3HXMW7ufYzuZMfq7t07EYbexKyuffh2S+Hh5RtiO\nP318HxJi7cxeuYdFGw/y1U0n0ad9U1KnfVm6TfNGDrLyivnHmb3588iu5faxZncOZz69iGPaJPLN\nrVWYHF3UKTXqyWoF9rMwwT0VeBr4NJQJFKK+OaZNE168dAjvLd3Bhcd1LG1T71bidHHvhD58uXIP\ne7ILOKt/O7q1asxpTy6kUayd1684nuP+Pc/vvicMbM8Xv1fej2D6/9aUWT7z6UWc2b9tmXXuYqR/\nf7WWjfuPEO+w89bi7dw7vg9XjuhCrtVr2J3HO1pYwgXP/0z/lCT6d0giNsbGuYM68OCctYzu2Yqx\nvdoE/fmIuiFQEc1bQD/gK2CW1npVOBMjOXgR7UqcLmxKYbMp1u3NIbewhMN5xVz9ZjoDOzbj/gl9\n6dWuCf/32WpW7DzM1JFdWb07hxk/bSWleQIZWeUni7lzXC+emLehtK4gGP06NCXBYefXbVk0iY/h\nsUkDeXjOOrYczC2z3R3jevLIXDP81A1junPbqcdw9jM/MmFQezq1aMSZ/dsBsCc7n993ZjOuX9ty\nxxLhVZMyeBfgPuPeGypAy4xOQtSu7Pxinpm/kXbNEihxupg6sitKKeav3cfVb6bzyMQBfJSewdJt\n1Z8fuDLDuyazeEtmmeUWjWP5du0+ikpcTB/fhxHdW6KU4pTHf2DOzSfRu11wYUJrjUvjt77gcF4R\nP2/OLL2gCA9pBy9EA5B5tJDkxDhWZhzmrcXbGXVMK/72we+0ahJXbprI2TeeyC9bMvnXl1WfSyAQ\nu03RONZOTkEJ15zYhdtP70m8w47WurTXs9Yap0uzN6eAJVsOccGQlNLZzub/bRQPz1nH4xcOKq1M\nvnzGUhZuOMDiu8ay+3ABQzo3Lz2ey6U5UljSYJudSoAXQgBm4Lis3KLS6RsLip38vvMwP2w4wMqM\nbJITY0sHdws1m4JTerfhmzX7yj13w5juPLvATC8xonsyP23K5P/O7sPFQzsR77Az8pEF7DiUx+Tj\nOzJz6U7euup4dmblcd6xHXjj5208Mnc96fecQsvEuLCkvS6TAC+ECIrLpSkscfG/lbu546OVXHRc\nR0b3bE1OQTHDuyZz3vM/cWqftsxcugOAS4d1YntmXpnZwNyGdG7Oshr2JrYp+PqWkUyZsZTd2eVH\nRnG3FAJ495qhjOjekj3Z+SzaeJD2SQl0bdWY9s0S2Hkoj4ysfIZ3S+ZIQTH3fr6aYV2TmZSWglKK\nc579kcuGpzKxHo5HJAFeCBFSh/OKSEpwlBa57DyUx5Vv/Mq4vm15dsEmJg1J4eELBnDGU4uqNIF8\nTVw2rDMrMw7ze0Z26bo2TeN47uLBTHxxMQCb/n0GLy/aUlpx/M7VQ0lLbU6vf84FYM39p7Nu7xE+\nTN+J1nDtqG6ktmxc5bS4XCau2vzUJ4SaBHghRK3JyMqjXVICdpuixOli5q87+ednq+jYIoHPrh+B\nBmYt3UGTeAf3fhGWPpOVGpiSVOYi8OD5/bnrkz8AGNurdZl5Dvq2b0pBsZOOLRpx+2k9sSlF04QY\nbnv/dzo0T+CJCwf5PcZtH6zgs992seXBs0rXPfHtBk7p3Yb+KcFNXxksCfBCiIjakZlH04QYmjWK\nLV3ndGkemL2G/UcKOLVPGzq1aMwFL/zMZcM6k1fkZPXubNbt9eT+bxzbnWe+21RaDu8tHPMQB+Om\nk3swe+VuthzIpXkjBz/cMQatYeB93wDmjiHGboa7OOaeOYCZryC/yMkXv+9i0pCOfLVqD6f3bVvt\nOYgjEuCVUjOAs4H9Wut+wbxGArwQwu3g0UJ+2nSwdETOpvGO0pY47h67fxnZlfMGd6BX26bc8N5y\nZq/cU/r6xLgYjhaWRCTtbs9fMpixvVpz08zfSiuXF981ljs+WsmijQeZNCSFD5dlcMe4nlw/unu1\njhGpAD8SOAq8JQFeCBFKC9btJzbGxojuLUvXFRQ7yS9ycsXrS2kS76BxnJ2vV+/jkYkD+FNaRzbu\nO8Jf31tOm6bxfiuFAXq0TmTjfjMf8HMXD+av7y2vlfdzxQmpTJ/Qt1qvrfGk29WhtV6olEoN1/6F\nEA2Xv3H24x124h12Pr/hRAC2Hcwlr8hZ2jmqR5smpWPuuFyaT37bxe7D+Tz+7YbSfbxw6RD+8nY6\nCbF2zhrQjnhHGs98t4kbx3bn6jdN5rNv+6ac0C2ZVxZt5cK0jnz6264aFw+Fa4KYsJbBWwF+dmU5\neKXUVGAqQKdOnYZs3749bOkRQghvWw/mMuax73nn6qH0T0kiKcGBy6UpcrqId5QdX2j2yt20TIxj\nWNdksvOLueuTldx1Rm8S42L4afNBbnjvt9Jt1z0wjvRtWfzryzVcMqwzz323ib05lU+At+FfZ1Qr\n0EeskjWYAO9NimiEEPWVu17g3+f145Khncs8l3m0kEfmrqdjiwQe+2YDNgVje7UhNbkRr/5o5iJ2\nTxZfVREpohFCiIbkH2f2pl+HJIZ3Sy73XHJiHA9PHACYwN6rbZPSNvK92jWla6uqt7UPhgR4IYQI\nAX9j7vvTp33ZwdfC2Xs2bFO/K6VmAouBnkqpDKXU1eE6lhBCiPLC2Ypmcrj2LYQQIrCw5eCFEEJE\nlgR4IYSIUhLghRAiSkmAF0KIKCUBXgghopQEeCGEiFIS4IUQIkpJgBdCiCglAV4IIaKUBHghhIhS\nEuCFECJKSYAXQogoJQFeCCGilAR4IYSIUhLghRAiSkmAF0KIKCUBXgghopQEeCGEiFIS4IUQIkpJ\ngBdCiCglAV4IIaKUBHghhIhSEuCFECJKSYAXQogoJQFeCCGilAR4IYSIUhLghRAiSkmAF0KIKCUB\nXgghopQEeCGEiFIS4IUQIkpJgBdCiCglAV4IIaKUBHghhIhSEuCFECJKhTXAK6XGKaXWK6U2KaWm\nhfNYQgghygpbgFdK2YHngDOAPsBkpVSfcB1PCCFEWeHMwR8PbNJab9FaFwGzgHPCeDwhhBBeYsK4\n7w7ATq/lDGCo70ZKqanAVGvxqFJqfTWP1xI4WM3X1lfynhsGec/Rrybvt3NFT4QzwAdFa/0y8HJN\n96OUStdap4UgSfWGvOeGQd5z9AvX+w1nEc0uoKPXcoq1TgghRC0IZ4D/FeihlOqilIoFLgK+COPx\nhBBCeAlbEY3WukQpdQPwNWAHZmitV4freISgmKcekvfcMMh7jn5heb9Kax2O/QohhIgw6ckqhBBR\nSgK8EEJEqXof4KN1OASlVEel1AKl1Bql1Gql1M3W+hZKqW+VUhut/82t9Uop9bT1OaxUSg2O7Duo\nPqWUXSn1m1JqtrXcRSm1xHpv71uV9iil4qzlTdbzqZFMd3UppZoppT5SSq1TSq1VSg2P9vOslLrV\n+l6vUkrNVErFR9t5VkrNUErtV0qt8lpX5fOqlJpibb9RKTWlKmmo1wE+yodDKAH+prXuAwwD/mq9\nt2nAfK11D2C+tQzmM+hh/U0FXqj9JIfMzcBar+WHgSe01t2BLOBqa/3VQJa1/glru/roKWCu1roX\nMBDz3qP2PCulOgA3AWla636YRhgXEX3n+Q1gnM+6Kp1XpVQL4F5MJ9HjgXvdF4WgaK3r7R8wHPja\na/ku4K5IpytM7/Vz4FRgPdDOWtcOWG89fgmY7LV96Xb16Q/TX2I+MBaYDShMD78Y33OOaaE13Hoc\nY22nIv0eqvh+k4CtvumO5vOMp5d7C+u8zQZOj8bzDKQCq6p7XoHJwEte68tsF+ivXufg8T8cQocI\npSVsrFvSY4ElQBut9R7rqb1AG+txtHwWTwJ3AC5rORk4rLUusZa931fpe7aez7a2r0+6AAeA161i\nqVeVUo2J4vOstd4FPAbsAPZgztsyovs8u1X1vNbofNf3AB/1lFKJwMfALVrrHO/ntLmkR007V6XU\n2cB+rfWySKelFsUAg4EXtNbHArl4btuBqDzPzTEDD3YB2gONKV+UEfVq47zW9wAf1cMhKKUcmOD+\nrtb6E2v1PqVUO+v5dsB+a300fBYjgAlKqW2Y0UfHYsqnmyml3J3yvN9X6Xu2nk8CMmszwSGQAWRo\nrZdYyx9hAn40n+dTgK1a6wNa62LgE8y5j+bz7FbV81qj813fA3zUDoeglFLAa8BarfXjXk99Abhr\n0qdgyubd6y+3auOHAdlet4L1gtb6Lq11itY6FXMuv9NaXwIsACZam/m+Z/dnMdHavl7ldLXWe4Gd\nSqme1qqTgTVE8XnGFM0MU0o1sr7n7vcctefZS1XP69fAaUqp5tadz2nWuuBEuhIiBJUYZwIbgM3A\nPyKdnhC+rxMxt28rgRXW35mYssf5wEZgHtDC2l5hWhRtBv7AtFCI+PuowfsfDcy2HncFlgKbgA+B\nOGt9vLW8yXq+a6TTXc33OghIt871Z0DzaD/PwH3AOmAV8DYQF23nGZiJqWMoxtypXV2d8wpcZb33\nTcCVVUmDDFUghBBRqr4X0QghhKiABHghhIhSEuCFECJKSYAXQogoJQFeCCGilAR40aAopZxKqRVe\nfyEbgVQpleo9cqAQkRa2KfuEqKPytdaDIp0IIWqD5OCFAJRS25RSjyil/lBKLVVKdbfWpyqlvrPG\n6J6vlOpkrW+jlPpUKfW79XeCtSu7UuoVa6zzb5RSCRF7U6LBkwAvGpoEnyKaC72ey9Za9weexYxq\nCfAM8KbWegDwLvC0tf5p4Aet9UDM2DHuCeV7AM9prfsCh4ELwvx+hKiQ9GQVDYpS6qjWOtHP+m3A\nWK31FmuQt71a62Sl1EHM+N3F1vo9WuuWSqkDQIrWutBrH6nAt9pM5oBS6k7AobX+V/jfmRDlSQ5e\nCA9dweOqKPR67ETquUQESYAXwuNCr/+Lrcc/Y0a2BLgEWGQ9ng9cB6VzyCbVViKFCJbkLkRDk6CU\nWuG1PFdr7W4q2VwptRKTC59srbsRM9vS3zEzL11prb8ZeFkpdTUmp34dZuRAIeoMKYMXgtIy+DSt\n9cFIp0WIUJEiGiGEZbhWUQAAAC5JREFUiFKSgxdCiCglOXghhIhSEuCFECJKSYAXQogoJQFeCCGi\nlAR4IYSIUv8PaUC2pAUjvUcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your plots might be slightly different, i.e. they should look very similar to the plots below!\n",
    "def plot_history(history):\n",
    "  hist = pd.DataFrame(history.history)\n",
    "  hist['epoch'] = history.epoch\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Mean Abs Error [MPG]')\n",
    "  plt.plot(hist['epoch'], hist['mae'],\n",
    "           label='Train Error')\n",
    "  plt.plot(hist['epoch'], hist['val_mae'],\n",
    "           label = 'Val Error')\n",
    "  plt.ylim([0,5])\n",
    "  plt.legend()\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Mean Square Error [$MPG^2$]')\n",
    "  plt.plot(hist['epoch'], hist['mse'],\n",
    "           label='Train Error')\n",
    "  plt.plot(hist['epoch'], hist['val_mse'],\n",
    "           label = 'Val Error')\n",
    "  plt.ylim([0,20])\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "plot_history(nn_reg1_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "01yJgAxu21MS"
   },
   "source": [
    "> Based on the above plots, it looks like that there is not too much improvement after around 100 epochs. Later when we study Chapter 11, you will learn that there is a technique called `EarlyStopping` that can be used here which stops training if there is not much improvement after a fixed number of epochs. Moreover for now, you should fine-tune the hyperparameters of the network to see if you can see any improvements. Report the results of your hyperparameter tuning in the following cell. This is what's called <b>Grid Search</b> in hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "colab_type": "code",
    "id": "N3cpHERz21MT",
    "outputId": "a853f884-48ef-4fa2-b790-9d935c7ae8ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEKCAYAAAAYd05sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEclQSvAiIhlKCV5EJEP9f0+X7xFQYkyLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npGfTNQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### START CODING HERE ### \n",
    "# Build another regression neural network with the same architecture,\n",
    "# but you later compile it with different hyperparameters\n",
    "nn_reg2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=[len(X_train.keys())]),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "# Fine-tune the optimizer as follows:\n",
    "# First, try a different optimizer - SGD with the learning_rate = 0.001\n",
    "# Then, switch optimizer to Adam,\n",
    "# and use three different values of learning_rate in the order of 10 or 10^(-1) like 0.01, 0.1 and 1.\n",
    "# You may try other values for learning_rate.\n",
    "# Report the results of your hyperparameter tuning in the following cell.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=1)\n",
    "\n",
    "# Compile nn_reg2 with loss='mae', optimizer=optimizer and metrics=['mae', 'mse']\n",
    "nn_reg2.compile(loss='mae', optimizer=optimizer, metrics=['mae', 'mse'])\n",
    "\n",
    "# Fit nn_reg2 on X_train, y_train, epochs=EPOCHS, validation_split=0.2, verbose=0\n",
    "nn_reg2_history = nn_reg2.fit(X_train, y_train, epochs=EPOCHS, validation_split=0.2, verbose=0)\n",
    "### END CODING HERE ###\n",
    "\n",
    "plot_history(nn_reg2_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "sBsaJ70121MW",
    "outputId": "c3783ad6-de66-456c-9f14-75c9199c80b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "79/79 - 0s - loss: 1.7407 - mae: 1.7407 - mse: 6.2447\n",
      "Testing set Mean Abs Error:  1.74 MPG\n"
     ]
    }
   ],
   "source": [
    "### START CODING HERE ### \n",
    "# Evaluate nn_reg2 using .evaluate() method on X_test, y_test and verbose=2\n",
    "loss2, mae2, mse2 = nn_reg2.evaluate(X_test, y_test, verbose=2)\n",
    "### END CODING HERE ###\n",
    "\n",
    "print(\"Testing set Mean Abs Error: {:5.2f} MPG\".format(mae2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFnDzLly21Ma"
   },
   "source": [
    "Report the results of your hyperparameter tuning HERE:\n",
    "\n",
    "SGD - Testing Set mae: 1.7407 MPG\n",
    "<br>\n",
    "Adam learning_rate1 - Testing Set mae: 2.1983 MPG\n",
    "<br>\n",
    "Adam learning_rate2 - Testing Set mae: 2.3100 MPG\n",
    "<br>\n",
    "Adam learning_rate3 - Testing Set mae: 5.7882 MPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovFKoSWp21Mc"
   },
   "source": [
    "> You can check the quality of the model predictions by the following plot. Use this plot to answer Part II Q1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "colab_type": "code",
    "id": "eTFjJmvU21Mg",
    "outputId": "717ee8aa-ab4e-4964-bbf3-28f6ac0befe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ8AAAEKCAYAAAAM4tCNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de5gcdZ3v8fd3LiETYhxIxhgHWEJg\nkwMLJJqNSNQVVIKCkEUXcV2f6HJO1suuoLvR4Ho0iCsBznrbZ71EUOPiJSIIIawCAl5BMJBACBDl\nEhZGIAlkCMFJMpfv+aOqJz09fanq6equ6vm8nmee6equ7v4Guj9T9buVuTsiInG1NLoAEckmhYeI\nVEXhISJVUXiISFUUHiJSFYWHiFSlLckXN7OtwAvAIDDg7vPN7GBgDXA4sBU42913JlmHiNRePY48\nTnL3ue4+P9xeDtzi7kcBt4TbIpIxjThtORNYHd5eDSxuQA0iMkaW5AhTM3sM2Ak48HV3X2Vmve7e\nGT5uwM7cdsFzlwJLAQ488MBXzZkzJ7E6RcargUHn0R27eeHJ3+9w9644z020zQN4rbv3mNnLgJvN\n7KH8B93dzaxoern7KmAVwPz58339+vUJlyoyvmzbtYdzvvFb9j2/hwcvesvjcZ+f6GmLu/eEv7cB\nPwYWAM+Y2QyA8Pe2JGsQkdFywfH083v49vsWVPUaiYWHmR1oZi/J3QZOAe4H1gJLwt2WANclVYOI\njFYYHAtmHlzV6yR52jId+HHQrEEb8D13/6mZ/Q74oZmdCzwOnJ1gDSKSp1bBAQmGh7s/Chxf5P5n\ngTcm9b4iUlwtgwM0wlRkXKh1cIDCQ6TpJREcoPAQaWpJBQcoPESaVpLBAQoPkaaUdHCAwkOk6dQj\nOEDhIdJU6hUcoPAQaRr1DA5QeIg0hXoHByg8RDKvEcEBCg+RTGtUcIDCQySzGhkcoPAQyaRGBwco\nPEQyJw3BAQoPkUxJS3CAwkMkM9IUHKDwEMmEtAUHKDxEUi+NwQEKD5FUS2twgMJDJLXSHByg8BBJ\npbQHByg8RFInC8EBCg+RVMlKcIDCQyQ1shQcoPAQSYWsBQcoPEQaLovBAQoPkYbKanCAwkOkYbIc\nHKDwEGmIrAcHKDxE6q4ZggOgrdEFiIwnUYPj2g09XHbjFv7Y28crOjtYtmg2i+d117na8hQeInUS\nJzguuGYTff2DAPT09nHBNZsAUhUgOm0RqYM4pyqX3bhlODhy+voHuezGLUmXGYvCQyRhcds4/tjb\nF+v+RlF4iCSomsbRV3R2xLq/URIPDzNrNbMNZrYu3J5pZnea2cNmtsbMJiRdg0gjVNursmzRbDra\nW0fc19HeyrJFs5Mos2r1OPI4D3gwb/sS4AvufiSwEzi3DjWI1NVYumMXz+vm4rOOpbuzAwO6Ozu4\n+KxjU9VYCgn3tpjZIcBpwL8BHzUzA04G/jbcZTWwAvhqknWI1FMtxnEsntedurAolPSRxxeBjwFD\n4fZUoNfdB8LtJ4Gi/4XMbKmZrTez9du3b0+4TJHaaJYBYFEkFh5mdjqwzd3vrub57r7K3ee7+/yu\nrq4aVydSe+MpOCDZ05aFwBlm9lZgIjAF+BLQaWZt4dHHIUBPgjWI1MV4Cw5I8MjD3S9w90Pc/XDg\nHOBWd383cBvwjnC3JcB1SdUgUg/jMTigMeM8Pk7QePowQRvIFQ2oQaQmxmtwQJ3mtrj7z4Gfh7cf\nBRbU431FkjSegwM0wlSkKuM9OEDhIRKbgiOg8BCJQcGxn8JDJCIFx0gKD5EIFByjKTxEKlBwFKfw\nEClDwVGa1jAVKaFYcGRhYeJ6UXiIFFEqOLKwMHG96LRFpECpU5WsLExcLzrykHEjyilHuTaOrCxM\nXC8KDxkXopxyrL59Kxete4CBIWfa5AmjQuEVnR30FAmKtC1MXC86bZFxodIpx+rbt7Ji7WYGhhyA\nHbv3ccE1m7h2w/7lZrKyMHG9KDxkXCh3yrFt1x4uWvcAXvBYYXtGVhYmrhedtsi4UOqUY/qUiZzz\njd8OH3EUKgydLCxMXC868pBxodgpx8S2Fobcefr5PUybXPzyQZ2T2utRXiYpPGRcKDzlePmUiUzp\naGf33gG+/b4FfPK0o2lvtVHP271nYES7h+xXNjzMbFeFnxfM7Pf1KlZkLBbP6+Y3y0/mzk+8kUkH\ntA4Hx4KZB7N4XjcHThh9Ft8/5ON2HEclldo8HnH3eeV2MLMNNaxHJFHlxnE839df9DnjdRxHJZVO\nW94e4TWi7CPScJUmuWXlAtNpUTY8wsWKy4qyj0ijRZkdq3Ec8VRq8zjXzJblbffktXW8P/nyRMYu\nzrT6ie37vxKdHe3jehxHJZXaPN4PnJq3vc3du81sInAj8LXEKhOpQuH8laWvP4LVd2ytGByFw9cB\n9g4MFd1XApXaPMzdn83bvgrA3fcAOhGUVMkFQE9vH04wf2XF2s08ubOv4hGHZszGVyk8OvM33P1z\nAGbWAkxLqiiRahQLAAemTGyruAKYZszGVyk8bjKzzxa5/zPATQnUI1K1Ul/0Z3fvq/hc9bTEVyk8\nlgFHmtnDZnZ1+PMwcCTwL8mXJxLdWAKgXE/LtRt6WLjyVmYuv4GFK2/ViNNQ2QZTd38ROMfMjgCO\nCe9+wN0fSbwyGbeqXSd06euPYMXazSNmx0btas29fuH7Alp6sISy4WFmLwM+QXCksQm42N131aMw\nGZ/irhOaC5qe3j7aWozWFqNzUjvP7t4Xe4HiYjNmF668tWRDqsKjvO8AdwP/AZwOfBl4b8I1SROJ\nexRRrtej8HmFQTMw5ExobeGTpx1dsy+2GlJLq9TmMcPd/9Xdb3T3fwKOq0dR0hyKdZ0Wrs5VKM6X\ntVjQ7Bscqmn3qhpSS6s4Jd/MDjKzg83sYKC1YFukpGrGTsT5shZb3Adqe1SgIeulVQqPlxKctuR+\npgD3hLfXJ1uaZF01h/xRv6zbdu2hrWX0+htQ26MCLT1YWqXelsPrVIc0oWpWGy/V65H/Zc3NVWlp\nMSaYsW9w/zDyJI4KtPRgcZV6W15Z7nF3v6fMcycCvwQOCN/nR+7+aTObCfwAmEpwBPMed688ikcy\n56Q5XVz52/8pen855b6s+ZPcrjz31fyxt0+Xf2yQSr0t64H7gR3hdv5xogMnl3nuXuBkd99tZu3A\nr83sJ8BHgS+4+w/M7GvAucBXq6peUu22h7ZHuj9qj0yp2bEKi8aoFB4fBd4B9BEcLfzY3XdHeWF3\ndyC3b3v4kwucvw3vXw2sQOHRlKK0eRQb17Hsqnu58PrN9P6pfzhMTpw1VVerT5lKbR5fBL4YjjA9\nB7jFzB4HPufuGyu9uJm1EpyaHAn8J/AI0OvuA+EuTwJF/2yY2VJgKcBhhx0W7V8jqRKlzaNYj0z/\nkLPzT8GSgD29fSy/+r4RixUrONIh0urp4Wph1xFMhlsA/HnE5w26+1zgkPB5c6IW5u6r3H2+u8/v\n6ip/jizpFKXnJEq36p6BIba/sFfBkTKVGkxzRxxnAk8QnLp8zt1jdaS7e6+Z3Qa8Bug0s7bw6OMQ\nQLOMmlSUnpNSRyeFHBQcKVOpzeNh4D6Co45dwGHAB8yCdlN3/3ypJ5pZF9AfBkcH8GbgEuA2gnaU\nHwBLwteWcWL948+NCJOT5nSx5q4n6C9xxbacgxK4+FK1E/AkUCk8PgPDkxQnx3ztGcDqsN2jBfih\nu68zsweAH4TrhGwAroj5upIRxRpD87tue3r7WHPXE0RZ7G9PQbtIErVptmw8lRpMV1T7wu5+HzDq\nmi9h+8mCal9XKkvLX9RijaGFKh1x5PT113Y90TgT8KS4SqunL630AlH2kfqpZjJaUtI881SzZceu\n0mnLcjPbUeZxA84DVtWuJBmLNP1FjdoYGkWt2zyqGTovI1UKj18Ab6uwz801qkVqIE1/UZctmj3q\ncgbVaG81Pv22YyrvGEOx2jRbNp5KbR7vq1chUhtp+otarKv2pDldfP/OJxj04m0d3eE+tz20PdE2\nmyjdyFJepSMPyZix/EWN29BabH8Y/YX8zfKRU6Dm/9nBo2qc0NrCpe84rq5fXs2WHRuFR5Op9i9q\nNWuHFpuTgkH/oJd9jcXzunm+r5+L1j3AwJAzbfKEmi4dKPWh8GhC1fxFjdvQWmpOSqFir7Ft1x5W\n37GVCW0tfE9DzjMr0twWMzvPzKZY4Aozu8fMTkm6OKmfuA2tcRpg8/eNc9FpSbdI4QH8fXjJhVOA\ng4D3ACsTq0rqLu5Cv3EaYHP7KjiaS9TTltwiQG8F/svdN1tugos0hbgNrcX2b28xBt3JP3tpMXhx\n7wCHL79heM3RzkntvPPrd5RtZFX7R/pFDY+7zewmYCZwgZm9BCJNSZCMiNvQWqobds3vnmBocH96\nDDn09gVrcwyEqbIjvHZsT28fy350L/j+9hLNMckO8xL97SN2MmsB5gKPhrNkpwLd4fyVxM2fP9/X\nr9di7Wm3cOWtNRtR2t3ZMaqLV5JjZne7+/w4z4l05OHuQ2b2DHC0mamHRoqq5ShWzTFJv0hBYGaX\nAO8EHgByJ7lOsDq6CFDbuSyaY5J+UY8iFgOz3X1vksVIti1bNJtlP7p3eJBYFO2tNqLNAzTHJCui\nhsejBKufKzxkWOHw9JPmdDFQIThaDF7a0T5iZXRQb0sWRQ2PPwEbzewW8gLE3T+cSFXScJXmuVRa\nJSxfi4E7ZYNBYZE9UcNjbfgj48C1G3pGnH4Md6kysos26lT7IR95tTBpDlF7W1ab2QT2X3Jhi7v3\nJ1eWNNKF128e1W7RP+hceP3m4fCI2xuSW9WsMIQku6LObXkD8AeCCzd9Bfi9mb0+wbqkCtdu6GHh\nyluZufwGFq68teqlB3MXXCp3f7W9IbkQkuyLOrfl34FT3P2v3P31wCLgC8mVJXHVe+3SZYtmM7Ft\n5MenvdUifaBKhZNkS9TwaHf3LbkNd/89Qe+LpES5KfVxdXYU/1+bf/+Js6YypaN9uC2ju7ODy95x\nPJ9/51y6OzsqtnGM5chI0iFqg+l6M7scuDLcfjeg8eIpUsu1S08/fsaonpMWgxVnBOuI5mbH7t47\nwJp/eM2o2bG59oy5F940PK+lkOawZF/UI48PEIwu/XD480B4n6RELabOQ3D6s+auJ0o+Hmda/Yoz\njqG9pfQxSLVHRpIOUXtb9gKfD38khWq1GvhlN24puiLYkMOnrrufP+0bHF46sNJRTX63bqlh63/s\n7UvNRaoknrKzas3sh+5+tpltYv9lJ4e5+3FJFpejWbXR1OJLOHP5DaP/R5fQ0d7KxWcdG+k9Ss24\nPWhSO3v6h0aFXtTXldqoZlZtpfCY4e5PmdmfFXvc3R+PWWNVFB5jEyVUcvvEndgWdep84YhUCELi\ngLaWou0impJfX9WER9k2D3d/Krz5QXd/PP8H+GC1hUr9ROnCzd8nrqgNsovndXPxWccO98R0d3Zw\n8VnH8nyJBlVNyU+/qL0tbwY+XnDfW4rcJykTZVX0KEPND2g19haZ9BanQbbYqu6ljnY0JT/9Kl3o\n+gNhe8ccM7sv7+cxYFN9SpSxiNKFG+Wv/BCM6jmpxdT5ZYtm09HeWvPXleRVOvL4HvAT4GJged79\nL7j7c4lVJTUT5fKTURbx6R90DprUzqQJbZHaTqI22uqyj9kVdQ3TE4DN7v5CuD0F+F/ufmfC9QFq\nMB2LUg2V+b0ZxfYpxoDHVp42pveSdKp5g2merwK787Z3h/dJypVqqCy8/OPFZx3Ly6dMLPtaldoh\najlEXtIv8nVbPO8QJVwQWQshZ0SUy0+eOGsqkw5oZdKEVs597Uwu/9VjsQec1XKIvKRf5GUIzezD\n7D/a+CDB0oSSIaXaI4oNOZ/VNTl2O0SU9hVpHlHD4/3Al4FPEow0vQVYWu4JZnYo8B1gevicVe7+\nJTM7GFgDHA5sBc52953VFC/RFVs28IJrNvF8Xz+r79jKkzv7mDKxbcSV3OIO0qrVEHnJhkgNplW9\nsNkMYIa73xNeYe5uglXY3ws85+4rzWw5cJC7lx0vogbTsSs1PLytxWhpCVYw3ze4/yKA7a3GgRPa\neL6vP1YPiOapZFPNL/pkZh9z90vN7D8oPrel5ALI4ejUp8LbL5jZg0A3cCbwhnC31cDP0WCzxJVq\ndxgYcqZNah++BGRO/6APDxuPM30+SvuKNIdKpy0Phr/H9GffzA4H5gF3AtPzhr0/TXBaU+w5SwlP\njQ477LCxvL1Quj1i2uQJPFsQHMUUjkoVqTS35frw9+piP1HewMwmA1cD57v7roLXd4oc0YSPrXL3\n+e4+v6urK9I/Rko7aU7XqNW9JrS28MnTjo7coKleE8lX6bTlekp8uQHc/YwKz28nCI7vuvs14d3P\n5M3WnQFsi1mzxHTthh6uvrtn1P/IfYNDnL9mIx3tLbS3WsUrvTlB24naMQQqDxL7fwSLHz8G9AHf\nCH92A4+Ue6KZGXAF8KC75y8itBZYEt5eAlwXv2yJ48LrN5cdPdrXPzQ8/NwI1tgotQJY0gsrS3aU\nPfJw918AmNm/F7TEXm9mldpBFgLvATaZ2cbwvk8AK4Efmtm5wOPA2VVVPo7F6dG4dkNP5NXK3fcP\nPy+3vofaPwSij/M40MyOcPdHAcxsJnBguSe4+68pfaGwN0YvUfKVGq8BxXtCVv7kocivnb8oT67X\npNTKYmr/kKhzWz4C/NzMfm5mvwBuA85PriwpJc78kW279vD0rj1jer9aLawszSdSeLj7T4GjgPMI\nVk+f7e43JlmYFBd1/khuyHmca8QeOKF11H1ab0NKiXq5yUnAMuAf3f1e4DAzOz3RyqSoKEcC+XNV\n3nz0yyK/dnvr6I9DlFm5Mj5FbfP4FsHw8teE2z3AVcC6JIqS0k6a08V3f/s/I9ohjKDtY+HKW1n6\n+iNYfcfW4UluH1mzscQrjVZqPVGNGpViorZ5zHL3S4F+AHf/E6UbQyUhpcZr5LZ7evtYsXYzT+7s\nG54dG6dhU+0YEkfU8NhnZh2En1MzmwXsTawqKSrKQsUOTJnYNnwlt1KBUJj8aseQuKKGx6eBnwKH\nmtl3CabkfyyxqqSoqEcR+XNVSjV4vvuEw9SOIWNSsc0jHCn6EHAWcALBH63z3H1HwrVJgSgLFQO8\ntKOdhStvHR5E9vZXdXPbQ9s1TV5qqmJ4uLub2X+7+7HADXWoSUootthOofYW48V9AyOm0199d4+O\nLKTmop623GNmf5loJVJRfrcpBAv5tLUY0yZPGD79mDyxbdQENy1CLEmI2lX7auDvzGwr8CLBqYvX\n60LXst/ied2cOGvqqDVHc2YuL35wqOHkUmtRw2NRolVIZKtv38pF6x4IVgCbPGFUKGgRYqmXSpeb\nnGhm5xOMLj0V6Cm42LXU0erbt/LptZsZGApOS3bs3seyH907Ynq8hpNLvVQ68lhNMDDsVwQXtj6a\nYH6L1ECxqfUw8tKLJ83p4raHtpfsZekfdC68fvNwY6gu3yj1Unb1dDPbFPayEF7k6S53f2W9istp\nxtXTi12asb3FwKi4olcxW8tcBlKkkpqvnk44HB3A3QeCIR9SC8VGi/YPJXMZDJEkVAqP480st2ix\nAR3hdq63ZUqi1TWxWvZ+dHa01+y1RKKqtAzh6AUepCaijhatpL3FWHHGMTWoSCSeqIPEpMaWLZpN\ne2v1p4G5QWGX/c3xagyVhtCV7htosIqGUQhWN9/wqVNqXI1IPDryaJAVazczVHm3oqKuhi6SJB15\n1FH+uI6x9Ku0qtdLUkDhUSfFxnVUa7DM2ByRetFpS51EWQUsqm7NU5EUUHjUSa3GdWieiqSFwqNO\nqp3V2t5iw9eQ1XKBkiZq86iTKKuA5XR3dmhSm6SewqNOcgGw8icPlb0EZHdnB79ZfnK9yhKpmk5b\n6ujEWVOZdEArkya08k8nH6l1NyTTdORRJ/mXgMwtHTira7LW3ZDMUnjUQbHgAF3GUbJNpy0JKxUc\nIlmnI48aKbakYLlVzkWyTuFRA4VDz3t6+1h+9X1M6Whn994BBYc0pcROW8zsm2a2zczuz7vvYDO7\n2cz+EP4+KKn3r6diQ8/3DAyx/YW9Cg5pWkm2eXyb4HIN+ZYDt7j7UQQXy16e4PvXTamh5w4KDmla\niYWHu/8SeK7g7jMJLudA+HtxUu9fT6WGnmsCmzSzeve2THf3p8LbTwPTS+1oZkvNbL2Zrd++fXt9\nqqvSskWzmdg28j+lBnxJs2tYV60HF4wpuTCFu69y9/nuPr+rq6uOlcV34qypTOkIJq+BJrDJ+FDv\n3pZnzGyGuz9lZjOAbXV+/5rLjePYvXeANf/wGrVxyLhR7yOPtcCS8PYS4Lo6v39NaQCYjGdJdtV+\nH7gDmG1mT5rZucBK4M1m9gfgTeF2Jik4ZLxL7LTF3d9V4qE3JvWe9aLgENHcltgUHCIBhUcMCg6R\n/RQeESk4REZSeESg4BAZTeFRgYJDpDiFRxkKDpHSFB4lKDhEylN4FKHgEKlM4VFAwSESjcIjj4JD\nJDqFR0jBIRKPwgMFh0g1xn14KDhEqjOuw0PBIVK9cRseCg6RsRmX4aHgEBm7cRceCg6R2hhX4aHg\nEKmdcRMeCg6R2hoX4aHgEKm9pg8PBYdIMpo6PBQcIslp2vBQcIgkqynDQ8EhkrymCw8Fh0h9NFV4\nKDhE6qdpwkPBIVJfTREeCg6R+st8eCg4RBoj0+Gh4BBpnMyGh4JDpLEyGR4KDpHGy1x4KDhE0iFT\n4aHgEEmPzISHgkMkXRoSHmZ2qpltMbOHzWx5pf0HBl3BIZIydQ8PM2sF/hN4C3A08C4zO7rccx7d\nsVvBIZIyjTjyWAA87O6Puvs+4AfAmeWe0D/oCg6RlGlrwHt2A0/kbT8JvLpwJzNbCiwNN/e++oip\n99ehtlqYBuxodBExZKneLNUK2ap3dtwnNCI8InH3VcAqADNb7+7zG1xSJFmqFbJVb5ZqhWzVa2br\n4z6nEactPcCheduHhPeJSIY0Ijx+BxxlZjPNbAJwDrC2AXWIyBjU/bTF3QfM7B+BG4FW4JvuvrnC\n01YlX1nNZKlWyFa9WaoVslVv7FrN3ZMoRESaXGZGmIpIuig8RKQqqQ6PuMPY683Mvmlm28zs/rz7\nDjazm83sD+HvgxpZY46ZHWpmt5nZA2a22czOC+9Pa70TzewuM7s3rPfC8P6ZZnZn+JlYEza6p4KZ\ntZrZBjNbF26nudatZrbJzDbmumnjfhZSGx7VDGNvgG8Dpxbctxy4xd2PAm4Jt9NgAPhndz8aOAH4\nUPjfM6317gVOdvfjgbnAqWZ2AnAJ8AV3PxLYCZzbwBoLnQc8mLed5loBTnL3uXljUeJ9Ftw9lT/A\na4Ab87YvAC5odF1F6jwcuD9vewswI7w9A9jS6BpL1H0d8OYs1AtMAu4hGIm8A2gr9hlpcI2HhF+4\nk4F1gKW11rCercC0gvtifRZSe+RB8WHs3Q2qJY7p7v5UePtpYHojiynGzA4H5gF3kuJ6w9OAjcA2\n4GbgEaDX3QfCXdL0mfgi8DFgKNyeSnprBXDgJjO7O5wKAjE/C6kdnt4M3N3NLFV94WY2GbgaON/d\nd5nZ8GNpq9fdB4G5ZtYJ/BiY0+CSijKz04Ft7n63mb2h0fVE9Fp37zGzlwE3m9lD+Q9G+Syk+cgj\nq8PYnzGzGQDh720NrmeYmbUTBMd33f2a8O7U1pvj7r3AbQSH/p1mlvujl5bPxELgDDPbSjBL/GTg\nS6SzVgDcvSf8vY0gmBcQ87OQ5vDI6jD2tcCS8PYSgraFhrPgEOMK4EF3/3zeQ2mttys84sDMOgja\nZx4kCJF3hLulol53v8DdD3H3wwk+p7e6+7tJYa0AZnagmb0kdxs4BbifuJ+FRjfcVGjUeSvwe4Jz\n3X9tdD1F6vs+8BTQT3BOey7Bue4twB+AnwEHN7rOsNbXEpzn3gdsDH/emuJ6jwM2hPXeD3wqvP8I\n4C7gYeAq4IBG11pQ9xuAdWmuNazr3vBnc+67FfezoOHpIlKVNJ+2iEiKKTxEpCoKDxGpisJDRKqi\n8BCRqig8RKQqCo8UM7Op4ZTpjWb2tJn15G3XZHq3mb3EzJ4Nh63n37/OzN5e5nlvMrNra1FDide/\n0sweM7P/HW5/1sw8nJeT2+dfwvvmhttPhtPM7zOzn4ZDr3P/xq+b2SNmdo+ZrTezvw8fmx3+9+xN\n6t/SrBQeKebuz3owZXou8DWC6d1zw599EIwcNbOq/z+6+wsEA4OGL7wVruNwAnDD2P4FY/YRd788\nb3sTwQjOnLczcgo8wOvc/TiCwWW5KeXfAp4BjnL3VxIMjpsG4O5bgExcHiFtFB4ZZGZHhov6fJdg\nhOCh+X85zewcM7s8vD3dzK4J/9reFa6JUej7jP5S3uDue8zsBDO7I1zk5jdmdlSRej5rZufnbT9k\nZoeEt5eE77vRzL5iZi1m1mZm/xUeJdxvZh+O+E+/Bvjr8HX/nGDK+3Ml9v0lcKSZzQaOB1a4+xAE\n8znc/dKI7yklKDyyaw7BkcjRlJ9w9WXgUg8WfDkbuLzIPv8NvDpv5ahzCAIFgr/sr3P3ecBFwGej\nFmhmf0HwZT8xPHpqC1/7VQRrSRzr7n8BfCfiS/YCT5vZHOBdBJPQir2vAacTHKkcA2zMBYfUjqbk\nZ9cj7h7lKl9vAmbnTb0/yMw63L0vd4e77zWzG4CzLFhC7xiCuQ0AncB3zGxWFTW+CfhLYH34/h0E\na7TcGNb0ZYJTo5tivOYaggB6G/BXwAcKHv8VwZoaGwlW8npT/oNm9ingLGCqux+KVE3hkV0v5t0e\nIli5Kmdi3m0DFuTaSMr4PrCM4Av+Y9+/iM2/EayA9RUzOxL4aZHnDjDyKDb3/kZwXZ7/W/gEMzuO\nYInJDxGcJi0t3KeEtQRHQ7e7++789UhCr/NgCn/ufTYTrAnS4u5D7v4Z4DNmtjvi+0kJOm1pAuEh\n+U4zOypsPP3rvId/RvAFBSDXM1HELQRHHO9n/ykLwEvZf1r03hLP3UpwKoKZLWD/Oiw/A842s2nh\nY1PN7DAz6yK4ZtBVwKeAV0b4ZwLg7ruBjwMXR9x/C8Hpy4W5hmUzm8jIsJUqKDyax8cJTgduJ1ge\nIOdDwMKw+/IB4P8Ue7IHq2Ohtt4AAACfSURBVHZdA0wBfp330CXAZWZ2D6W/cFcB0y1YRX4p8Gj4\nmpuAC4Gfmdl9BKcn0wnC5ZcWLDH4LeATcf6h7v49d98Y4ynvA14OPGLBSuE3A/8c5z1lNE3Jl1Qy\nsyuBH7l7YmNJ8t6rDdjh7p1Jv1cz0ZGHpFUvcHFukFhSwq7c9QTjQCQGHXmISFV05CEiVVF4iEhV\nFB4iUhWFh4hU5f8D5qd+0IFdvCcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the best hyperparameters you found for compiling nn_reg2, then run this cell\n",
    "nn_reg2_preds = nn_reg2.predict(X_test).flatten()\n",
    "\n",
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(y_test, nn_reg2_preds)\n",
    "plt.xlabel('True Values [MPG]')\n",
    "plt.ylabel('Predictions [MPG]')\n",
    "lims = [0, 50]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "_ = plt.plot(lims, lims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lc8nyPYX21Ml"
   },
   "source": [
    "## Part II - Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OB-ZcQbM21Mm"
   },
   "source": [
    "<b>ANSWER THE FOLLOWING QUESTIONS HERE:</b>\n",
    "\n",
    "Q1- How do you interprete the plot above in terms of the model performance (Predictions vs True Values)? GIVE COMPLETE ANSWER!\n",
    "\n",
    "**The line in the form** *y = x* **shows the ideal distribution of predicted values that are exactly the same as true values. The closer that the points on the scatter plot come to the line, the better the results are from our model's predictions against the testing set.**\n",
    "\n",
    "Q2 - How do you interprete the first regression model's plots of mse and mae, i.e. `plot_history(nn_reg1_history)`? What is the impact of choosing `mse` vs `mae`? Why the axis scales are different for `mse` and `mae`? GIVE COMPLETE ANSWER!\n",
    "\n",
    "**We do not see a large amount of improvement after ~150 epochs in the `mse` or `mae`. When choosing between the two, `mae` does not take account for positive/negative direction (which can be problematic in some cases of over/undershooting), while picking `mse` will place greater weight (and subsequent penalty) on higher values of error due to the squaring. `mae` just takes the absolute value of all errors, while `mse` squares all raw error values (+/-), and the resulting values are (obviously) quadratically larger than those found in `mae`, which explains the difference in scale.**\n",
    "\n",
    "Q3 - What is the role of `validation_split` hyperparameter in `fit` method? What does it change exactly and why is it used? GIVE COMPLETE ANSWER!\n",
    "\n",
    "**`validation_split` denotes the percentage of data (from 0 to 1) used for validation. Validation, in this context, is used to tune parameters at the end of each epoch, whereas the other fraction of the data is used for training, as is expected.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "at32Tpgj21Mn"
   },
   "source": [
    "## Optional Part III - <font color=green>Extra Credit</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a26fRcwI21Mn"
   },
   "source": [
    "<b>Notice:</b> This part is totally optional and for earning <b><font color=green>extra credit</font></b> in the \"Assignment\" section of your final grade. Attempt this part only if you have enough time and you're inclined to challenge yourself a bit!<br>\n",
    "\n",
    "[Download the video games dataset](https://raw.githubusercontent.com/fereydoonvafaei/UMBC-CMSC-478-Fall-2019/master/Assignment-4/video.csv)<br> \n",
    "\n",
    "You can read about the data [here](https://www.kaggle.com/rush4ratio/video-game-sales-with-ratings). <br>\n",
    "\n",
    "Build a neural network that can predict the \"<b>Rating</b>\" of each game based on other features. Alternatively, you may predict either the global sales or regional sales (in North America, Europe, etc) for each row/video game. Perform any necessary preprocessing steps needed on the dataset. <br>\n",
    "\n",
    "You should create a separate notebook for Extra Credit attempt and submit it via a separate link in Blackboard. If you can get good results based on the instructor's judgement of your work, you may earn up to 50 points of extra credit for A5 that can be used for the missing points of \"Assignment\" section of your final grade.<br>\n",
    "\n",
    "<b>Note:</b> Extra credits for A5 can only be used to compensate for \"Assignment\" section NOT for any other sections of the final grade such as quizzes or exams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "6SES-JQF21Mp",
    "outputId": "500b0ad3-4cf1-4e32-b774-082d044eb9d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16719, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Year_of_Release</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>NA_Sales</th>\n",
       "      <th>EU_Sales</th>\n",
       "      <th>JP_Sales</th>\n",
       "      <th>Other_Sales</th>\n",
       "      <th>Global_Sales</th>\n",
       "      <th>Critic_Score</th>\n",
       "      <th>Critic_Count</th>\n",
       "      <th>User_Score</th>\n",
       "      <th>User_Count</th>\n",
       "      <th>Developer</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wii Sports</td>\n",
       "      <td>Wii</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>41.36</td>\n",
       "      <td>28.96</td>\n",
       "      <td>3.77</td>\n",
       "      <td>8.45</td>\n",
       "      <td>82.53</td>\n",
       "      <td>76.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>8</td>\n",
       "      <td>322.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Super Mario Bros.</td>\n",
       "      <td>NES</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>Platform</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>29.08</td>\n",
       "      <td>3.58</td>\n",
       "      <td>6.81</td>\n",
       "      <td>0.77</td>\n",
       "      <td>40.24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mario Kart Wii</td>\n",
       "      <td>Wii</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>Racing</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>15.68</td>\n",
       "      <td>12.76</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.29</td>\n",
       "      <td>35.52</td>\n",
       "      <td>82.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>709.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wii Sports Resort</td>\n",
       "      <td>Wii</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>15.61</td>\n",
       "      <td>10.93</td>\n",
       "      <td>3.28</td>\n",
       "      <td>2.95</td>\n",
       "      <td>32.77</td>\n",
       "      <td>80.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8</td>\n",
       "      <td>192.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pokemon Red/Pokemon Blue</td>\n",
       "      <td>GB</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>Role-Playing</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>11.27</td>\n",
       "      <td>8.89</td>\n",
       "      <td>10.22</td>\n",
       "      <td>1.00</td>\n",
       "      <td>31.37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Name Platform  ...  Developer Rating\n",
       "0                Wii Sports      Wii  ...   Nintendo      E\n",
       "1         Super Mario Bros.      NES  ...        NaN    NaN\n",
       "2            Mario Kart Wii      Wii  ...   Nintendo      E\n",
       "3         Wii Sports Resort      Wii  ...   Nintendo      E\n",
       "4  Pokemon Red/Pokemon Blue       GB  ...        NaN    NaN\n",
       "\n",
       "[5 rows x 16 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_data = pd.read_csv('video.csv')\n",
    "print(video_data.shape)\n",
    "video_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "nu9uSWwu21M3",
    "outputId": "beb96dfd-67ac-4b41-cb12-219feeada055"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6825, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Year_of_Release</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>NA_Sales</th>\n",
       "      <th>EU_Sales</th>\n",
       "      <th>JP_Sales</th>\n",
       "      <th>Other_Sales</th>\n",
       "      <th>Global_Sales</th>\n",
       "      <th>Critic_Score</th>\n",
       "      <th>Critic_Count</th>\n",
       "      <th>User_Score</th>\n",
       "      <th>User_Count</th>\n",
       "      <th>Developer</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wii Sports</td>\n",
       "      <td>Wii</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>41.36</td>\n",
       "      <td>28.96</td>\n",
       "      <td>3.77</td>\n",
       "      <td>8.45</td>\n",
       "      <td>82.53</td>\n",
       "      <td>76.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>8</td>\n",
       "      <td>322.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mario Kart Wii</td>\n",
       "      <td>Wii</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>Racing</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>15.68</td>\n",
       "      <td>12.76</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.29</td>\n",
       "      <td>35.52</td>\n",
       "      <td>82.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>709.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wii Sports Resort</td>\n",
       "      <td>Wii</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>15.61</td>\n",
       "      <td>10.93</td>\n",
       "      <td>3.28</td>\n",
       "      <td>2.95</td>\n",
       "      <td>32.77</td>\n",
       "      <td>80.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8</td>\n",
       "      <td>192.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>New Super Mario Bros.</td>\n",
       "      <td>DS</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Platform</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>11.28</td>\n",
       "      <td>9.14</td>\n",
       "      <td>6.50</td>\n",
       "      <td>2.88</td>\n",
       "      <td>29.80</td>\n",
       "      <td>89.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>431.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Wii Play</td>\n",
       "      <td>Wii</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Misc</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>13.96</td>\n",
       "      <td>9.18</td>\n",
       "      <td>2.93</td>\n",
       "      <td>2.84</td>\n",
       "      <td>28.92</td>\n",
       "      <td>58.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>129.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Name Platform  Year_of_Release  ... User_Count Developer  Rating\n",
       "0             Wii Sports      Wii           2006.0  ...      322.0  Nintendo       E\n",
       "2         Mario Kart Wii      Wii           2008.0  ...      709.0  Nintendo       E\n",
       "3      Wii Sports Resort      Wii           2009.0  ...      192.0  Nintendo       E\n",
       "6  New Super Mario Bros.       DS           2006.0  ...      431.0  Nintendo       E\n",
       "7               Wii Play      Wii           2006.0  ...      129.0  Nintendo       E\n",
       "\n",
       "[5 rows x 16 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_data.dropna(inplace=True)\n",
    "print(video_data.shape)\n",
    "video_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tcqNY2Tm21M9"
   },
   "source": [
    "## Grading\n",
    "\n",
    "For Assignment 5, your notebook will be run and graded with a maximum of 100 points. Make sure that you get the correct outputs for all cells that you implement and answer ALL questions COMPLETELY. Also, your notebook should be written with no grammatical and spelling errors and should be nicely-formatted and easy-to-read.\n",
    "\n",
    "The breakdown of the 100 points is as follows:\n",
    "\n",
    "Part I implementaion has 40 points:\n",
    "- 10 points: preprocessing steps.\n",
    "- 15 points: nn_clf implementation, and compile.\n",
    "- 15 points: correct ROC curve for nn_clf.\n",
    "\n",
    "Part I questions have 10 points (5 points each).\n",
    "\n",
    "Part II implementaion has 35 points:\n",
    "- 10 points: preprocessing steps.\n",
    "- 10 points: nn_reg1 implementation, and compile.\n",
    "- 15 points: nn_reg2 implementation, and compile including hyperparameter tuning.\n",
    "\n",
    "Part II questions have 15 points (5 points each).\n",
    "\n",
    "Part III is optional and for Extra Credit only - up to 50 extra points based on the quality of your work.\n",
    "\n",
    "Follow the instructions of each section carefully. Up to 10 points may be deducted if your submitted notebook is not easy to read and follow or if it has grammatical and spelling errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dsdyba3921M-"
   },
   "source": [
    "## How to Submit and Due Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JCRLYh5T21NG"
   },
   "source": [
    "Name your notebook ```Lastname-A5.ipynb```. Submit the file using the ```Assignment-5``` link on Blackboard.\n",
    "\n",
    "If you attempt the Extra Credit in Part III, create a separate notebook including all the necessary code, name it `Lastname-A5-EC.ipynb` and submit it using the ```A5-Extra-Credit``` link on Blackboard.\n",
    "\n",
    "Grading will be based on \n",
    "\n",
    "  * correct implementation, correct answer to the questions, and\n",
    "  * readability of the notebook.\n",
    "  \n",
    "<font color=red><b>Due Date: Monday December 2nd 11:59PM.</b></font>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "l2M_t7_o21J8",
    "u2CoIWnI21KC",
    "qQvOscS621Kd",
    "W0eAp9gE21Ku",
    "YafkeW_Q21LK",
    "4817UJSP21LQ",
    "g85OxiJk21LT",
    "tQu6I20D21Ld",
    "5QZ0VTnP21Ll",
    "RE9h30jU21Lz",
    "2HqqLmXI21MD",
    "lc8nyPYX21Ml",
    "at32Tpgj21Mn",
    "tcqNY2Tm21M9",
    "dsdyba3921M-"
   ],
   "name": "Assignment-5.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
